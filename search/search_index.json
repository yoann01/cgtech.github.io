{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"IntroHoudini/","text":"Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. TIP EXAMPLE Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Visit the community forum PYTHON EXAMPLE 1 import tensorflow as tf C++ EXAMPLE 1 2 3 4 5 6 7 8 9 10 void setup () { pinMode ( 0 , OUTPUT ); } void loop () { digitalWrite ( 0 , HIGH ); delay ( 500 ); digitalWrite ( 0 , LOW ); delay ( 500 ); }","title":"Introduction"},{"location":"IntroHoudini/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"IntroHoudini/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. TIP EXAMPLE Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Visit the community forum PYTHON EXAMPLE 1 import tensorflow as tf C++ EXAMPLE 1 2 3 4 5 6 7 8 9 10 void setup () { pinMode ( 0 , OUTPUT ); } void loop () { digitalWrite ( 0 , HIGH ); delay ( 500 ); digitalWrite ( 0 , LOW ); delay ( 500 ); }","title":"Project layout"},{"location":"IntroMaya/","text":"","title":"Introduction"},{"location":"color/","text":"","title":"Color"},{"location":"hVars/","text":"HOUDINI TIP | TAKING ADVANTAGE OF ENVIRONMENT VARIABLES Like many other applications, Houdini\u2019s configuration can be modified by using environment variables. It is a very useful way of creating different Houdini setups for different purposes \u2013 let it be different projects, different plugin versions, sandboxes for testing assets, personal configurations, etc. While this post is written for Houdini and have examples for windows and linux, it is applicable for any other application and operating system. A COUPLE OF WORDS ABOUT ENVIRONMENT VARIABLES. At first I will try to explain basic concepts of env vars and later on I will show examples for Houdini (linux and windows). Environment variables might be a bit hidden to many users, but they can be very helpful. Actually a lot of operating system behavior is controlled by them. Apart from configuring Houdini, env vars can be used to modify a search path for binaries, libraries, can set up proxy settings, get current user\u2019s name, path to home, temp directory, hostname, shell and much more. An useful thing is that they can be applied globally to the whole system, and locally to the current environment in a shell (a terminal session or environment set up and executed from batch/bash script). For example one might specify global settings which are available to the whole system (path to system cache dir, username, path to root directory of all projects\u2026) and local settings for each configured environments (houdini version, project name, project root, path to textures directory, license information about a plugin\u2026). All child environments (set up through batch/bash script) will inherit global settings (available to the whole operating system or an user) and can have specific local settings needed for their purpose. Here are couple of situations which env vars can efficiently address: -[x] having a multiple versions of Houdini using different renderer plugin versions and different assets libraries -[x] using different configurations for different projects / sequences / shots / tasks -[x] sharing the same settings (home dir, temp dir, cache dir, footage path, rv path) for various applications (Houdini, Nuke, Katana, Maya) USING ENVIRONMENT VARIABLES ON WINDOWS Here I will show an example of one of my Houdini startup script which sets an environment for a project and executes Houdini: houdini.bat. Note that Houdini understands a lot of useful env vars, documentation can be found here. @echo off rem houdini launcher rem source global vars call \\\\ network_drive \\c urrent_project \\0 0_pipeline \\g lobals.bat rem set Houdini paths rem variables PIPELINE, ROOT were set in globals.bat set \"HOUDINI_VERSION=Houdini 15.5.632\" set \"HOUDINI_PATH=&;%PIPELINE%/houdini\" set \"HOUDINI_OTLSCAN_PATH=&;%ROOT%/20_assets/otls\" set \"HOUDINI_SPLASH_FILE=%PIPELINE%/houdini/splash.jpg\" set \"HOUDINI_SPLASH_MESSAGE= | JELLY FISH | %HOUDINI_VERSION% | %USERNAME% |\" set \"JOB=%ROOT%\" set \"HOUDINI_BACKUP_FILENAME= $BASENAME_bak_$N \" set \"HOUDINI_BACKUP_DIR=bak\" set \"HOUDINI_NO_START_PAGE_SPLASH=1\" set \"HOUDINI_ANONYMOUS_STATISTICS=0\" set \"HOME=%ROOT%/05_user/%USERNAME%\" set \"HOUDINI_DESK_PATH=&;C:/Users/%USERNAME%/Documents/houdini15.5/desktop\" set \"HOUDINI_TEMP_DIR=%HOME%/tmp\" set \"HOUDINI_BUFFEREDSAVE=1\" set \"HOUDINI_IMAGE_DISPLAY_GAMMA=1\" set \"HOUDINI_IMAGE_DISPLAY_LUT=%PIPELINE%/houdini/linear-to-srgb_14bit.lut\" set \"HOUDINI_IMAGE_DISPLAY_OVERRIDE=1\" set \"MEGASCANS=%JOB%/20_assets/megascans/Megascans/\" set \"MEGASCANS3D=%MEGASCANS%3d/\" rem convert to forward slashes set \"MEGASCANS=%MEGASCANS:\\=/%\" set \"MEGASCANS3D=%MEGASCANS3D:\\=/%\" rem create temp dir for houdini user if it does not exist, also convert to forwardslashes set \"TMP=%HOUDINI_TEMP_DIR%\" set \"TMP=%TMP:/=\\%\" IF not exist %TMP% ( mkdir %TMP% ) rem run Houdini set \"HOUDINI_DIR=C:\\Program Files\\Side Effects Software\\%HOUDINI_VERSION%\\bin\" set \"PATH=%HOUDINI_DIR%;%PATH%\" cd ../../ start houdinifx Here I will explain some of batch syntax: @echo off \u2013 hides printing of commands into the console, it makes output more readable rem foo \u2013 lines starting with rem are comments call foo.bat \u2013 executes foo.bat batch script and inherits all env vars specified in it %MYVAR% \u2013 value of MYVAR variable set \u201cMYVAR=FOO\u201d \u2013 sets MYVAR variable to FOO \u2013 those variables are specific to softwares, which are expecting them set \u201cPATH=%PATH%;FOO\u201d \u2013 appends FOO to the PATH variable which is used for locating binaries to be executed (houdinifx command is not known to the whole windows system, only when houdini bin directory (containing houdinifx.exe) is appended to the PATH var) start foo \u2013 executes foo.exe (which must be found in one of the paths in the PATH env var) while passing on all existing variables to this child environment This script can be used for setting up local environment variables, to define a variable for the whole system environment on windows, follow this video. USING ENVIRONMENT VARIABLES ON LINUX (Linux version should be directly transferable to mac OS.) In linux env vars can be set up on multiple locations (~/.profile, ~/.bashrc, ~/.bash_profile, ~/.bash_login, /etc/environment \u2026) System env vars can be specified in /etc/environment. User env vars can be specified in ~/.profile (~ symbol points to the user home directory). ~/.profile, ~/.bashrc, ~/.bash_profile, and ~/.bash_login have similar usage, but I recommend using ~/.profile, which is the only one executed when running applications from graphical environment in a desktop session.. In a Bash terminal the quickest way of executing an application inheriting env vars is to use this syntax:","title":"Environment variables"},{"location":"hVars/#houdini-tip--taking-advantage-of-environment-variables","text":"Like many other applications, Houdini\u2019s configuration can be modified by using environment variables. It is a very useful way of creating different Houdini setups for different purposes \u2013 let it be different projects, different plugin versions, sandboxes for testing assets, personal configurations, etc. While this post is written for Houdini and have examples for windows and linux, it is applicable for any other application and operating system.","title":"HOUDINI TIP | TAKING ADVANTAGE OF ENVIRONMENT VARIABLES"},{"location":"hVars/#a-couple-of-words-about-environment-variables","text":"At first I will try to explain basic concepts of env vars and later on I will show examples for Houdini (linux and windows). Environment variables might be a bit hidden to many users, but they can be very helpful. Actually a lot of operating system behavior is controlled by them. Apart from configuring Houdini, env vars can be used to modify a search path for binaries, libraries, can set up proxy settings, get current user\u2019s name, path to home, temp directory, hostname, shell and much more. An useful thing is that they can be applied globally to the whole system, and locally to the current environment in a shell (a terminal session or environment set up and executed from batch/bash script). For example one might specify global settings which are available to the whole system (path to system cache dir, username, path to root directory of all projects\u2026) and local settings for each configured environments (houdini version, project name, project root, path to textures directory, license information about a plugin\u2026). All child environments (set up through batch/bash script) will inherit global settings (available to the whole operating system or an user) and can have specific local settings needed for their purpose. Here are couple of situations which env vars can efficiently address: -[x] having a multiple versions of Houdini using different renderer plugin versions and different assets libraries -[x] using different configurations for different projects / sequences / shots / tasks -[x] sharing the same settings (home dir, temp dir, cache dir, footage path, rv path) for various applications (Houdini, Nuke, Katana, Maya)","title":"A COUPLE OF WORDS ABOUT ENVIRONMENT VARIABLES."},{"location":"hVars/#using-environment-variables-on-windows","text":"Here I will show an example of one of my Houdini startup script which sets an environment for a project and executes Houdini: houdini.bat. Note that Houdini understands a lot of useful env vars, documentation can be found here. @echo off rem houdini launcher rem source global vars call \\\\ network_drive \\c urrent_project \\0 0_pipeline \\g lobals.bat rem set Houdini paths rem variables PIPELINE, ROOT were set in globals.bat set \"HOUDINI_VERSION=Houdini 15.5.632\" set \"HOUDINI_PATH=&;%PIPELINE%/houdini\" set \"HOUDINI_OTLSCAN_PATH=&;%ROOT%/20_assets/otls\" set \"HOUDINI_SPLASH_FILE=%PIPELINE%/houdini/splash.jpg\" set \"HOUDINI_SPLASH_MESSAGE= | JELLY FISH | %HOUDINI_VERSION% | %USERNAME% |\" set \"JOB=%ROOT%\" set \"HOUDINI_BACKUP_FILENAME= $BASENAME_bak_$N \" set \"HOUDINI_BACKUP_DIR=bak\" set \"HOUDINI_NO_START_PAGE_SPLASH=1\" set \"HOUDINI_ANONYMOUS_STATISTICS=0\" set \"HOME=%ROOT%/05_user/%USERNAME%\" set \"HOUDINI_DESK_PATH=&;C:/Users/%USERNAME%/Documents/houdini15.5/desktop\" set \"HOUDINI_TEMP_DIR=%HOME%/tmp\" set \"HOUDINI_BUFFEREDSAVE=1\" set \"HOUDINI_IMAGE_DISPLAY_GAMMA=1\" set \"HOUDINI_IMAGE_DISPLAY_LUT=%PIPELINE%/houdini/linear-to-srgb_14bit.lut\" set \"HOUDINI_IMAGE_DISPLAY_OVERRIDE=1\" set \"MEGASCANS=%JOB%/20_assets/megascans/Megascans/\" set \"MEGASCANS3D=%MEGASCANS%3d/\" rem convert to forward slashes set \"MEGASCANS=%MEGASCANS:\\=/%\" set \"MEGASCANS3D=%MEGASCANS3D:\\=/%\" rem create temp dir for houdini user if it does not exist, also convert to forwardslashes set \"TMP=%HOUDINI_TEMP_DIR%\" set \"TMP=%TMP:/=\\%\" IF not exist %TMP% ( mkdir %TMP% ) rem run Houdini set \"HOUDINI_DIR=C:\\Program Files\\Side Effects Software\\%HOUDINI_VERSION%\\bin\" set \"PATH=%HOUDINI_DIR%;%PATH%\" cd ../../ start houdinifx Here I will explain some of batch syntax: @echo off \u2013 hides printing of commands into the console, it makes output more readable rem foo \u2013 lines starting with rem are comments call foo.bat \u2013 executes foo.bat batch script and inherits all env vars specified in it %MYVAR% \u2013 value of MYVAR variable set \u201cMYVAR=FOO\u201d \u2013 sets MYVAR variable to FOO \u2013 those variables are specific to softwares, which are expecting them set \u201cPATH=%PATH%;FOO\u201d \u2013 appends FOO to the PATH variable which is used for locating binaries to be executed (houdinifx command is not known to the whole windows system, only when houdini bin directory (containing houdinifx.exe) is appended to the PATH var) start foo \u2013 executes foo.exe (which must be found in one of the paths in the PATH env var) while passing on all existing variables to this child environment This script can be used for setting up local environment variables, to define a variable for the whole system environment on windows, follow this video. USING ENVIRONMENT VARIABLES ON LINUX (Linux version should be directly transferable to mac OS.) In linux env vars can be set up on multiple locations (~/.profile, ~/.bashrc, ~/.bash_profile, ~/.bash_login, /etc/environment \u2026) System env vars can be specified in /etc/environment. User env vars can be specified in ~/.profile (~ symbol points to the user home directory). ~/.profile, ~/.bashrc, ~/.bash_profile, and ~/.bash_login have similar usage, but I recommend using ~/.profile, which is the only one executed when running applications from graphical environment in a desktop session.. In a Bash terminal the quickest way of executing an application inheriting env vars is to use this syntax:","title":"USING ENVIRONMENT VARIABLES ON WINDOWS"},{"location":"houdiniPython/","text":"","title":"Python"},{"location":"indexsd/","text":"Welcome to cg_tech","title":"Welcome to cg_tech"},{"location":"indexsd/#welcome-to-cg_tech","text":"","title":"Welcome to cg_tech"},{"location":"indexsds/","text":"body { color:black; background-color: #424242; background-image: linear-gradient( 45deg, #2b2b2b 25%, transparent 25%, transparent 75%, #2b2b2b 75%, #2b2b2b ), linear-gradient( -45deg, #2b2b2b 25%, transparent 25%, transparent 75%, #2b2b2b 75%, #2b2b2b ); background-size: 60px 60px; background-position: 0 0; animation: slide 5s infinite linear; } @keyframes slide { from { background-position: 0 0; } to { background-position: -120px 60px; } } /* Application header should be static for the landing page */ .md-header { position: initial; } /* Remove spacing, as we cannot hide it completely */ .md-main__inner { margin: 0; } /* Hide table of contents */ @media screen and (min-width: 60em) { .md-sidebar--secondary { display: none; } } /* Hide navigation */ @media screen and (min-width: 76.25em) { .md-sidebar--primary { display: none; } } .modal { position: absolute; left: 50%; top: 55%; transform: translate(-50%, -50%); width: 370px; display: inline-flex; flex-direction: column; align-items: center; padding: 1.4rem 1rem; border: 3px solid black; border-radius: 5px; background: white; box-shadow: 8px 8px 0 rgba(0, 0, 0, 0.2); } .message { font-size:2em; margin: 0px 0px 0px; position: relative; top: -8px; } .btn { color:inherit; font-family:inherit; font-size: inherit; background-image: linear-gradient(130deg, #00C0FF 0%, #FFCF00 49%, #FC4F4F 100%); padding: 0.3rem 3.4rem; border: 3px solid black; border-radius: 3px; margin-right: 2.6rem; box-shadow: 0 0 0 black; transition: all 0.2s; } .btn:last-child { margin: 0; } .btn:hover { box-shadow: 0.4rem 0.4rem 0 #141414; transform: translate(-0.4rem, -0.4rem); } .btn:active { box-shadow: 0 0 0 black; transform: translate(0, 0); } .options { display: flex; flex-direction: row; justify-content: space-between; margin-top: 0.85rem; } .container { position: relative; } .container img { width: auto; height: auto; border-radius: 3px; box-shadow: 0 6px 10px 6px rgba(0, 0, 0, 0.1); } .container .info { position: absolute; top: 6.5%; left: 6.5%; transform: translate(-50%, -50%); -ms-transform: translate(-50%, -50%); background-color: white; color: white; font-size: 19px; padding: 0px 4px; border-radius: 10%; cursor: pointer; } .container .info:hover { background-color: #5466ce; } .container .tooltip{ position: absolute; display: inline; top: 6.5%; left: 6.5%; transform: translate(-50%, -50%); -ms-transform: translate(-50%, -50%); } .container .tooltip:after{ display: block; visibility: hidden; position: absolute; bottom: 0; left: 20%; opacity: 0; content: attr(tool-tips); /* might also use attr(title) */ height: auto; min-width: 250px; padding: 5px 8px; z-index: 999; color: #fff; text-decoration: none; text-align: center; background: rgba(0,0,0,0.85); -webkit-border-radius: 5px; -moz-border-radius: 5px; border-radius: 5px; } .container .tooltip:before { position: absolute; visibility: hidden; width: 0; height: 0; left: 50%; bottom: 0px; opacity: 0; content: \"\"; border-style: solid; border-width: 6px 6px 0 6px; border-color: rgba(0,0,0,0.85) transparent transparent transparent; } .container .tooltip:hover:after{ visibility: visible; opacity: 1; bottom: 20px; } .container .tooltip:hover:before{ visibility: visible; opacity: 1; bottom: 14px; } a.tooltip.left:after { left: auto; right: 100%; bottom: -45%; } a.tooltip.left:hover:after { right: 110%; bottom: -45%; } a.tooltip.left:before { border-width: 5px 0 5px 10px; border-color: transparent transparent transparent rgba(0,0,0,0.85); left: auto; right: 90%; bottom: 2%; } a.tooltip.left:hover:before { right: 100%; bottom: 2%; } Welcome to My Site! \u24d8 About Me","title":"Indexsds"},{"location":"solaris/","text":"Mots cl\u00e9s: - SOLARIS -Houdini - USD USD/SOLARIS Et avant de commencer, un petit rappel sur l'USD. USD est une biblioth\u00e8que de gestion des sc\u00e9nographies cr\u00e9\u00e9es par Pixar. C'est un format de fichier cr\u00e9\u00e9 pour que plusieurs personnes travaillent en m\u00eame temps. La principale diff\u00e9rence par rapport au format de fichier existant est Rendre plusieurs fichiers USD \"proc\u00e9duraux\", appel\u00e9s \"sc\u00e9nographie proc\u00e9durale\" La possibilit\u00e9 de composer et de construire un seul sc\u00e9nographie. \u00c0 partir des diapositives du s\u00e9minaire de lancement. SOLARIS supporte nativement cet USD. Synth\u00e9tiser proc\u00e9duralement le fichier USD pour g\u00e9rer le SOP, Vous pouvez construire un sc\u00e9nario. Une autre caract\u00e9ristique est que les n\u0153uds de ce SOLARIS (r\u00e9seau Stage) sont en m\u00e9moire ou dans un fichier Chaque n\u0153ud est un fichier USD et trait\u00e9 comme une couche. Traitement des n\u0153uds = Traitement USD = Construction proc\u00e9durale du graphe de sc\u00e8ne C'est le monde de SOLARIS. Le monde de SOLARIS est aussi le monde de l'USD lui-m\u00eame. C'est un aper\u00e7u approximatif de SOLARIS et USD. T\u00e9l\u00e9charger le fichier USD Nous sommes donc pr\u00eats \u00e0 ouvrir. Pour cet \u00e9chantillon, j'apporterai un fichier USD officiel de Pixar. http://graphics.pixar.com/usd/downloads.html Chaque fois que je t\u00e9l\u00e9charge le KITCHEN SET familier sur le site de t\u00e9l\u00e9chargement officiel. Ouvrir SOLARIS Une fois t\u00e9l\u00e9charg\u00e9, lancez Houdini18. Une fois d\u00e9marr\u00e9, changez la disposition de l'\u00e9cran en Solaris. Composition de l'\u00e9cran SOLARIS a ajout\u00e9 deux nouvelles vues que Houdini n'avait pas auparavant. Le premier est l'arborescence des graphes de sc\u00e8ne Ceci affiche un ``scenegraph en USD'' (appel\u00e9s \u00e9tapes) des \u00e9tapes du n\u0153ud actuellement s\u00e9lectionn\u00e9. Ce SOLARIS signifie \"USD Procedural Scene Graph\" Un n\u0153ud est trait\u00e9 comme un fichier USD = couche. Lorsque vous connectez des n\u0153uds, les n\u0153uds sont compos\u00e9s de mani\u00e8re proc\u00e9durale. et pour les n\u0153uds qui sont actuellement s\u00e9lectionn\u00e9s ou dont l'indicateur d'affichage est activ\u00e9 L'\u00e9tape est \"le r\u00e9sultat de la composition des n\u0153uds jusqu'\u00e0 ce point\". Le graphique de sc\u00e8ne r\u00e9sultant est affich\u00e9 comme C'est ce \"Scene Graph Path\". L'autre est \"Scene Graph Detail\". Il s'agit d'un panneau dans lequel vous pouvez centrer les attributs de la primitive actuellement s\u00e9lectionn\u00e9e et d'autres informations diverses. Quand je commence \u00e0 expliquer les d\u00e9tails, il y a suffisamment de mati\u00e8re pour \u00e9crire un article ici. Je vais l'omettre cette fois, mais quand vous regardez chaque individu, vous pouvez voir comment ils sont compos\u00e9s. Vous pouvez voir les valeurs d'attribut actuelles, etc. Ces deux-l\u00e0 seront plus attach\u00e9s \u00e0 USD qu'aux fonctionnalit\u00e9s SOLARIS. \u00c9tant donn\u00e9 que le r\u00e9sultat de la construction d'un graphe sc\u00e9nique dans SOLARIS est affich\u00e9 dans ces deux Si vous v\u00e9rifiez ces deux panneaux avec la cr\u00e9ation de n\u0153uds C'est facile \u00e0 comprendre \u00e0 bien des \u00e9gards. Par cons\u00e9quent, je vais montrer ces deux ensemble dans l'explication ci-dessous. USD, ouvrons-le Passons donc au sujet principal. Ouvrez le fichier USD t\u00e9l\u00e9charg\u00e9 avec Houdini/SOLARIS. Tout d'abord, utilisez le n\u0153ud \"LoadLayer\" pour ouvrir le fichier USD. La raison pour laquelle ce n'est pas LoadUSD est En effet, USD fait r\u00e9f\u00e9rence aux fichiers USD en tant que couches. Pour plus de d\u00e9tails \u00e0 ce sujet, https://fereria.github.io/reincarnation_tech/11_Pipeline/01_USD/04_layer_stage/ Voir article pr\u00e9c\u00e9dent. Cependant, puisque l'ensemble de cuisine est Zup Comme ce sera \u00e9trange si c'est comme \u00e7a, ouvrez les fen\u00eatres 3D, Changez l'orientation en \"Z UP\". J'ai pu charger avec succ\u00e8s. Je voudrais dire que ce sera OK si divers processus sont effectu\u00e9s \u00e0 ce sujet. En fait, SOLARIS a diff\u00e9rentes mani\u00e8res de lire l'USD. Jetons un coup d'\u0153il aux autres m\u00e9thodes de chargement. ChargerCouche Tout d'abord, le LoadLayer utilis\u00e9 ci-dessus \"charge l'USD sp\u00e9cifi\u00e9 en tant que couche\". C'est facile \u00e0 comprendre en regardant le SceneGraphPath, mais en utilisant ce n\u0153ud Pour root, \"chargez\" l'USD sp\u00e9cifi\u00e9 tel quel. Je n'ai rien fait d'autre. Ouvrir avec le n\u0153ud d'arc de composition Alors, quel genre de m\u00e9thode existe-t-il autre que LoadLayer ? Divers n\u0153uds d'arc de composition pr\u00e9par\u00e9s comme n\u0153uds SOLARIS je vais le charger. Lorsqu'il est charg\u00e9 avec ce n\u0153ud, ainsi que la fonctionnalit\u00e9 des arcs de composition, Vous pouvez charger des fichiers USD. S'il est lu comme ceci, le ClassPrim dans le fichier usda charg\u00e9 est cach\u00e9 Il ne peut plus \u00eatre remplac\u00e9. Mais si je le charge avec LoadLayer puis que je connecte ce n\u0153ud \u00e0 un n\u0153ud de r\u00e9f\u00e9rence, Synth\u00e9tisez la couche que vous souhaitez lire par r\u00e9f\u00e9rence une fois en tant que sous-couche Apr\u00e8s cela </sdfPath\u2026> Prim dans la sc\u00e8ne comme celle-ci Il sera charg\u00e9 en tant que r\u00e9f\u00e9rence. Dans ce cas, la couche n'est pas encapsul\u00e9e et une autre prim dans la couche sp\u00e9cifi\u00e9e est \u00e9galement charg\u00e9e. Semblable \u00e0 l'h\u00e9ritage, vous pouvez d\u00e9sormais remplacer la prim de r\u00e9f\u00e9rence. Pour plus de d\u00e9tails ici car la v\u00e9rification est r\u00e9sum\u00e9e se il vous pla\u00eet se r\u00e9f\u00e9rer. Ouvrir dans le sous-calque Tout d'abord, ouvrez-le dans le n\u0153ud Sous-couche. Lorsqu'il est ouvert avec une sous-couche, les bases sont les m\u00eames que lorsqu'il est charg\u00e9 avec LoadLayer. Le chemin du graphe de sc\u00e8ne sera \u00e9galement le m\u00eame que lorsque LoadLayer est utilis\u00e9. La diff\u00e9rence avec loadlayer est que vous pouvez combiner des fichiers USD avec des sous-couches avec ce seul n\u0153ud. Par exemple, chargez trois fichiers comme celui-ci : Comme une boule verte avec base.usda comme \u00e7a. Changez la couleur en rouge avec add_color.usda Essayez de le rendre carr\u00e9 avec final.usda. Ensuite, il sera synth\u00e9tis\u00e9 dans l'ordre \u00e0 partir du haut, et le r\u00e9sultat de la synth\u00e8se de tout sera produit \u00e0 la suite de ce n\u0153ud. L'avantage de synth\u00e9tiser plusieurs USD dans cette sous-couche est Il est possible d'activer/d\u00e9sactiver l'effet de l'USD en utilisant Mute Layer et Enable. Par exemple, d\u00e9sactivons \"add_color.usda\" parmi ces trois \u00e9chantillons. Ensuite, ce qui se passe, c'est que seule la couche (fichier USD) appel\u00e9e \"Red\" est muette Vous vous retrouverez avec un cube vert. De cette fa\u00e7on, pour activer / d\u00e9sactiver la couche de logiciel 2D C'est l'effet du noeud Sublayer qui vous permet de basculer et de v\u00e9rifier facilement l'effet de calque USD. Pendant le chargement, vous pouvez l'activer et le d\u00e9sactiver comme un calque PhotoShop C'est l'avantage de lire en utilisant ce n\u0153ud. Ouvrir avec r\u00e9f\u00e9rence Essayez maintenant de le charger \u00e0 l'aide du n\u0153ud de r\u00e9f\u00e9rence. Le n\u0153ud de r\u00e9f\u00e9rence a \u00e9galement une fonction pour r\u00e9f\u00e9rencer le r\u00e9sultat d'entr\u00e9e de MultiInput En r\u00e9glant Type de r\u00e9f\u00e9rence sur Fichiers de r\u00e9f\u00e9rence, Vous pouvez utiliser ce n\u0153ud pour \"Charger le fichier USD par r\u00e9f\u00e9rence\". En regardant le SceneGraphPath, nous pouvons voir que les noms primitifs sont verts. De plus, la grande diff\u00e9rence avec LoadLayer est que \"le nom du n\u0153ud sup\u00e9rieur est diff\u00e9rent\". Lorsqu'il est charg\u00e9 avec LoadLayer, la structure primitive \u00e9crite dans le fichier USD est Il sera charg\u00e9 tel quel. Mais pour les r\u00e9f\u00e9rences, la diff\u00e9rence est Sous le nom de la primitive sp\u00e9cifi\u00e9 dans Destination Primitive Lit la primitive USD lue par r\u00e9f\u00e9rence. Puisque $OS est le nom du n\u0153ud, la primitive enfant est charg\u00e9e sous reference1 qui est le nom du n\u0153ud. Essayez de cliquer avec le bouton droit sur le n\u0153ud de r\u00e9f\u00e9rence et d'ouvrir Actions LOP > Inspecter la couche active. #sdf 1.4.32 def HoudiniLayerInfo \"HoudiniLayerInfo\" ( donn\u00e9espersonnalis\u00e9es = { cha\u00eene HoudiniCreatorNode = \"/stage/reference1\" cha\u00eene[] HoudiniEditorNodes = [ \"/stage/reference1\" ] } ) { } def \"r\u00e9f\u00e9rence1\" ( pr\u00e9fixer les r\u00e9f\u00e9rences = @C:/pyEnv/JupyterUSD_py27/usd/Kitchen_set/Kitchen_set.usd@ ) { } Lorsque vous ouvrez ce menu, vous pouvez voir l'\u00e9tat actuel de l'\u00e9dition du n\u0153ud (= fichier USD du n\u0153ud) Vous pouvez le v\u00e9rifier avec un fichier ASCII. J'ai expliqu\u00e9 que tous les n\u0153uds sont des couches, mais cet ActiveLayer est \"USD de ce n\u0153ud\" C'est pourquoi. Donc, en regardant cela, vous pouvez voir ce que fait ce n\u0153ud maintenant et ce qu'il fait en termes d'USD Tu peux le v\u00e9rifier. Donc, d'apr\u00e8s ce que je peux voir, vous pouvez voir qu'il est lu par r\u00e9f\u00e9rence. Diff\u00e9rence de lecture avec le n\u0153ud de r\u00e9f\u00e9rence * 2020/01/05 ajout\u00e9 Il a \u00e9t\u00e9 trouv\u00e9 plus tard lors de l'examen du comportement d\u00e9taill\u00e9 du n\u0153ud Lorsqu'il est charg\u00e9 \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence et apr\u00e8s le chargement \u00e0 l'aide d'un LoadLayer J'ai trouv\u00e9 qu'il y a une grande diff\u00e9rence entre les n\u0153uds de connexion et de r\u00e9f\u00e9rence. La \"diff\u00e9rence\" est si encapsuler ou non . Lorsqu'il est charg\u00e9 avec un n\u0153ud de r\u00e9f\u00e9rence, lorsqu'il est visualis\u00e9 avec l'USDA pr\u00e9fixer les r\u00e9f\u00e9rences = @C :/pyEnv/JupyterUSD_py27/usd/Kitchen_set/Kitchen_set.usd@ En sp\u00e9cifiant le chemin du fichier comme celui-ci, le prim sp\u00e9cifi\u00e9 par USDA DefaultPrim ou SdfPath du chemin sp\u00e9cifi\u00e9 sera charg\u00e9 par r\u00e9f\u00e9rence. Ouvrir avec StageManager Enfin, comment ouvrir autre que l'arc de composition. C'est le noeud StageManager. Il existe de nombreux exemples d'utilisation de ce StageManager dans les vid\u00e9os de didacticiel Quelle est la diff\u00e9rence entre les trois ci-dessus et ce StageManager ? Comme la grande diff\u00e9rence est le gestionnaire \"Stage\", ce n\u0153ud est Vous pouvez construire une sc\u00e8ne par elle-m\u00eame. Une \u00e9tape dans ce cas fait r\u00e9f\u00e9rence au r\u00e9sultat de la composition de plusieurs fichiers USD. Une fois ouvert, l'\u00e9cran StageManager ressemblera \u00e0 la fen\u00eatre de l'image ci-dessus. Cliquez sur l'ic\u00f4ne du dossier qu'il contient. Jusqu'\u00e0 pr\u00e9sent, les n\u0153uds \u00e9taient principalement ceux qui lisaient un fichier USD avec un n\u0153ud Ce StageManager construit une sc\u00e8ne (un graphe de sc\u00e8ne r\u00e9sultant de la composition USD) par lui-m\u00eame Parce que c'est un n\u0153ud qui peut le faire, chargez plusieurs USD avec des r\u00e9f\u00e9rences, etc. Vous pouvez construire une \u00e9tape tout en regroupant. Ainsi, pour ce n\u0153ud StageManager seul, le ScenerGraphPath est Une \u00e9tape termin\u00e9e est construite. (Orange est charg\u00e9 par l'arc de composition) \u00c9crivez le fichier ASCII tel quel et ouvrez-le Peu de gens le font, mais avec inlineusd, Vous pouvez \u00e9crire et lire des fichiers USD \u00e0 la main au lieu de fichiers. En \u00e9crivant en ASCII pour USD Source comme ceci, Vous pouvez r\u00e9ellement cr\u00e9er une sc\u00e8ne dans SceneGraphPath comme celle-ci. Alors, comment puis-je l'ouvrir ! ! ? ? Comme vous pouvez le voir, il existe de nombreuses fa\u00e7ons d'ouvrir un fichier USD. Je ne sais pas quoi faire avec \u00e7a ! Pr\u00e9sentation de ma propre recommandation pour ceux qui disent. \"StageMnaager\" si vous voulez placer beaucoup d'objets Si vous voulez faire des mises en page dans la sc\u00e8ne, vous pourriez vous retrouver avec des centaines d'\u00e9l\u00e9ments. Vous devrez le charger et le placer. Dans un tel cas, il est tr\u00e8s g\u00eanant de les lire un par un ou de les regrouper. Dans ce cas, si vous utilisez StageManager, vous pouvez charger tous ensemble par r\u00e9f\u00e9rence Le chargement de plusieurs actifs est facile avec le glisser-d\u00e9poser Vous pouvez facilement effectuer des op\u00e9rations telles que le regroupement et la modification de la hi\u00e9rarchie. En d'autres termes, la construction de la sc\u00e8ne est termin\u00e9e avec ce seul n\u0153ud. Par cons\u00e9quent, ce n\u0153ud est recommand\u00e9 pour les mises en page. Bien s\u00fbr, si vous mettez tout cela ensemble dans un seul StageManager, cela deviendra d\u00e9sordonn\u00e9 ! ! ! Dans ce cas, vous pouvez \u00e9galement connecter plusieurs StageManagers. Dans ce cas, ajoutez des assets avec + \u03b1 au graphe de sc\u00e8ne de la sc\u00e8ne re\u00e7u en entr\u00e9e devient possible. \"Reference\" si vous souhaitez encapsuler une couche DefaultPrim de la couche que vous souhaitez lire par r\u00e9f\u00e9rence ou autre que la prim sp\u00e9cifi\u00e9e Si vous souhaitez \"l'encapsuler\", chargez-le \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence. Lorsqu'elles sont lues avec le n\u0153ud de r\u00e9f\u00e9rence, toutes les prims autres que la prim sp\u00e9cifi\u00e9e sont masqu\u00e9es. Il ne peut plus \u00eatre remplac\u00e9. Donc, si vous voulez lire un caract\u00e8re, etc. Le chargement \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence semble bon. sinon \"LoadLayer\" En dehors de cela, LoadLayer + n\u0153ud d'arc de composition ou autre n\u0153ud pratique (Graft, etc.) est recommand\u00e9. Apr\u00e8s cela, je veux construire une structure bas\u00e9e sur la base de n\u0153uds de SOLARIS pour le mod\u00e8le lu par LoadLayer (Tout en combinant des n\u0153uds l\u00e9gers qui souhaitent cr\u00e9er des variations avec des variantes et attribuer des shaders Je veux construire une sc\u00e8ne\u2026 etc. Vous pouvez lire le fichier directement avec SubLayer ou Reference Dans ce cas, il est tr\u00e8s difficile de remplacer l'ordre d'entr\u00e9e ou de le d\u00e9sactiver temporairement. Je l'ai trouv\u00e9 difficile \u00e0 utiliser car la listabilit\u00e9 au niveau du n\u0153ud n'\u00e9tait pas si bonne. Si vous lisez et composez toujours avec LoadLayer, Il est facile de basculer et clarifie \"lecture d'un fichier\" Personnellement, je pense qu'il est pr\u00e9f\u00e9rable d'utiliser LoadLayer pour le chargement unique de fichiers USD. Bien s\u00fbr, vous pouvez utiliser les deux Bien entendu, les n\u0153uds d'arc de composition StageMnaager et LoadLayer + peuvent \u00eatre utilis\u00e9s ensemble. Lors de la construction collective d'une sc\u00e8ne avec StageManager, faites des variations d\u00e9taill\u00e9es avec des n\u0153uds Enfin, synth\u00e9tisez En utilisant diff\u00e9rentes mani\u00e8res d'ouvrir plusieurs USD dans le bon sens Il est \u00e9galement possible de construire le plus simplement possible des n\u0153uds dans une sc\u00e8ne complexe. sommaire Il s'agissait d'une introduction \u00e0 l'ouverture de fichiers USD dans SOLARIS/LOP. Il est \u00e9tonnant qu'il existe autant d'approches diff\u00e9rentes simplement en ouvrant un seul fichier USD. Vous pouvez \u00e9galement ouvrir des sc\u00e8nes + SOP avec LOP et ainsi de suite. SOLARIS est tr\u00e8s flexible lorsqu'il s'agit d'USD et permet la cr\u00e9ation de divers USD en fonction de la fa\u00e7on de penser. Je pensais que c'\u00e9tait un outil. Je me demande si ce genre d'article est la premi\u00e8re \u00e9tape pour \u00e9crire beaucoup d'articles SOLARIS ? Ensuite, il s'agit du n\u0153ud d'arc de composition que j'ai \u00e9crit bri\u00e8vement cette fois J'aimerais \u00e9crire un peu plus \u00e0 ce sujet.","title":"Solaris"},{"location":"solaris/#usdsolaris","text":"Et avant de commencer, un petit rappel sur l'USD. USD est une biblioth\u00e8que de gestion des sc\u00e9nographies cr\u00e9\u00e9es par Pixar. C'est un format de fichier cr\u00e9\u00e9 pour que plusieurs personnes travaillent en m\u00eame temps. La principale diff\u00e9rence par rapport au format de fichier existant est Rendre plusieurs fichiers USD \"proc\u00e9duraux\", appel\u00e9s \"sc\u00e9nographie proc\u00e9durale\" La possibilit\u00e9 de composer et de construire un seul sc\u00e9nographie. \u00c0 partir des diapositives du s\u00e9minaire de lancement. SOLARIS supporte nativement cet USD. Synth\u00e9tiser proc\u00e9duralement le fichier USD pour g\u00e9rer le SOP, Vous pouvez construire un sc\u00e9nario. Une autre caract\u00e9ristique est que les n\u0153uds de ce SOLARIS (r\u00e9seau Stage) sont en m\u00e9moire ou dans un fichier Chaque n\u0153ud est un fichier USD et trait\u00e9 comme une couche. Traitement des n\u0153uds = Traitement USD = Construction proc\u00e9durale du graphe de sc\u00e8ne C'est le monde de SOLARIS. Le monde de SOLARIS est aussi le monde de l'USD lui-m\u00eame. C'est un aper\u00e7u approximatif de SOLARIS et USD.","title":"USD/SOLARIS"},{"location":"solaris/#t\u00e9l\u00e9charger-le-fichier-usd","text":"Nous sommes donc pr\u00eats \u00e0 ouvrir. Pour cet \u00e9chantillon, j'apporterai un fichier USD officiel de Pixar. http://graphics.pixar.com/usd/downloads.html Chaque fois que je t\u00e9l\u00e9charge le KITCHEN SET familier sur le site de t\u00e9l\u00e9chargement officiel.","title":"T\u00e9l\u00e9charger le fichier USD"},{"location":"solaris/#ouvrir-solaris","text":"Une fois t\u00e9l\u00e9charg\u00e9, lancez Houdini18. Une fois d\u00e9marr\u00e9, changez la disposition de l'\u00e9cran en Solaris.","title":"Ouvrir SOLARIS"},{"location":"solaris/#composition-de-l\u00e9cran","text":"SOLARIS a ajout\u00e9 deux nouvelles vues que Houdini n'avait pas auparavant. Le premier est l'arborescence des graphes de sc\u00e8ne Ceci affiche un ``scenegraph en USD'' (appel\u00e9s \u00e9tapes) des \u00e9tapes du n\u0153ud actuellement s\u00e9lectionn\u00e9. Ce SOLARIS signifie \"USD Procedural Scene Graph\" Un n\u0153ud est trait\u00e9 comme un fichier USD = couche. Lorsque vous connectez des n\u0153uds, les n\u0153uds sont compos\u00e9s de mani\u00e8re proc\u00e9durale. et pour les n\u0153uds qui sont actuellement s\u00e9lectionn\u00e9s ou dont l'indicateur d'affichage est activ\u00e9 L'\u00e9tape est \"le r\u00e9sultat de la composition des n\u0153uds jusqu'\u00e0 ce point\". Le graphique de sc\u00e8ne r\u00e9sultant est affich\u00e9 comme C'est ce \"Scene Graph Path\". L'autre est \"Scene Graph Detail\". Il s'agit d'un panneau dans lequel vous pouvez centrer les attributs de la primitive actuellement s\u00e9lectionn\u00e9e et d'autres informations diverses. Quand je commence \u00e0 expliquer les d\u00e9tails, il y a suffisamment de mati\u00e8re pour \u00e9crire un article ici. Je vais l'omettre cette fois, mais quand vous regardez chaque individu, vous pouvez voir comment ils sont compos\u00e9s. Vous pouvez voir les valeurs d'attribut actuelles, etc. Ces deux-l\u00e0 seront plus attach\u00e9s \u00e0 USD qu'aux fonctionnalit\u00e9s SOLARIS. \u00c9tant donn\u00e9 que le r\u00e9sultat de la construction d'un graphe sc\u00e9nique dans SOLARIS est affich\u00e9 dans ces deux Si vous v\u00e9rifiez ces deux panneaux avec la cr\u00e9ation de n\u0153uds C'est facile \u00e0 comprendre \u00e0 bien des \u00e9gards. Par cons\u00e9quent, je vais montrer ces deux ensemble dans l'explication ci-dessous.","title":"Composition de l'\u00e9cran"},{"location":"solaris/#usd-ouvrons-le","text":"Passons donc au sujet principal. Ouvrez le fichier USD t\u00e9l\u00e9charg\u00e9 avec Houdini/SOLARIS. Tout d'abord, utilisez le n\u0153ud \"LoadLayer\" pour ouvrir le fichier USD. La raison pour laquelle ce n'est pas LoadUSD est En effet, USD fait r\u00e9f\u00e9rence aux fichiers USD en tant que couches. Pour plus de d\u00e9tails \u00e0 ce sujet, https://fereria.github.io/reincarnation_tech/11_Pipeline/01_USD/04_layer_stage/ Voir article pr\u00e9c\u00e9dent. Cependant, puisque l'ensemble de cuisine est Zup Comme ce sera \u00e9trange si c'est comme \u00e7a, ouvrez les fen\u00eatres 3D, Changez l'orientation en \"Z UP\". J'ai pu charger avec succ\u00e8s. Je voudrais dire que ce sera OK si divers processus sont effectu\u00e9s \u00e0 ce sujet. En fait, SOLARIS a diff\u00e9rentes mani\u00e8res de lire l'USD. Jetons un coup d'\u0153il aux autres m\u00e9thodes de chargement.","title":"USD, ouvrons-le"},{"location":"solaris/#chargercouche","text":"Tout d'abord, le LoadLayer utilis\u00e9 ci-dessus \"charge l'USD sp\u00e9cifi\u00e9 en tant que couche\". C'est facile \u00e0 comprendre en regardant le SceneGraphPath, mais en utilisant ce n\u0153ud Pour root, \"chargez\" l'USD sp\u00e9cifi\u00e9 tel quel. Je n'ai rien fait d'autre.","title":"ChargerCouche"},{"location":"solaris/#ouvrir-avec-le-n\u0153ud-darc-de-composition","text":"Alors, quel genre de m\u00e9thode existe-t-il autre que LoadLayer ? Divers n\u0153uds d'arc de composition pr\u00e9par\u00e9s comme n\u0153uds SOLARIS je vais le charger. Lorsqu'il est charg\u00e9 avec ce n\u0153ud, ainsi que la fonctionnalit\u00e9 des arcs de composition, Vous pouvez charger des fichiers USD. S'il est lu comme ceci, le ClassPrim dans le fichier usda charg\u00e9 est cach\u00e9 Il ne peut plus \u00eatre remplac\u00e9. Mais si je le charge avec LoadLayer puis que je connecte ce n\u0153ud \u00e0 un n\u0153ud de r\u00e9f\u00e9rence, Synth\u00e9tisez la couche que vous souhaitez lire par r\u00e9f\u00e9rence une fois en tant que sous-couche Apr\u00e8s cela </sdfPath\u2026> Prim dans la sc\u00e8ne comme celle-ci Il sera charg\u00e9 en tant que r\u00e9f\u00e9rence. Dans ce cas, la couche n'est pas encapsul\u00e9e et une autre prim dans la couche sp\u00e9cifi\u00e9e est \u00e9galement charg\u00e9e. Semblable \u00e0 l'h\u00e9ritage, vous pouvez d\u00e9sormais remplacer la prim de r\u00e9f\u00e9rence. Pour plus de d\u00e9tails ici car la v\u00e9rification est r\u00e9sum\u00e9e se il vous pla\u00eet se r\u00e9f\u00e9rer.","title":"Ouvrir avec le n\u0153ud d'arc de composition"},{"location":"solaris/#ouvrir-dans-le-sous-calque","text":"Tout d'abord, ouvrez-le dans le n\u0153ud Sous-couche. Lorsqu'il est ouvert avec une sous-couche, les bases sont les m\u00eames que lorsqu'il est charg\u00e9 avec LoadLayer. Le chemin du graphe de sc\u00e8ne sera \u00e9galement le m\u00eame que lorsque LoadLayer est utilis\u00e9. La diff\u00e9rence avec loadlayer est que vous pouvez combiner des fichiers USD avec des sous-couches avec ce seul n\u0153ud. Par exemple, chargez trois fichiers comme celui-ci : Comme une boule verte avec base.usda comme \u00e7a. Changez la couleur en rouge avec add_color.usda Essayez de le rendre carr\u00e9 avec final.usda. Ensuite, il sera synth\u00e9tis\u00e9 dans l'ordre \u00e0 partir du haut, et le r\u00e9sultat de la synth\u00e8se de tout sera produit \u00e0 la suite de ce n\u0153ud. L'avantage de synth\u00e9tiser plusieurs USD dans cette sous-couche est Il est possible d'activer/d\u00e9sactiver l'effet de l'USD en utilisant Mute Layer et Enable. Par exemple, d\u00e9sactivons \"add_color.usda\" parmi ces trois \u00e9chantillons. Ensuite, ce qui se passe, c'est que seule la couche (fichier USD) appel\u00e9e \"Red\" est muette Vous vous retrouverez avec un cube vert. De cette fa\u00e7on, pour activer / d\u00e9sactiver la couche de logiciel 2D C'est l'effet du noeud Sublayer qui vous permet de basculer et de v\u00e9rifier facilement l'effet de calque USD. Pendant le chargement, vous pouvez l'activer et le d\u00e9sactiver comme un calque PhotoShop C'est l'avantage de lire en utilisant ce n\u0153ud.","title":"Ouvrir dans le sous-calque"},{"location":"solaris/#ouvrir-avec-r\u00e9f\u00e9rence","text":"Essayez maintenant de le charger \u00e0 l'aide du n\u0153ud de r\u00e9f\u00e9rence. Le n\u0153ud de r\u00e9f\u00e9rence a \u00e9galement une fonction pour r\u00e9f\u00e9rencer le r\u00e9sultat d'entr\u00e9e de MultiInput En r\u00e9glant Type de r\u00e9f\u00e9rence sur Fichiers de r\u00e9f\u00e9rence, Vous pouvez utiliser ce n\u0153ud pour \"Charger le fichier USD par r\u00e9f\u00e9rence\". En regardant le SceneGraphPath, nous pouvons voir que les noms primitifs sont verts. De plus, la grande diff\u00e9rence avec LoadLayer est que \"le nom du n\u0153ud sup\u00e9rieur est diff\u00e9rent\". Lorsqu'il est charg\u00e9 avec LoadLayer, la structure primitive \u00e9crite dans le fichier USD est Il sera charg\u00e9 tel quel. Mais pour les r\u00e9f\u00e9rences, la diff\u00e9rence est Sous le nom de la primitive sp\u00e9cifi\u00e9 dans Destination Primitive Lit la primitive USD lue par r\u00e9f\u00e9rence. Puisque $OS est le nom du n\u0153ud, la primitive enfant est charg\u00e9e sous reference1 qui est le nom du n\u0153ud. Essayez de cliquer avec le bouton droit sur le n\u0153ud de r\u00e9f\u00e9rence et d'ouvrir Actions LOP > Inspecter la couche active. #sdf 1.4.32 def HoudiniLayerInfo \"HoudiniLayerInfo\" ( donn\u00e9espersonnalis\u00e9es = { cha\u00eene HoudiniCreatorNode = \"/stage/reference1\" cha\u00eene[] HoudiniEditorNodes = [ \"/stage/reference1\" ] } ) { } def \"r\u00e9f\u00e9rence1\" ( pr\u00e9fixer les r\u00e9f\u00e9rences = @C:/pyEnv/JupyterUSD_py27/usd/Kitchen_set/Kitchen_set.usd@ ) { } Lorsque vous ouvrez ce menu, vous pouvez voir l'\u00e9tat actuel de l'\u00e9dition du n\u0153ud (= fichier USD du n\u0153ud) Vous pouvez le v\u00e9rifier avec un fichier ASCII. J'ai expliqu\u00e9 que tous les n\u0153uds sont des couches, mais cet ActiveLayer est \"USD de ce n\u0153ud\" C'est pourquoi. Donc, en regardant cela, vous pouvez voir ce que fait ce n\u0153ud maintenant et ce qu'il fait en termes d'USD Tu peux le v\u00e9rifier. Donc, d'apr\u00e8s ce que je peux voir, vous pouvez voir qu'il est lu par r\u00e9f\u00e9rence.","title":"Ouvrir avec r\u00e9f\u00e9rence"},{"location":"solaris/#diff\u00e9rence-de-lecture-avec-le-n\u0153ud-de-r\u00e9f\u00e9rence--20200105-ajout\u00e9","text":"Il a \u00e9t\u00e9 trouv\u00e9 plus tard lors de l'examen du comportement d\u00e9taill\u00e9 du n\u0153ud Lorsqu'il est charg\u00e9 \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence et apr\u00e8s le chargement \u00e0 l'aide d'un LoadLayer J'ai trouv\u00e9 qu'il y a une grande diff\u00e9rence entre les n\u0153uds de connexion et de r\u00e9f\u00e9rence. La \"diff\u00e9rence\" est si encapsuler ou non . Lorsqu'il est charg\u00e9 avec un n\u0153ud de r\u00e9f\u00e9rence, lorsqu'il est visualis\u00e9 avec l'USDA pr\u00e9fixer les r\u00e9f\u00e9rences = @C :/pyEnv/JupyterUSD_py27/usd/Kitchen_set/Kitchen_set.usd@ En sp\u00e9cifiant le chemin du fichier comme celui-ci, le prim sp\u00e9cifi\u00e9 par USDA DefaultPrim ou SdfPath du chemin sp\u00e9cifi\u00e9 sera charg\u00e9 par r\u00e9f\u00e9rence.","title":"Diff\u00e9rence de lecture avec le n\u0153ud de r\u00e9f\u00e9rence * 2020/01/05 ajout\u00e9"},{"location":"solaris/#ouvrir-avec-stagemanager","text":"Enfin, comment ouvrir autre que l'arc de composition. C'est le noeud StageManager. Il existe de nombreux exemples d'utilisation de ce StageManager dans les vid\u00e9os de didacticiel Quelle est la diff\u00e9rence entre les trois ci-dessus et ce StageManager ? Comme la grande diff\u00e9rence est le gestionnaire \"Stage\", ce n\u0153ud est Vous pouvez construire une sc\u00e8ne par elle-m\u00eame. Une \u00e9tape dans ce cas fait r\u00e9f\u00e9rence au r\u00e9sultat de la composition de plusieurs fichiers USD. Une fois ouvert, l'\u00e9cran StageManager ressemblera \u00e0 la fen\u00eatre de l'image ci-dessus. Cliquez sur l'ic\u00f4ne du dossier qu'il contient. Jusqu'\u00e0 pr\u00e9sent, les n\u0153uds \u00e9taient principalement ceux qui lisaient un fichier USD avec un n\u0153ud Ce StageManager construit une sc\u00e8ne (un graphe de sc\u00e8ne r\u00e9sultant de la composition USD) par lui-m\u00eame Parce que c'est un n\u0153ud qui peut le faire, chargez plusieurs USD avec des r\u00e9f\u00e9rences, etc. Vous pouvez construire une \u00e9tape tout en regroupant. Ainsi, pour ce n\u0153ud StageManager seul, le ScenerGraphPath est Une \u00e9tape termin\u00e9e est construite. (Orange est charg\u00e9 par l'arc de composition)","title":"Ouvrir avec StageManager"},{"location":"solaris/#\u00e9crivez-le-fichier-ascii-tel-quel-et-ouvrez-le","text":"Peu de gens le font, mais avec inlineusd, Vous pouvez \u00e9crire et lire des fichiers USD \u00e0 la main au lieu de fichiers. En \u00e9crivant en ASCII pour USD Source comme ceci, Vous pouvez r\u00e9ellement cr\u00e9er une sc\u00e8ne dans SceneGraphPath comme celle-ci.","title":"\u00c9crivez le fichier ASCII tel quel et ouvrez-le"},{"location":"solaris/#alors-comment-puis-je-louvrir----","text":"Comme vous pouvez le voir, il existe de nombreuses fa\u00e7ons d'ouvrir un fichier USD. Je ne sais pas quoi faire avec \u00e7a ! Pr\u00e9sentation de ma propre recommandation pour ceux qui disent.","title":"Alors, comment puis-je l'ouvrir ! ! ? ?"},{"location":"solaris/#stagemnaager-si-vous-voulez-placer-beaucoup-dobjets","text":"Si vous voulez faire des mises en page dans la sc\u00e8ne, vous pourriez vous retrouver avec des centaines d'\u00e9l\u00e9ments. Vous devrez le charger et le placer. Dans un tel cas, il est tr\u00e8s g\u00eanant de les lire un par un ou de les regrouper. Dans ce cas, si vous utilisez StageManager, vous pouvez charger tous ensemble par r\u00e9f\u00e9rence Le chargement de plusieurs actifs est facile avec le glisser-d\u00e9poser Vous pouvez facilement effectuer des op\u00e9rations telles que le regroupement et la modification de la hi\u00e9rarchie. En d'autres termes, la construction de la sc\u00e8ne est termin\u00e9e avec ce seul n\u0153ud. Par cons\u00e9quent, ce n\u0153ud est recommand\u00e9 pour les mises en page. Bien s\u00fbr, si vous mettez tout cela ensemble dans un seul StageManager, cela deviendra d\u00e9sordonn\u00e9 ! ! ! Dans ce cas, vous pouvez \u00e9galement connecter plusieurs StageManagers. Dans ce cas, ajoutez des assets avec + \u03b1 au graphe de sc\u00e8ne de la sc\u00e8ne re\u00e7u en entr\u00e9e devient possible.","title":"\"StageMnaager\" si vous voulez placer beaucoup d'objets"},{"location":"solaris/#reference-si-vous-souhaitez-encapsuler-une-couche","text":"DefaultPrim de la couche que vous souhaitez lire par r\u00e9f\u00e9rence ou autre que la prim sp\u00e9cifi\u00e9e Si vous souhaitez \"l'encapsuler\", chargez-le \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence. Lorsqu'elles sont lues avec le n\u0153ud de r\u00e9f\u00e9rence, toutes les prims autres que la prim sp\u00e9cifi\u00e9e sont masqu\u00e9es. Il ne peut plus \u00eatre remplac\u00e9. Donc, si vous voulez lire un caract\u00e8re, etc. Le chargement \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence semble bon.","title":"\"Reference\" si vous souhaitez encapsuler une couche"},{"location":"solaris/#sinon-loadlayer","text":"En dehors de cela, LoadLayer + n\u0153ud d'arc de composition ou autre n\u0153ud pratique (Graft, etc.) est recommand\u00e9. Apr\u00e8s cela, je veux construire une structure bas\u00e9e sur la base de n\u0153uds de SOLARIS pour le mod\u00e8le lu par LoadLayer (Tout en combinant des n\u0153uds l\u00e9gers qui souhaitent cr\u00e9er des variations avec des variantes et attribuer des shaders Je veux construire une sc\u00e8ne\u2026 etc. Vous pouvez lire le fichier directement avec SubLayer ou Reference Dans ce cas, il est tr\u00e8s difficile de remplacer l'ordre d'entr\u00e9e ou de le d\u00e9sactiver temporairement. Je l'ai trouv\u00e9 difficile \u00e0 utiliser car la listabilit\u00e9 au niveau du n\u0153ud n'\u00e9tait pas si bonne. Si vous lisez et composez toujours avec LoadLayer, Il est facile de basculer et clarifie \"lecture d'un fichier\" Personnellement, je pense qu'il est pr\u00e9f\u00e9rable d'utiliser LoadLayer pour le chargement unique de fichiers USD.","title":"sinon \"LoadLayer\""},{"location":"solaris/#bien-s\u00fbr-vous-pouvez-utiliser-les-deux","text":"Bien entendu, les n\u0153uds d'arc de composition StageMnaager et LoadLayer + peuvent \u00eatre utilis\u00e9s ensemble. Lors de la construction collective d'une sc\u00e8ne avec StageManager, faites des variations d\u00e9taill\u00e9es avec des n\u0153uds Enfin, synth\u00e9tisez En utilisant diff\u00e9rentes mani\u00e8res d'ouvrir plusieurs USD dans le bon sens Il est \u00e9galement possible de construire le plus simplement possible des n\u0153uds dans une sc\u00e8ne complexe.","title":"Bien s\u00fbr, vous pouvez utiliser les deux"},{"location":"solaris/#sommaire","text":"Il s'agissait d'une introduction \u00e0 l'ouverture de fichiers USD dans SOLARIS/LOP. Il est \u00e9tonnant qu'il existe autant d'approches diff\u00e9rentes simplement en ouvrant un seul fichier USD. Vous pouvez \u00e9galement ouvrir des sc\u00e8nes + SOP avec LOP et ainsi de suite. SOLARIS est tr\u00e8s flexible lorsqu'il s'agit d'USD et permet la cr\u00e9ation de divers USD en fonction de la fa\u00e7on de penser. Je pensais que c'\u00e9tait un outil. Je me demande si ce genre d'article est la premi\u00e8re \u00e9tape pour \u00e9crire beaucoup d'articles SOLARIS ? Ensuite, il s'agit du n\u0153ud d'arc de composition que j'ai \u00e9crit bri\u00e8vement cette fois J'aimerais \u00e9crire un peu plus \u00e0 ce sujet.","title":"sommaire"},{"location":"C/Cmake/","text":"Info CMake is an open-source, cross-platform family of tools designed to build, test and package software. CMake is used to control the software compilation process using simple platform and compiler independent configuration files, and generate native makefiles and workspaces that can be used in the compiler environment of your choice. The suite of CMake tools were created by Kitware in response to the need for a powerful, cross-platform build environment for open-source projects such as ITK and VTK. CMake is part of Kitware\u2019s collection of commercially supported open-source platforms for software development.","title":"CMake"},{"location":"C/Math101/","text":"MATH basics Negate x*-1 addition (order doesn't matter) 2+4=6 soustraction (order matters!) 2-4=-2 2+(-4) =-2 4*-(1) negate(mult by -1) ex 2* (-1) = -2 -2 * (-1) = 2 ex: when we want to subtract one vector from another,in order to control the order of the vector we instead add them together and negate the one we want to be subtracted. Invert 1/x Similar relationship than addition and substraction Multiplication(order doesn't matter) 2*4=8 Division(order matters ) 2/4 = 0.5 we can represent division in term of multiplication usiong the invert function 2*(0.125) = 0.5 (\u00bc, invert) 1/x Power and Root Power 2 ^ 3= 8 Root 3 squareroot of 8 = 2 same as 8 ^ 0.33 = 2 \u2153 invert Note When we fix our colors (OETF, gamma) we essentialy multiply by 2.2. and when we want toreturnto linear color space, we multiply by the inverse of 2.2","title":"Math101"},{"location":"C/Math101/#math","text":"basics","title":"MATH"},{"location":"C/Math101/#negate-x-1","text":"addition (order doesn't matter) 2+4=6 soustraction (order matters!) 2-4=-2 2+(-4) =-2 4*-(1) negate(mult by -1) ex 2* (-1) = -2 -2 * (-1) = 2 ex: when we want to subtract one vector from another,in order to control the order of the vector we instead add them together and negate the one we want to be subtracted.","title":"Negate x*-1"},{"location":"C/Math101/#invert-1x","text":"Similar relationship than addition and substraction Multiplication(order doesn't matter) 2*4=8 Division(order matters ) 2/4 = 0.5 we can represent division in term of multiplication usiong the invert function 2*(0.125) = 0.5 (\u00bc, invert) 1/x","title":"Invert 1/x"},{"location":"C/Math101/#power-and-root","text":"Power 2 ^ 3= 8 Root 3 squareroot of 8 = 2 same as 8 ^ 0.33 = 2 \u2153 invert Note When we fix our colors (OETF, gamma) we essentialy multiply by 2.2. and when we want toreturnto linear color space, we multiply by the inverse of 2.2","title":"Power and Root"},{"location":"C/hyperV/","text":"Cr\u00e9er un bureau virtuel avec Hyper-V Activation d'Hyper-V Tout d'abord, activez Hyper-V pour pouvoir utiliser des bureaux virtuels. Dans Windows Search, s\u00e9lectionnez Activer ou d\u00e9sactiver les fonctionnalit\u00e9s Windows. 2 S\u00e9lectionnez \"hyper-V\" dans la liste. Apr\u00e8s avoir s\u00e9lectionn\u00e9, Windows sera (probablement) red\u00e9marr\u00e9. Dans la barre de recherche saisissez \"Hyper-V Manager\" et ex\u00e9cutez-le.. Note Si Hyper-V n'est pas activ\u00e9 dans les fonctionnalit\u00e9s Windows, Hyper-V Manager ne s'affichera pas. Cr\u00e9er un pc virtuel S\u00e9lectionnez Actions \u2192 Cliquez sur Cr\u00e9er.... D\u00e9finissez le syst\u00e8me d'exploitation sur Windows 10 DevEnviroment. Une fois que vous l'avez ex\u00e9cut\u00e9, tout ce que vous avez \u00e0 faire est d'attendre et un environnement PC virtuel sera cr\u00e9\u00e9 pour vous. Cliquez sur \"Connecter\" lorsque vous avez termin\u00e9. Setup Tout d'abord, d\u00e9marrez l'environnement virtuel cr\u00e9\u00e9 et avant les param\u00e8tres initiaux de Windows, configurez la mise en r\u00e9seau de votre environnement virtuel. Ex\u00e9cutez Action \u2192 Gestionnaire de commutateur virtuel. S\u00e9lectionnez le nouveau commutateur de r\u00e9seau virtuel, s\u00e9lectionnez \"Externe\" \u2192 Cr\u00e9er un commutateur virtuel. S\u00e9lectionnez le syst\u00e8me d'exploitation de gestion \"R\u00e9seau externe\". Cochez ON et \"Appliquer\". Connectez-vous ensuite \u00e0 la machine virtuelle cr\u00e9\u00e9e. S\u00e9lectionnez Param\u00e8tres de la machine virtuelle pour ouvrir l'\u00e9cran des param\u00e8tres de l'environnement virtuel. Depuis l'adaptateur r\u00e9seau, remplacez le commutateur virtuel par celui cr\u00e9\u00e9 en \u2191. Si ce r\u00e9glage n'a pas \u00e9t\u00e9 effectu\u00e9, le r\u00e9glage LAN d'Internet ne s'affichera pas. Internet n'\u00e9tait pas disponible dans l'environnement virtuel. Cr\u00e9er un point de contr\u00f4le La bonne chose \u00e0 propos des environnements virtuels est que vous pouvez cr\u00e9er des \"points de contr\u00f4le\". Vous pouvez enregistrer les param\u00e8tres actuels de l'environnement virtuel. Comme cette fois, je veux v\u00e9rifier le test de l'outil dans un \u00e9tat compl\u00e8tement nu (autre que mon propre environnement avec certains param\u00e8tres) Donc, cr\u00e9ez un \"\u00e9tat initial\" avec seulement l'installation minimale Je veux revenir \u00e0 ce moment si n\u00e9cessaire. \"Op\u00e9ration\" \u2192 \"Point de contr\u00f4le\" sur l'\u00e9cran de connexion de la machine virtuelle Choisir. Donnez-lui un nom et cliquez sur \"oui\". Un arbre est cr\u00e9\u00e9 pour chaque point de contr\u00f4le. Donc, cr\u00e9ez un point de contr\u00f4le au moment o\u00f9 vous voulez ramifier le travail dans une certaine mesure Ce faisant, vous pouvez retourner tout l'environnement ou changer jusqu'\u00e0 un certain point. Puisqu'il est possible de rebrancher \u00e0 partir de la synchronisation modifi\u00e9e Vous pouvez reproduire la situation dans divers mod\u00e8les et environnements. De plus, lorsque vous cr\u00e9ez un point de contr\u00f4le, non seulement l'environnement de r\u00e9glage du PC \u00c0 ce moment-l\u00e0, \"l'\u00e9tat de l'application en cours d'ex\u00e9cution\" est enregistr\u00e9. Par exemple, m\u00eame si vous cr\u00e9ez un \"point de contr\u00f4le\" avec Maya en cours d'ex\u00e9cution Maya sera sauvegard\u00e9e comme elle a commenc\u00e9. La cr\u00e9ation de l'environnement est maintenant termin\u00e9e. Travailler avec Maya sur une machine virtuelle \u00e9tait comme travailler sur un PC normal. L'atmosph\u00e8re est comme l'utilisation de Maya via un bureau \u00e0 distance. L'environnement Python a \u00e9t\u00e9 s\u00e9par\u00e9 dans une certaine mesure avec pipenv etc. Je voulais tester l'environnement autour de Maya compl\u00e8tement s\u00e9par\u00e9ment, y compris les param\u00e8tres Windows. Il peut \u00eatre assez bon de construire avec une machine virtuelle simple et performante. \u53c2\u8003 https://qiita.com/nomurasan/items/3c58b964943a24751802 https://fereria.github.io/reincarnation_tech/10_Programming/00_Settings/hyper-v/","title":"Hyper-V"},{"location":"C/hyperV/#cr\u00e9er-un-bureau-virtuel-avec-hyper-v","text":"","title":"Cr\u00e9er un bureau virtuel avec Hyper-V"},{"location":"C/hyperV/#activation-dhyper-v","text":"Tout d'abord, activez Hyper-V pour pouvoir utiliser des bureaux virtuels. Dans Windows Search, s\u00e9lectionnez Activer ou d\u00e9sactiver les fonctionnalit\u00e9s Windows. 2 S\u00e9lectionnez \"hyper-V\" dans la liste. Apr\u00e8s avoir s\u00e9lectionn\u00e9, Windows sera (probablement) red\u00e9marr\u00e9. Dans la barre de recherche saisissez \"Hyper-V Manager\" et ex\u00e9cutez-le.. Note Si Hyper-V n'est pas activ\u00e9 dans les fonctionnalit\u00e9s Windows, Hyper-V Manager ne s'affichera pas.","title":"Activation d'Hyper-V"},{"location":"C/hyperV/#cr\u00e9er-un-pc-virtuel","text":"S\u00e9lectionnez Actions \u2192 Cliquez sur Cr\u00e9er.... D\u00e9finissez le syst\u00e8me d'exploitation sur Windows 10 DevEnviroment. Une fois que vous l'avez ex\u00e9cut\u00e9, tout ce que vous avez \u00e0 faire est d'attendre et un environnement PC virtuel sera cr\u00e9\u00e9 pour vous. Cliquez sur \"Connecter\" lorsque vous avez termin\u00e9.","title":"Cr\u00e9er un pc virtuel"},{"location":"C/hyperV/#setup","text":"Tout d'abord, d\u00e9marrez l'environnement virtuel cr\u00e9\u00e9 et avant les param\u00e8tres initiaux de Windows, configurez la mise en r\u00e9seau de votre environnement virtuel. Ex\u00e9cutez Action \u2192 Gestionnaire de commutateur virtuel. S\u00e9lectionnez le nouveau commutateur de r\u00e9seau virtuel, s\u00e9lectionnez \"Externe\" \u2192 Cr\u00e9er un commutateur virtuel. S\u00e9lectionnez le syst\u00e8me d'exploitation de gestion \"R\u00e9seau externe\". Cochez ON et \"Appliquer\". Connectez-vous ensuite \u00e0 la machine virtuelle cr\u00e9\u00e9e. S\u00e9lectionnez Param\u00e8tres de la machine virtuelle pour ouvrir l'\u00e9cran des param\u00e8tres de l'environnement virtuel. Depuis l'adaptateur r\u00e9seau, remplacez le commutateur virtuel par celui cr\u00e9\u00e9 en \u2191. Si ce r\u00e9glage n'a pas \u00e9t\u00e9 effectu\u00e9, le r\u00e9glage LAN d'Internet ne s'affichera pas. Internet n'\u00e9tait pas disponible dans l'environnement virtuel.","title":"Setup"},{"location":"C/hyperV/#cr\u00e9er-un-point-de-contr\u00f4le","text":"La bonne chose \u00e0 propos des environnements virtuels est que vous pouvez cr\u00e9er des \"points de contr\u00f4le\". Vous pouvez enregistrer les param\u00e8tres actuels de l'environnement virtuel. Comme cette fois, je veux v\u00e9rifier le test de l'outil dans un \u00e9tat compl\u00e8tement nu (autre que mon propre environnement avec certains param\u00e8tres) Donc, cr\u00e9ez un \"\u00e9tat initial\" avec seulement l'installation minimale Je veux revenir \u00e0 ce moment si n\u00e9cessaire. \"Op\u00e9ration\" \u2192 \"Point de contr\u00f4le\" sur l'\u00e9cran de connexion de la machine virtuelle Choisir. Donnez-lui un nom et cliquez sur \"oui\". Un arbre est cr\u00e9\u00e9 pour chaque point de contr\u00f4le. Donc, cr\u00e9ez un point de contr\u00f4le au moment o\u00f9 vous voulez ramifier le travail dans une certaine mesure Ce faisant, vous pouvez retourner tout l'environnement ou changer jusqu'\u00e0 un certain point. Puisqu'il est possible de rebrancher \u00e0 partir de la synchronisation modifi\u00e9e Vous pouvez reproduire la situation dans divers mod\u00e8les et environnements. De plus, lorsque vous cr\u00e9ez un point de contr\u00f4le, non seulement l'environnement de r\u00e9glage du PC \u00c0 ce moment-l\u00e0, \"l'\u00e9tat de l'application en cours d'ex\u00e9cution\" est enregistr\u00e9. Par exemple, m\u00eame si vous cr\u00e9ez un \"point de contr\u00f4le\" avec Maya en cours d'ex\u00e9cution Maya sera sauvegard\u00e9e comme elle a commenc\u00e9. La cr\u00e9ation de l'environnement est maintenant termin\u00e9e. Travailler avec Maya sur une machine virtuelle \u00e9tait comme travailler sur un PC normal. L'atmosph\u00e8re est comme l'utilisation de Maya via un bureau \u00e0 distance. L'environnement Python a \u00e9t\u00e9 s\u00e9par\u00e9 dans une certaine mesure avec pipenv etc. Je voulais tester l'environnement autour de Maya compl\u00e8tement s\u00e9par\u00e9ment, y compris les param\u00e8tres Windows. Il peut \u00eatre assez bon de construire avec une machine virtuelle simple et performante.","title":"Cr\u00e9er un point de contr\u00f4le"},{"location":"C/hyperV/#\u53c2\u8003","text":"https://qiita.com/nomurasan/items/3c58b964943a24751802 https://fereria.github.io/reincarnation_tech/10_Programming/00_Settings/hyper-v/","title":"\u53c2\u8003"},{"location":"C/numericsLimits/","text":"Numeric Limits Note Below demonstrates the limits of all the numeric primitive types in c++ and therefore can help in chosing which would work best for certain scernarios. #include <iostream> #include <cstdint> #include <limits> template < typename T > std :: string typeStr (){ std :: string pf ( __PRETTY_FUNCTION__ ); auto tEqualPos = pf . rfind ( \"T = \" ); auto closeBracPos = pf . rfind ( \"]\" ); if ( tEqualPos != std :: string :: npos && closeBracPos != std :: string :: npos && closeBracPos > tEqualPos ){ return pf . substr ( tEqualPos + 4 , closeBracPos - 4 - tEqualPos ); } else { return \"indeterminate\" ; } } int main ( int argc , char * argv []){ std :: cout << \"Unsigned integers (only numbers >= 0)\" << std :: endl ; std :: cout << \" uint8_t low: \" << static_cast < uint16_t > ( std :: numeric_limits < uint8_t >:: lowest ()) << std :: endl ; std :: cout << \" uint8_t min: \" << static_cast < uint16_t > ( std :: numeric_limits < uint8_t >:: min ()) << std :: endl ; std :: cout << \" uint8_t max: \" << static_cast < uint16_t > ( std :: numeric_limits < uint8_t >:: max ()) << std :: endl ; std :: cout << \" sizeof uint8_t: \" << sizeof ( uint8_t ) << std :: endl ; std :: cout << \" actual type of uint8_t: \" << typeStr < uint8_t > () << std :: endl << std :: endl ; std :: cout << \" uint16_t low: \" << std :: numeric_limits < uint16_t >:: lowest () << std :: endl ; std :: cout << \" uint16_t min: \" << std :: numeric_limits < uint16_t >:: min () << std :: endl ; std :: cout << \" uint16_t max: \" << std :: numeric_limits < uint16_t >:: max () << std :: endl ; std :: cout << \" sizeof uint16_t: \" << sizeof ( uint16_t ) << std :: endl ; std :: cout << \" actual type of uint16_t: \" << typeStr < uint16_t > () << std :: endl << std :: endl ; std :: cout << \" uint32_t low: \" << std :: numeric_limits < uint32_t >:: lowest () << std :: endl ; std :: cout << \" uint32_t min: \" << std :: numeric_limits < uint32_t >:: min () << std :: endl ; std :: cout << \" uint32_t max: \" << std :: numeric_limits < uint32_t >:: max () << std :: endl ; std :: cout << \" sizeof uint32_t: \" << sizeof ( uint32_t ) << std :: endl ; std :: cout << \" actual type of uint32_t: \" << typeStr < uint32_t > () << std :: endl << std :: endl ; std :: cout << \" uint min: \" << std :: numeric_limits < uint >:: lowest () << std :: endl ; std :: cout << \" uint min: \" << std :: numeric_limits < uint >:: min () << std :: endl ; std :: cout << \" uint max: \" << std :: numeric_limits < uint >:: max () << std :: endl ; std :: cout << \" sizeof uint: \" << sizeof ( uint ) << std :: endl ; std :: cout << \" actual type of uint: \" << typeStr < uint > () << std :: endl << std :: endl ; std :: cout << \" uint64_t low: \" << std :: numeric_limits < uint64_t >:: lowest () << std :: endl ; std :: cout << \" uint64_t min: \" << std :: numeric_limits < uint64_t >:: min () << std :: endl ; std :: cout << \" uint64_t max: \" << std :: numeric_limits < uint64_t >:: max () << std :: endl ; std :: cout << \" sizeof uint64_t: \" << sizeof ( uint64_t ) << std :: endl ; std :: cout << \" actual type of uint64_t: \" << typeStr < uint64_t > () << std :: endl << std :: endl ; std :: cout << \" size_t low: \" << std :: numeric_limits < size_t >:: lowest () << std :: endl ; std :: cout << \" size_t min: \" << std :: numeric_limits < size_t >:: min () << std :: endl ; std :: cout << \" size_t max: \" << std :: numeric_limits < size_t >:: max () << std :: endl ; std :: cout << \" sizeof size_t: \" << sizeof ( size_t ) << std :: endl ; std :: cout << \" actual type of size_t: \" << typeStr < size_t > () << std :: endl << std :: endl ; std :: cout << \"Signed Integers\" << std :: endl ; std :: cout << \" int8_t low: \" << static_cast < int16_t > ( std :: numeric_limits < int8_t >:: lowest ()) << std :: endl ; std :: cout << \" int8_t min: \" << static_cast < int16_t > ( std :: numeric_limits < int8_t >:: min ()) << std :: endl ; std :: cout << \" int8_t max: \" << static_cast < int16_t > ( std :: numeric_limits < int8_t >:: max ()) << std :: endl ; std :: cout << \" sizeof int8_t: \" << sizeof ( int8_t ) << std :: endl ; std :: cout << \" actual type of int8_t: \" << typeStr < int8_t > () << std :: endl << std :: endl ; std :: cout << \" int16_t low: \" << std :: numeric_limits < int16_t >:: lowest () << std :: endl ; std :: cout << \" int16_t min: \" << std :: numeric_limits < int16_t >:: min () << std :: endl ; std :: cout << \" int16_t max: \" << std :: numeric_limits < int16_t >:: max () << std :: endl ; std :: cout << \" sizeof int16_t: \" << sizeof ( int16_t ) << std :: endl ; std :: cout << \" actual type of int16_t: \" << typeStr < int16_t > () << std :: endl << std :: endl ; std :: cout << \" int32_t min: \" << std :: numeric_limits < int32_t >:: lowest () << std :: endl ; std :: cout << \" int32_t min: \" << std :: numeric_limits < int32_t >:: min () << std :: endl ; std :: cout << \" int32_t max: \" << std :: numeric_limits < int32_t >:: max () << std :: endl ; std :: cout << \" sizeof int32_t: \" << sizeof ( int32_t ) << std :: endl ; std :: cout << \" actual type of int32_t: \" << typeStr < int32_t > () << std :: endl << std :: endl ; std :: cout << \" int min: \" << std :: numeric_limits < int >:: lowest () << std :: endl ; std :: cout << \" int min: \" << std :: numeric_limits < int >:: min () << std :: endl ; std :: cout << \" int max: \" << std :: numeric_limits < int >:: max () << std :: endl ; std :: cout << \" sizeof int: \" << sizeof ( int ) << std :: endl ; std :: cout << \" actual type of int: \" << typeStr < int > () << std :: endl << std :: endl ; std :: cout << \" int64_t low: \" << std :: numeric_limits < int64_t >:: lowest () << std :: endl ; std :: cout << \" int64_t min: \" << std :: numeric_limits < int64_t >:: min () << std :: endl ; std :: cout << \" int64_t max: \" << std :: numeric_limits < int64_t >:: max () << std :: endl ; std :: cout << \" sizeof int64_t: \" << sizeof ( int64_t ) << std :: endl ; std :: cout << \" actual type of int64_t: \" << typeStr < int64_t > () << std :: endl << std :: endl ; std :: cout << \"Floating Point Numbers\" << std :: endl ; std :: cout << \" float low: \" << std :: numeric_limits < float >:: lowest () << std :: endl ; std :: cout << \" float min: \" << std :: numeric_limits < float >:: min () << std :: endl ; std :: cout << \" float max: \" << std :: numeric_limits < float >:: max () << std :: endl ; std :: cout << \" sizeof float: \" << sizeof ( float ) << std :: endl ; std :: cout << \" actual type of float: \" << typeStr < float > () << std :: endl << std :: endl ; std :: cout << \" double low: \" << std :: numeric_limits < double >:: lowest () << std :: endl ; std :: cout << \" double min: \" << std :: numeric_limits < double >:: min () << std :: endl ; std :: cout << \" double max: \" << std :: numeric_limits < double >:: max () << std :: endl ; std :: cout << \" sizeof double: \" << sizeof ( double ) << std :: endl ; std :: cout << \" actual type of double: \" << typeStr < double > () << std :: endl << std :: endl ; std :: cout << \" long double low: \" << std :: numeric_limits < long double >:: lowest () << std :: endl ; std :: cout << \" long double min: \" << std :: numeric_limits < long double >:: min () << std :: endl ; std :: cout << \" long double max: \" << std :: numeric_limits < long double >:: max () << std :: endl ; std :: cout << \" sizeof long double: \" << sizeof ( long double ) << std :: endl ; std :: cout << \"actual type of long double: \" << typeStr < long double > () << std :: endl << std :: endl ; return 0 ; } Compile Command: g++-5 -std = c++14 main.cpp -o exampleCpp Execution Call: ./exampleCpp Output: Unsigned integers ( only numbers > = 0 ) uint8_t low: 0 uint8_t min: 0 uint8_t max: 255 sizeof uint8_t: 1 actual type of uint8_t: unsigned char ; std::string = std::basic_string<char> uint16_t low: 0 uint16_t min: 0 uint16_t max: 65535 sizeof uint16_t: 2 actual type of uint16_t: short unsigned int ; std::string = std::basic_string<char> uint32_t low: 0 uint32_t min: 0 uint32_t max: 4294967295 sizeof uint32_t: 4 actual type of uint32_t: unsigned int ; std::string = std::basic_string<char> uint min: 0 uint min: 0 uint max: 4294967295 sizeof uint: 4 actual type of uint: unsigned int ; std::string = std::basic_string<char> uint64_t low: 0 uint64_t min: 0 uint64_t max: 18446744073709551615 sizeof uint64_t: 8 actual type of uint64_t: long unsigned int ; std::string = std::basic_string<char> size_t low: 0 size_t min: 0 size_t max: 18446744073709551615 sizeof size_t: 8 actual type of size_t: long unsigned int ; std::string = std::basic_string<char> Signed Integers int8_t low: -128 int8_t min: -128 int8_t max: 127 sizeof int8_t: 1 actual type of int8_t: signed char ; std::string = std::basic_string<char> int16_t low: -32768 int16_t min: -32768 int16_t max: 32767 sizeof int16_t: 2 actual type of int16_t: short int ; std::string = std::basic_string<char> int32_t min: -2147483648 int32_t min: -2147483648 int32_t max: 2147483647 sizeof int32_t: 4 actual type of int32_t: int ; std::string = std::basic_string<char> int min: -2147483648 int min: -2147483648 int max: 2147483647 sizeof int: 4 actual type of int: int ; std::string = std::basic_string<char> int64_t low: -9223372036854775808 int64_t min: -9223372036854775808 int64_t max: 9223372036854775807 sizeof int64_t: 8 actual type of int64_t: long int ; std::string = std::basic_string<char> Floating Point Numbers float low: -3.40282e+38 float min: 1 .17549e-38 float max: 3 .40282e+38 sizeof float: 4 actual type of float: float ; std::string = std::basic_string<char> double low: -1.79769e+308 double min: 2 .22507e-308 double max: 1 .79769e+308 sizeof double: 8 actual type of double: double ; std::string = std::basic_string<char> long double low: -1.18973e+4932 long double min: 3 .3621e-4932 long double max: 1 .18973e+4932 sizeof long double: 16 actual type of long double: long double ; std::string = std::basic_string<char>","title":"Numerics limit"},{"location":"C/numericsLimits/#numeric-limits","text":"Note Below demonstrates the limits of all the numeric primitive types in c++ and therefore can help in chosing which would work best for certain scernarios. #include <iostream> #include <cstdint> #include <limits> template < typename T > std :: string typeStr (){ std :: string pf ( __PRETTY_FUNCTION__ ); auto tEqualPos = pf . rfind ( \"T = \" ); auto closeBracPos = pf . rfind ( \"]\" ); if ( tEqualPos != std :: string :: npos && closeBracPos != std :: string :: npos && closeBracPos > tEqualPos ){ return pf . substr ( tEqualPos + 4 , closeBracPos - 4 - tEqualPos ); } else { return \"indeterminate\" ; } } int main ( int argc , char * argv []){ std :: cout << \"Unsigned integers (only numbers >= 0)\" << std :: endl ; std :: cout << \" uint8_t low: \" << static_cast < uint16_t > ( std :: numeric_limits < uint8_t >:: lowest ()) << std :: endl ; std :: cout << \" uint8_t min: \" << static_cast < uint16_t > ( std :: numeric_limits < uint8_t >:: min ()) << std :: endl ; std :: cout << \" uint8_t max: \" << static_cast < uint16_t > ( std :: numeric_limits < uint8_t >:: max ()) << std :: endl ; std :: cout << \" sizeof uint8_t: \" << sizeof ( uint8_t ) << std :: endl ; std :: cout << \" actual type of uint8_t: \" << typeStr < uint8_t > () << std :: endl << std :: endl ; std :: cout << \" uint16_t low: \" << std :: numeric_limits < uint16_t >:: lowest () << std :: endl ; std :: cout << \" uint16_t min: \" << std :: numeric_limits < uint16_t >:: min () << std :: endl ; std :: cout << \" uint16_t max: \" << std :: numeric_limits < uint16_t >:: max () << std :: endl ; std :: cout << \" sizeof uint16_t: \" << sizeof ( uint16_t ) << std :: endl ; std :: cout << \" actual type of uint16_t: \" << typeStr < uint16_t > () << std :: endl << std :: endl ; std :: cout << \" uint32_t low: \" << std :: numeric_limits < uint32_t >:: lowest () << std :: endl ; std :: cout << \" uint32_t min: \" << std :: numeric_limits < uint32_t >:: min () << std :: endl ; std :: cout << \" uint32_t max: \" << std :: numeric_limits < uint32_t >:: max () << std :: endl ; std :: cout << \" sizeof uint32_t: \" << sizeof ( uint32_t ) << std :: endl ; std :: cout << \" actual type of uint32_t: \" << typeStr < uint32_t > () << std :: endl << std :: endl ; std :: cout << \" uint min: \" << std :: numeric_limits < uint >:: lowest () << std :: endl ; std :: cout << \" uint min: \" << std :: numeric_limits < uint >:: min () << std :: endl ; std :: cout << \" uint max: \" << std :: numeric_limits < uint >:: max () << std :: endl ; std :: cout << \" sizeof uint: \" << sizeof ( uint ) << std :: endl ; std :: cout << \" actual type of uint: \" << typeStr < uint > () << std :: endl << std :: endl ; std :: cout << \" uint64_t low: \" << std :: numeric_limits < uint64_t >:: lowest () << std :: endl ; std :: cout << \" uint64_t min: \" << std :: numeric_limits < uint64_t >:: min () << std :: endl ; std :: cout << \" uint64_t max: \" << std :: numeric_limits < uint64_t >:: max () << std :: endl ; std :: cout << \" sizeof uint64_t: \" << sizeof ( uint64_t ) << std :: endl ; std :: cout << \" actual type of uint64_t: \" << typeStr < uint64_t > () << std :: endl << std :: endl ; std :: cout << \" size_t low: \" << std :: numeric_limits < size_t >:: lowest () << std :: endl ; std :: cout << \" size_t min: \" << std :: numeric_limits < size_t >:: min () << std :: endl ; std :: cout << \" size_t max: \" << std :: numeric_limits < size_t >:: max () << std :: endl ; std :: cout << \" sizeof size_t: \" << sizeof ( size_t ) << std :: endl ; std :: cout << \" actual type of size_t: \" << typeStr < size_t > () << std :: endl << std :: endl ; std :: cout << \"Signed Integers\" << std :: endl ; std :: cout << \" int8_t low: \" << static_cast < int16_t > ( std :: numeric_limits < int8_t >:: lowest ()) << std :: endl ; std :: cout << \" int8_t min: \" << static_cast < int16_t > ( std :: numeric_limits < int8_t >:: min ()) << std :: endl ; std :: cout << \" int8_t max: \" << static_cast < int16_t > ( std :: numeric_limits < int8_t >:: max ()) << std :: endl ; std :: cout << \" sizeof int8_t: \" << sizeof ( int8_t ) << std :: endl ; std :: cout << \" actual type of int8_t: \" << typeStr < int8_t > () << std :: endl << std :: endl ; std :: cout << \" int16_t low: \" << std :: numeric_limits < int16_t >:: lowest () << std :: endl ; std :: cout << \" int16_t min: \" << std :: numeric_limits < int16_t >:: min () << std :: endl ; std :: cout << \" int16_t max: \" << std :: numeric_limits < int16_t >:: max () << std :: endl ; std :: cout << \" sizeof int16_t: \" << sizeof ( int16_t ) << std :: endl ; std :: cout << \" actual type of int16_t: \" << typeStr < int16_t > () << std :: endl << std :: endl ; std :: cout << \" int32_t min: \" << std :: numeric_limits < int32_t >:: lowest () << std :: endl ; std :: cout << \" int32_t min: \" << std :: numeric_limits < int32_t >:: min () << std :: endl ; std :: cout << \" int32_t max: \" << std :: numeric_limits < int32_t >:: max () << std :: endl ; std :: cout << \" sizeof int32_t: \" << sizeof ( int32_t ) << std :: endl ; std :: cout << \" actual type of int32_t: \" << typeStr < int32_t > () << std :: endl << std :: endl ; std :: cout << \" int min: \" << std :: numeric_limits < int >:: lowest () << std :: endl ; std :: cout << \" int min: \" << std :: numeric_limits < int >:: min () << std :: endl ; std :: cout << \" int max: \" << std :: numeric_limits < int >:: max () << std :: endl ; std :: cout << \" sizeof int: \" << sizeof ( int ) << std :: endl ; std :: cout << \" actual type of int: \" << typeStr < int > () << std :: endl << std :: endl ; std :: cout << \" int64_t low: \" << std :: numeric_limits < int64_t >:: lowest () << std :: endl ; std :: cout << \" int64_t min: \" << std :: numeric_limits < int64_t >:: min () << std :: endl ; std :: cout << \" int64_t max: \" << std :: numeric_limits < int64_t >:: max () << std :: endl ; std :: cout << \" sizeof int64_t: \" << sizeof ( int64_t ) << std :: endl ; std :: cout << \" actual type of int64_t: \" << typeStr < int64_t > () << std :: endl << std :: endl ; std :: cout << \"Floating Point Numbers\" << std :: endl ; std :: cout << \" float low: \" << std :: numeric_limits < float >:: lowest () << std :: endl ; std :: cout << \" float min: \" << std :: numeric_limits < float >:: min () << std :: endl ; std :: cout << \" float max: \" << std :: numeric_limits < float >:: max () << std :: endl ; std :: cout << \" sizeof float: \" << sizeof ( float ) << std :: endl ; std :: cout << \" actual type of float: \" << typeStr < float > () << std :: endl << std :: endl ; std :: cout << \" double low: \" << std :: numeric_limits < double >:: lowest () << std :: endl ; std :: cout << \" double min: \" << std :: numeric_limits < double >:: min () << std :: endl ; std :: cout << \" double max: \" << std :: numeric_limits < double >:: max () << std :: endl ; std :: cout << \" sizeof double: \" << sizeof ( double ) << std :: endl ; std :: cout << \" actual type of double: \" << typeStr < double > () << std :: endl << std :: endl ; std :: cout << \" long double low: \" << std :: numeric_limits < long double >:: lowest () << std :: endl ; std :: cout << \" long double min: \" << std :: numeric_limits < long double >:: min () << std :: endl ; std :: cout << \" long double max: \" << std :: numeric_limits < long double >:: max () << std :: endl ; std :: cout << \" sizeof long double: \" << sizeof ( long double ) << std :: endl ; std :: cout << \"actual type of long double: \" << typeStr < long double > () << std :: endl << std :: endl ; return 0 ; } Compile Command: g++-5 -std = c++14 main.cpp -o exampleCpp Execution Call: ./exampleCpp Output: Unsigned integers ( only numbers > = 0 ) uint8_t low: 0 uint8_t min: 0 uint8_t max: 255 sizeof uint8_t: 1 actual type of uint8_t: unsigned char ; std::string = std::basic_string<char> uint16_t low: 0 uint16_t min: 0 uint16_t max: 65535 sizeof uint16_t: 2 actual type of uint16_t: short unsigned int ; std::string = std::basic_string<char> uint32_t low: 0 uint32_t min: 0 uint32_t max: 4294967295 sizeof uint32_t: 4 actual type of uint32_t: unsigned int ; std::string = std::basic_string<char> uint min: 0 uint min: 0 uint max: 4294967295 sizeof uint: 4 actual type of uint: unsigned int ; std::string = std::basic_string<char> uint64_t low: 0 uint64_t min: 0 uint64_t max: 18446744073709551615 sizeof uint64_t: 8 actual type of uint64_t: long unsigned int ; std::string = std::basic_string<char> size_t low: 0 size_t min: 0 size_t max: 18446744073709551615 sizeof size_t: 8 actual type of size_t: long unsigned int ; std::string = std::basic_string<char> Signed Integers int8_t low: -128 int8_t min: -128 int8_t max: 127 sizeof int8_t: 1 actual type of int8_t: signed char ; std::string = std::basic_string<char> int16_t low: -32768 int16_t min: -32768 int16_t max: 32767 sizeof int16_t: 2 actual type of int16_t: short int ; std::string = std::basic_string<char> int32_t min: -2147483648 int32_t min: -2147483648 int32_t max: 2147483647 sizeof int32_t: 4 actual type of int32_t: int ; std::string = std::basic_string<char> int min: -2147483648 int min: -2147483648 int max: 2147483647 sizeof int: 4 actual type of int: int ; std::string = std::basic_string<char> int64_t low: -9223372036854775808 int64_t min: -9223372036854775808 int64_t max: 9223372036854775807 sizeof int64_t: 8 actual type of int64_t: long int ; std::string = std::basic_string<char> Floating Point Numbers float low: -3.40282e+38 float min: 1 .17549e-38 float max: 3 .40282e+38 sizeof float: 4 actual type of float: float ; std::string = std::basic_string<char> double low: -1.79769e+308 double min: 2 .22507e-308 double max: 1 .79769e+308 sizeof double: 8 actual type of double: double ; std::string = std::basic_string<char> long double low: -1.18973e+4932 long double min: 3 .3621e-4932 long double max: 1 .18973e+4932 sizeof long double: 16 actual type of long double: long double ; std::string = std::basic_string<char>","title":"Numeric Limits"},{"location":"C/vcpkg/","text":"Get started with vcpkg visit vcpkg Info vcpkg is a free C/C++ package manager for acquiring and managing libraries. Choose from over 1500 open source libraries to download and build in a single step or add your own private libraries to simplify your build process. Maintained by the Microsoft C++ team and open source contributors. Install vcpkg Installing vcpkg is a two-step process: first, clone the repo, then run the bootstrapping script to produce the vcpkg binary. The repo can be cloned anywhere, and will include the vcpkg binary after bootstrapping as well as any libraries that are installed from the command line. It is recommended to clone vcpkg as a submodule for CMake projects, but to install it globally for MSBuild projects. If installing globally, we recommend a short install path like: sh C:\\src\\vcpkg or sh C:\\dev\\vcpkg , since otherwise you may run into path issues for some port build systems. Step 1: Clone the vcpkg repo git clone https://github.com/Microsoft/vcpkg.git Make sure you are in the directory you want the tool installed to before doing this. Step 2: Run the bootstrap script to build vcpkg . \\v cpkg \\b ootstrap-vcpkg.bat Install libraries for your project vcpkg install [ packages to install ] Using vcpkg with MSBuild / Visual Studio (may require elevation) vcpkg integrate install After this, you can create a new project or open an existing one in the IDE. All installed libraries should already be discoverable by IntelliSense and usable in code without additional configuration. Using vcpkg with CMake In order to use vcpkg with CMake outside of an IDE, you can use the toolchain file: cmake -B [ build directory ] -S . -DCMAKE_TOOLCHAIN_FILE =[ path to vcpkg ] /scripts/buildsystems/vcpkg.cmake Then build with: cmake --build [ build directory ] With CMake, you will need to use find_package() to reference the libraries in your Cmakelists.txt files.","title":"Vcpkg"},{"location":"C/vcpkg/#get-started-with-vcpkg","text":"visit vcpkg Info vcpkg is a free C/C++ package manager for acquiring and managing libraries. Choose from over 1500 open source libraries to download and build in a single step or add your own private libraries to simplify your build process. Maintained by the Microsoft C++ team and open source contributors.","title":"Get started with vcpkg"},{"location":"C/vcpkg/#install-vcpkg","text":"Installing vcpkg is a two-step process: first, clone the repo, then run the bootstrapping script to produce the vcpkg binary. The repo can be cloned anywhere, and will include the vcpkg binary after bootstrapping as well as any libraries that are installed from the command line. It is recommended to clone vcpkg as a submodule for CMake projects, but to install it globally for MSBuild projects. If installing globally, we recommend a short install path like: sh C:\\src\\vcpkg or sh C:\\dev\\vcpkg , since otherwise you may run into path issues for some port build systems. Step 1: Clone the vcpkg repo git clone https://github.com/Microsoft/vcpkg.git Make sure you are in the directory you want the tool installed to before doing this. Step 2: Run the bootstrap script to build vcpkg . \\v cpkg \\b ootstrap-vcpkg.bat Install libraries for your project vcpkg install [ packages to install ] Using vcpkg with MSBuild / Visual Studio (may require elevation) vcpkg integrate install After this, you can create a new project or open an existing one in the IDE. All installed libraries should already be discoverable by IntelliSense and usable in code without additional configuration. Using vcpkg with CMake In order to use vcpkg with CMake outside of an IDE, you can use the toolchain file: cmake -B [ build directory ] -S . -DCMAKE_TOOLCHAIN_FILE =[ path to vcpkg ] /scripts/buildsystems/vcpkg.cmake Then build with: cmake --build [ build directory ] With CMake, you will need to use find_package() to reference the libraries in your Cmakelists.txt files.","title":"Install vcpkg"},{"location":"colors/ACES_DCC/","text":"OCIO ACES MAYA The default settings for color management is ACES and the default rendering sapce is ACEScg. You setup color color management in the maya preferences, windows> settings/preferences> preferences. Color management in maya is based on OpenColorIO (OICO). OCIO use a configuration file, usually called \"config.ocio\" in yaml format. this file defines the color spaces and transforms that are available. The same config file can be used by many creaction, compositing and editing software packages. Maya contain two config, the default config for new scenes is based on ACES, the academy color encoding system. The legacy config is used for compatibility in scenes from maya 2020 an earlier. If your scene uses a specific cconfig file, you cans browse and select the instead. ACES HOUDINI Computer programs like Houdini prefer to work in a linear color space internally, which is easy to manipulate digitally. It\u2019s often preferable for outputs, such as final images, printed film, and displays, to be in perceptual color space (gamma corrected/color corrected). Color management in Houdini involves translating between linear and perceptual color spaces at the boundary between Houdini and the outside world at appropriate times. OpenColorIO (OCIO) is an industry-standard open-source library for managing and translating color spaces. OCIO is more powerful and flexible than Houdini\u2019s default gamma and lookup table (LUT) support, and is recommended for professional use. Setup Set the $OCIO environment variable to the file path of an OpenColorIO configuration file (such as config.ocio). The existence of this environment variable controls whether Houdini automatically uses OCIO in various places. You can set the default colorspace using the OCIO_ACTIVE_DISPLAYS and OCIO_ACTIVE_VIEWS environment variables. For sRGB files and sources, the source colorspace used for sRGB is defined by the Houdini environment variable HOUDINI_OCIO_SRGB_FILE_COLORSPACE. If this variable is not defined, the OpenColorIO config file is searched for a colorspace that matches srgb, either fully or partially. You can turn various forms of OCIO support in the Edit \u25b8 Color Settings window. Note The OCIO, OCIO_ACTIVE_VIEWS, and OCIO_ACTIVE_DISPLAYS environment variables are not Houdini environment variables. They cannot be set through the Aliases and Variables dialog, nor can they be set in the houdini.env file. They must be set in the shell or desktop environment before launching Houdini. Image inputs Texture images are assumed to be linear in Houdini and Mantra. If needed, you can manually convert textures using the OCIO Color Transform VOP. If OCIO is configured and you set the File COP to linearize colors, it will use the OCIO file naming convention to deduce the color space of the input image and linearize it correctly. (This will also automatically switch the output to 16 bit to prevent banding.) Render output Renders from Mantra are in linear space. Flipbooks from the viewport are either sRGB or linear, depending on the \u201cRender Beauty Pass Only\u201d option (linear when On, sRGB when off). The image in the Render View can be color corrected using OCIO. The Correction toolbar is stowed by default at the bottom of the view. Click the stowbar just below the view area to show it. When OpenColorIO is active, Houdini replaces default gamma and LUT controls on the Correction toolbar with OCIO controls. The output of the Render Region tool can be color corrected using OCIO. (You can turn this on or off in the Edit \u25b8 Color Settings window.) Color correcting the display When OpenColorIO is active, the Scene View, Render View, and MPlay can use OCIO to color-correct display. When OCIO is active, Houdini replaces default gamma and LUT controls on the Correction toolbar with OCIO controls. In the Scene View, open the Viewport menu (the menu to the left of the Camera menu, in the top left corner of the viewport) and turn on Correction Toolbar to show the color correction toolbar at the bottom of the viewer. In the Render View, the Correction toolbar is stowed by default at the bottom of the view. Click the stowbar just below the view area to show it. When OpenColorIO is active ($OCIO points to a configuration file), gamma and LUT controls in the interface are replaced by Display and View menus. These define the output colorspace for the display. The colorspace within Houdini is linear, and the scene_linear role specifies the OpenColorIO linear colorspace. For example, when OCIO is active, the scene viewer\u2019s Color Correction toolbar (Viewport menu \u25b8 Correction Toolbar) has OCIO controls. MPlay can load both linear and non-linear files, such as OpenEXR (linear) and JPEG (sRGB). ......... ACES 3DSMAX Unfortunatly 3dsmax doesn't support OCIO","title":"Install"},{"location":"colors/ACES_DCC/#ocio","text":"","title":"OCIO"},{"location":"colors/ACES_DCC/#aces-maya","text":"The default settings for color management is ACES and the default rendering sapce is ACEScg. You setup color color management in the maya preferences, windows> settings/preferences> preferences. Color management in maya is based on OpenColorIO (OICO). OCIO use a configuration file, usually called \"config.ocio\" in yaml format. this file defines the color spaces and transforms that are available. The same config file can be used by many creaction, compositing and editing software packages. Maya contain two config, the default config for new scenes is based on ACES, the academy color encoding system. The legacy config is used for compatibility in scenes from maya 2020 an earlier. If your scene uses a specific cconfig file, you cans browse and select the instead.","title":"ACES MAYA"},{"location":"colors/ACES_DCC/#aces-houdini","text":"Computer programs like Houdini prefer to work in a linear color space internally, which is easy to manipulate digitally. It\u2019s often preferable for outputs, such as final images, printed film, and displays, to be in perceptual color space (gamma corrected/color corrected). Color management in Houdini involves translating between linear and perceptual color spaces at the boundary between Houdini and the outside world at appropriate times. OpenColorIO (OCIO) is an industry-standard open-source library for managing and translating color spaces. OCIO is more powerful and flexible than Houdini\u2019s default gamma and lookup table (LUT) support, and is recommended for professional use.","title":"ACES HOUDINI"},{"location":"colors/ACES_DCC/#setup","text":"Set the $OCIO environment variable to the file path of an OpenColorIO configuration file (such as config.ocio). The existence of this environment variable controls whether Houdini automatically uses OCIO in various places. You can set the default colorspace using the OCIO_ACTIVE_DISPLAYS and OCIO_ACTIVE_VIEWS environment variables. For sRGB files and sources, the source colorspace used for sRGB is defined by the Houdini environment variable HOUDINI_OCIO_SRGB_FILE_COLORSPACE. If this variable is not defined, the OpenColorIO config file is searched for a colorspace that matches srgb, either fully or partially. You can turn various forms of OCIO support in the Edit \u25b8 Color Settings window. Note The OCIO, OCIO_ACTIVE_VIEWS, and OCIO_ACTIVE_DISPLAYS environment variables are not Houdini environment variables. They cannot be set through the Aliases and Variables dialog, nor can they be set in the houdini.env file. They must be set in the shell or desktop environment before launching Houdini.","title":"Setup"},{"location":"colors/ACES_DCC/#image-inputs","text":"Texture images are assumed to be linear in Houdini and Mantra. If needed, you can manually convert textures using the OCIO Color Transform VOP. If OCIO is configured and you set the File COP to linearize colors, it will use the OCIO file naming convention to deduce the color space of the input image and linearize it correctly. (This will also automatically switch the output to 16 bit to prevent banding.)","title":"Image inputs"},{"location":"colors/ACES_DCC/#render-output","text":"Renders from Mantra are in linear space. Flipbooks from the viewport are either sRGB or linear, depending on the \u201cRender Beauty Pass Only\u201d option (linear when On, sRGB when off). The image in the Render View can be color corrected using OCIO. The Correction toolbar is stowed by default at the bottom of the view. Click the stowbar just below the view area to show it. When OpenColorIO is active, Houdini replaces default gamma and LUT controls on the Correction toolbar with OCIO controls. The output of the Render Region tool can be color corrected using OCIO. (You can turn this on or off in the Edit \u25b8 Color Settings window.)","title":"Render output"},{"location":"colors/ACES_DCC/#color-correcting-the-display","text":"When OpenColorIO is active, the Scene View, Render View, and MPlay can use OCIO to color-correct display. When OCIO is active, Houdini replaces default gamma and LUT controls on the Correction toolbar with OCIO controls. In the Scene View, open the Viewport menu (the menu to the left of the Camera menu, in the top left corner of the viewport) and turn on Correction Toolbar to show the color correction toolbar at the bottom of the viewer. In the Render View, the Correction toolbar is stowed by default at the bottom of the view. Click the stowbar just below the view area to show it. When OpenColorIO is active ($OCIO points to a configuration file), gamma and LUT controls in the interface are replaced by Display and View menus. These define the output colorspace for the display. The colorspace within Houdini is linear, and the scene_linear role specifies the OpenColorIO linear colorspace. For example, when OCIO is active, the scene viewer\u2019s Color Correction toolbar (Viewport menu \u25b8 Correction Toolbar) has OCIO controls. MPlay can load both linear and non-linear files, such as OpenEXR (linear) and JPEG (sRGB). .........","title":"Color correcting the display"},{"location":"colors/ACES_DCC/#aces-3dsmax","text":"Unfortunatly 3dsmax doesn't support OCIO","title":"ACES 3DSMAX"},{"location":"colors/Aces/","text":"ACES introduction For ACES Technical documentation visit acescentral . What is ACES and Why is it Recommended? The Academy Color Encoding System (ACES) - is a free, expandable, device-independent color management and image sharing system developed under the auspices of the Academy of Motion Picture Arts and Sciences . It is also a set of technical specifications for working with color, coding, and transformation. This system allows you to store all the data of digital images in the same mathematical space, which allows a more consistent workflow during the transfer of working material between different departments. It is also important that ACES allows you to create archival materials with a high dynamic range and wide color gamut, even taking into account possible future devices. Info OpenColorIO Configuration for ACES Github repo OpenColorIO-Config-ACES .","title":"Intro"},{"location":"colors/Aces/#aces-introduction","text":"For ACES Technical documentation visit acescentral .","title":"ACES introduction"},{"location":"colors/Aces/#what-is-aces-and-why-is-it-recommended","text":"The Academy Color Encoding System (ACES) - is a free, expandable, device-independent color management and image sharing system developed under the auspices of the Academy of Motion Picture Arts and Sciences . It is also a set of technical specifications for working with color, coding, and transformation. This system allows you to store all the data of digital images in the same mathematical space, which allows a more consistent workflow during the transfer of working material between different departments. It is also important that ACES allows you to create archival materials with a high dynamic range and wide color gamut, even taking into account possible future devices. Info OpenColorIO Configuration for ACES Github repo OpenColorIO-Config-ACES .","title":"What is ACES and Why is it Recommended? "},{"location":"colors/Aces_cheatSheet/","text":"NOTES ACEScg Conversion Cheat Sheet Note Textures for 3D materials: You need to preserve the look of the colors?: Utility - sRGB - Texture Diffuse SSS Albedo Spec Metallicity You need to preserve the numeric values of you pixels?: Utility - Raw Bump Normal Displacement You need your image to look as though you opened it up in Photoshop?: Output - sRGB Backplates or Compositing Abbreviations Note ACES - Academy Color Encoding System AMPAS - Academy of Motion Picture Arts and Science OCIO - Open Color IO OIIO - Open Image IO LUT - Look Up Table ICC - International Color Consortium ICM - Image Color Management IDT - Input Device Transform ODT - Output Device Transform LMT - Look Modification Transform RRT - Reference Rendering Transform UI - Unsigned Integer OCES - Output Color Encoding Specification CIExy 1931 Color Diagram This is a very common diagram depicting the extent of colors humans can perceive. It is important to note that the diagram does not depict value/intensity only hue and saturation. To explain gamut, primaries, and white points I will be using a CIExy diagram to visualize color-spaces so we can easily see the size of a colorspace and how much of the visual light spectrum can be represented by these colorspaces. gamut A colorspace's gamut is the set of all the colors that can be represented within that colorspace. In the diagram below the gamut is the set of all the colors within the colorspace triangle. Many popular gamuts have been graphed to help illustrate the wide range to pick from. Notice how few colors can actually be represented by sRGB. primaries For RGB images primaries are your reddest reds, greenest greens, and bluest blues of a color gamut. In the diagrams below you can see these are represented by the corners of the gamut triangles. Also note the position of the green primary of the ACEScg colorspace. It falls outside of the visible spectrum of hues (called an imaginary color). It was placed there so as many hues between rend and green could fall within the ACEScg gamut. One consequence of the primary being where it is is you will never use a fully green value by itself. white point The white point is the point within a colorspace we consider to be white. ACEScg and sRGB have different white points. Below the Kelvin temperature scale (and P3) is graphed over all human visible hues (CIE 1931 Chromaticity Diagram). Often you will see a white point of a colorspace refered to by its CIE Standard Luminant designation. These usually start with the letter \"D\" and two numbers. For example ACEScg and sRGB have D60 and D65 white points. These closely translate to a value on the Kelvin scale: D65 is 6500 Kelvin and D60 is 6000 Kelvin. gamma When dealing with gamma you are going to come across the terms linear and gamma curves quite often. ACES is linear but marjority of the time the images we take with our camera, make in photoshop, and download off the internet are not. So, we need to understand what gamma is and how to make a non-linear image linear. Back when computers were slow, drive space was expensive, and memory was small we needed to store our images in as efficient a manner as possible. First of all we could only store our images using 8 bits per color channel which meant each channel only had 256 individual values it could store. It turns out our eyes are more sensitive to small increases of value in darker colors than in lighter colors. We could store our images using a gamma curve to dedicate more of the 256 values in each channell to the darker colors rather than the lighter ones. This meant as a pixel value increased that value would increase by greater and greater amounts. So, to put it another way, going from 0 to 1 would result in a smaller increase in value than an increase from 254 to 255. In a linear image the value increases are uniform. 3D applications prefer a linear image as it makes the math for calculating color and light a lot easier and it makes using high dynamic range images possible. Understanding if your non-ACES images is linear or has a gamma curve is really important to converting it properly. Notice for the non-linear curves the values flip after 1. For an images with a dark gamma values begin to brighten after 1 and the oposite for a bright gamma. This is why we need to work with linear images when dealing with values above 1 (HDR iamges). When doing research on ACES on your own you will often come across the terms scene-referred and display-referred and it is important to understand what they mean and how they relate to your images. scene-referred Scene referred images are linear and are meant to represent real-world light values or light as it actually is. However, they look terrible when displayed raw on a monitor because they don't take into account the characteristics of the display (dynamic range, gamma, etc.). ACES and ACEScg are both Scene-Referred. Scene-referred images have a linear gamma curve. display-referred Display referred images are encoded in a way to make them look good when displayed or has the data encoded in a way affords efficient storage. sRGB, P3, Rec. 709, and Adobe RGB (1998) are all Display-Referred images. Display-Referred images are encoded to be looked at on a specific device (sRGB monitor, Rec. 709 TV, P3 movie screen) or come from a specific camera colorspace (RED DRAGONcolor, ARRI LogC, Sony S-Log, etc.). Display-referred images have a non-linear gamma curve.","title":"Notes"},{"location":"colors/Aces_cheatSheet/#notes","text":"","title":"NOTES"},{"location":"colors/Aces_cheatSheet/#acescg-conversion-cheat-sheet","text":"Note Textures for 3D materials: You need to preserve the look of the colors?: Utility - sRGB - Texture Diffuse SSS Albedo Spec Metallicity You need to preserve the numeric values of you pixels?: Utility - Raw Bump Normal Displacement You need your image to look as though you opened it up in Photoshop?: Output - sRGB Backplates or Compositing","title":"ACEScg Conversion Cheat Sheet"},{"location":"colors/Aces_cheatSheet/#abbreviations","text":"Note ACES - Academy Color Encoding System AMPAS - Academy of Motion Picture Arts and Science OCIO - Open Color IO OIIO - Open Image IO LUT - Look Up Table ICC - International Color Consortium ICM - Image Color Management IDT - Input Device Transform ODT - Output Device Transform LMT - Look Modification Transform RRT - Reference Rendering Transform UI - Unsigned Integer OCES - Output Color Encoding Specification","title":"Abbreviations"},{"location":"colors/Aces_cheatSheet/#ciexy-1931-color-diagram","text":"This is a very common diagram depicting the extent of colors humans can perceive. It is important to note that the diagram does not depict value/intensity only hue and saturation. To explain gamut, primaries, and white points I will be using a CIExy diagram to visualize color-spaces so we can easily see the size of a colorspace and how much of the visual light spectrum can be represented by these colorspaces.","title":"CIExy 1931 Color Diagram"},{"location":"colors/Aces_cheatSheet/#gamut","text":"A colorspace's gamut is the set of all the colors that can be represented within that colorspace. In the diagram below the gamut is the set of all the colors within the colorspace triangle. Many popular gamuts have been graphed to help illustrate the wide range to pick from. Notice how few colors can actually be represented by sRGB.","title":"gamut"},{"location":"colors/Aces_cheatSheet/#primaries","text":"For RGB images primaries are your reddest reds, greenest greens, and bluest blues of a color gamut. In the diagrams below you can see these are represented by the corners of the gamut triangles. Also note the position of the green primary of the ACEScg colorspace. It falls outside of the visible spectrum of hues (called an imaginary color). It was placed there so as many hues between rend and green could fall within the ACEScg gamut. One consequence of the primary being where it is is you will never use a fully green value by itself.","title":"primaries"},{"location":"colors/Aces_cheatSheet/#white-point","text":"The white point is the point within a colorspace we consider to be white. ACEScg and sRGB have different white points. Below the Kelvin temperature scale (and P3) is graphed over all human visible hues (CIE 1931 Chromaticity Diagram). Often you will see a white point of a colorspace refered to by its CIE Standard Luminant designation. These usually start with the letter \"D\" and two numbers. For example ACEScg and sRGB have D60 and D65 white points. These closely translate to a value on the Kelvin scale: D65 is 6500 Kelvin and D60 is 6000 Kelvin.","title":"white point"},{"location":"colors/Aces_cheatSheet/#gamma","text":"When dealing with gamma you are going to come across the terms linear and gamma curves quite often. ACES is linear but marjority of the time the images we take with our camera, make in photoshop, and download off the internet are not. So, we need to understand what gamma is and how to make a non-linear image linear. Back when computers were slow, drive space was expensive, and memory was small we needed to store our images in as efficient a manner as possible. First of all we could only store our images using 8 bits per color channel which meant each channel only had 256 individual values it could store. It turns out our eyes are more sensitive to small increases of value in darker colors than in lighter colors. We could store our images using a gamma curve to dedicate more of the 256 values in each channell to the darker colors rather than the lighter ones. This meant as a pixel value increased that value would increase by greater and greater amounts. So, to put it another way, going from 0 to 1 would result in a smaller increase in value than an increase from 254 to 255. In a linear image the value increases are uniform. 3D applications prefer a linear image as it makes the math for calculating color and light a lot easier and it makes using high dynamic range images possible. Understanding if your non-ACES images is linear or has a gamma curve is really important to converting it properly. Notice for the non-linear curves the values flip after 1. For an images with a dark gamma values begin to brighten after 1 and the oposite for a bright gamma. This is why we need to work with linear images when dealing with values above 1 (HDR iamges). When doing research on ACES on your own you will often come across the terms scene-referred and display-referred and it is important to understand what they mean and how they relate to your images.","title":"gamma"},{"location":"colors/Aces_cheatSheet/#scene-referred","text":"Scene referred images are linear and are meant to represent real-world light values or light as it actually is. However, they look terrible when displayed raw on a monitor because they don't take into account the characteristics of the display (dynamic range, gamma, etc.). ACES and ACEScg are both Scene-Referred. Scene-referred images have a linear gamma curve.","title":"scene-referred"},{"location":"colors/Aces_cheatSheet/#display-referred","text":"Display referred images are encoded in a way to make them look good when displayed or has the data encoded in a way affords efficient storage. sRGB, P3, Rec. 709, and Adobe RGB (1998) are all Display-Referred images. Display-Referred images are encoded to be looked at on a specific device (sRGB monitor, Rec. 709 TV, P3 movie screen) or come from a specific camera colorspace (RED DRAGONcolor, ARRI LogC, Sony S-Log, etc.). Display-referred images have a non-linear gamma curve.","title":"display-referred"},{"location":"colors/Aces_workflow/","text":"ACES Color Pipeline flowchart LR A(Scene Tristimuli) --> B[OECF] B --> C[IDT] C --> D[LMT] D --> E[RRT] E --> F[ODT] F --> G[EOCF] G --> H(Display Tristimuli) C -->|ACES| E style A stroke-width:2px,stroke-dasharray: 5 5 style H stroke-width:2px,stroke-dasharray: 5 5 Tristimulis refers to linear light, code value (from 0 to ) are linearly related to the power in the light that is entering in the camera lens. those numbers are 12bits and typically we don't have the downstream infrastruture that will handle 12 bits well established. OECF Tippically the first step in the color pipeline is an OECF ( opto electronic conversion function ), in the old days we will would've call this GAMMA.Built in the camera, IDT (input device transform) ingesting data map from one linear light color space to another. 3*3 matrix transform Note IDT: The Input Device Transform is the transform used to convert the pixel colors of images/videos from specific devices into an ACES colorspace. RRT: The Reference Rendering Transform prepares scene referred linear data into high dynamic range display referred data. This data is then meant to be handed over to an ODT to convert the data to be viewed in a specific display type. ODT: The Output Display Transform is responsible for converting the data created by the RRT to data that can be viewed on specific devices or color-spaces: sRGB, P3, Rec. 709. LMT RRT ODT ROCF Display Tristimuli","title":"Pipeline"},{"location":"colors/Aces_workflow/#aces-color-pipeline","text":"flowchart LR A(Scene Tristimuli) --> B[OECF] B --> C[IDT] C --> D[LMT] D --> E[RRT] E --> F[ODT] F --> G[EOCF] G --> H(Display Tristimuli) C -->|ACES| E style A stroke-width:2px,stroke-dasharray: 5 5 style H stroke-width:2px,stroke-dasharray: 5 5 Tristimulis refers to linear light, code value (from 0 to ) are linearly related to the power in the light that is entering in the camera lens. those numbers are 12bits and typically we don't have the downstream infrastruture that will handle 12 bits well established. OECF Tippically the first step in the color pipeline is an OECF ( opto electronic conversion function ), in the old days we will would've call this GAMMA.Built in the camera, IDT (input device transform) ingesting data map from one linear light color space to another. 3*3 matrix transform Note IDT: The Input Device Transform is the transform used to convert the pixel colors of images/videos from specific devices into an ACES colorspace. RRT: The Reference Rendering Transform prepares scene referred linear data into high dynamic range display referred data. This data is then meant to be handed over to an ODT to convert the data to be viewed in a specific display type. ODT: The Output Display Transform is responsible for converting the data created by the RRT to data that can be viewed on specific devices or color-spaces: sRGB, P3, Rec. 709. LMT RRT ODT ROCF Display Tristimuli","title":"ACES Color Pipeline"},{"location":"colors/CIE/","text":"ColorSpace Un espace de couleur est un mod\u00e8le math\u00e9matique utilis\u00e9 pour repr\u00e9senter les couleurs de mani\u00e8re standardis\u00e9e. Il utilise g\u00e9n\u00e9ralement plusieurs dimensions pour d\u00e9crire les couleurs, telles que la luminance, la teinte et la saturation. Les espaces de couleur sont utilis\u00e9s dans de nombreuses applications, comme l'impression, la photographie, les arts graphiques et les sciences de la couleur. Ils permettent de mesurer et de comparer les couleurs de mani\u00e8re objective, ce qui est utile pour garantir la fid\u00e9lit\u00e9 des couleurs dans les travaux professionnels. Il existe diff\u00e9rents espaces de couleur, chacun ayant des caract\u00e9ristiques et des utilisations diff\u00e9rentes. L'espace de couleur CIE XYZ est un espace de couleur absolu qui ne tient pas compte des caract\u00e9ristiques sp\u00e9cifiques de l'\u00e9cran ou de l'\u00e9clairage utilis\u00e9s pour afficher les couleurs. L'espace de couleur RGB est un espace de couleur adapt\u00e9 \u00e0 l'affichage sur \u00e9cran, tandis que l'espace de couleur CMYK est utilis\u00e9 pour les impressions.","title":"CIE XYZ"},{"location":"colors/CIE/#colorspace","text":"Un espace de couleur est un mod\u00e8le math\u00e9matique utilis\u00e9 pour repr\u00e9senter les couleurs de mani\u00e8re standardis\u00e9e. Il utilise g\u00e9n\u00e9ralement plusieurs dimensions pour d\u00e9crire les couleurs, telles que la luminance, la teinte et la saturation. Les espaces de couleur sont utilis\u00e9s dans de nombreuses applications, comme l'impression, la photographie, les arts graphiques et les sciences de la couleur. Ils permettent de mesurer et de comparer les couleurs de mani\u00e8re objective, ce qui est utile pour garantir la fid\u00e9lit\u00e9 des couleurs dans les travaux professionnels. Il existe diff\u00e9rents espaces de couleur, chacun ayant des caract\u00e9ristiques et des utilisations diff\u00e9rentes. L'espace de couleur CIE XYZ est un espace de couleur absolu qui ne tient pas compte des caract\u00e9ristiques sp\u00e9cifiques de l'\u00e9cran ou de l'\u00e9clairage utilis\u00e9s pour afficher les couleurs. L'espace de couleur RGB est un espace de couleur adapt\u00e9 \u00e0 l'affichage sur \u00e9cran, tandis que l'espace de couleur CMYK est utilis\u00e9 pour les impressions.","title":"ColorSpace"},{"location":"colors/introToLight/","text":"Introduction to Light, Color and Color Space Introduction Simple en apparence et assez courante, la notion de couleur est en r\u00e9alit\u00e9 une question complexe. Ce n'est pas seulement quelque chose qui peut \u00eatre d\u00e9crit scientifiquement, auquel cas nous ne pourrions avoir qu'une point de vue objectif et rationnel sur la question. Les couleurs sont \u00e9galement le r\u00e9sultat d'un processus qui implique la vision, l'un des syst\u00e8mes sensoriels \u00e0 travers lesquels nous percevons et interagissons avec le monde qui nous entoure. En tant que tel, c'est aussi une question tr\u00e8s subjective avec une composante psychologique (la signification des couleurs) et physiologique (comment notre cerveau traite-t-il les couleurs) (avez-vous d\u00e9j\u00e0 v\u00e9cu une dispute avec une autre personne \u00e0 propos de la couleur d'un objet ?). Vous avez aussi probablement tous \u00e9t\u00e9 tromp\u00e9s par des illusions d'optique bien connues qui sont un autre exemple de l'influence de l'esprit sur la fa\u00e7on dont nous percevons les formes et les couleurs. Nous n'entrerons pas dans les d\u00e9tails et nous nous en tiendrons \u00e0 la fa\u00e7on dont nous pouvons repr\u00e9senter, stocker et afficher les couleurs dans le monde des ordinateurs. Cependant, c'est juste pour dire que le sujet est beaucoup plus complexe qu'il n'y para\u00eet \u00e0 premi\u00e8re vue. L'\u00e9tude de la couleur est g\u00e9n\u00e9ralement appel\u00e9e science des couleurs , qui comprend tous les \u00e9l\u00e9ments que nous avons mentionn\u00e9s ci-dessus : Comment le cerveau traite-t-il les stimuli visuels en ce que nous percevons comme des couleurs. L'utilisation des couleurs d'un point de vue artistique et l'\u00e9tude de ondes \u00e9lectromagn\u00e9tiques responsables de la lumi\u00e8re telle qu'elle existe dans le monde physique. Traiter les couleurs dans le domaine des \u00e9crans num\u00e9riques, qui est aussi une science \u00e0 part enti\u00e8re (ce que l'on appelle parfois la gestion des couleurs ). Light Tout commence par la lumi\u00e8re. Avant de pouvoir nous pencher sur les couleurs, nous devons d'abord comprendre la lumi\u00e8re et comment la lumi\u00e8re interagit avec la mati\u00e8re. La lumi\u00e8re voyage dans l'espace sous forme d'ondes \u00e9lectromagn\u00e9tiques, mais peut \u00e9galement \u00eatre d\u00e9crite comme un flux de particules qu'Einstein appelait photons (c'est la raison pour laquelle nous disons que la lumi\u00e8re a une double nature onde-particule). Dans cette le\u00e7on cependant, nous ne consid\u00e9rerons la lumi\u00e8re que sous sa forme ondulatoire. Une onde p\u00e9riodique, comme nous le savons, est d\u00e9finie par sa fr\u00e9quence (le nombre de fois qu'un cycle est r\u00e9p\u00e9t\u00e9 par unit\u00e9 de temps) ou sa longueur d'onde (qui est l'inverse de la fr\u00e9quence) qui est la distance sur laquelle la forme de l'onde se r\u00e9p\u00e8te. La couleur de la lumi\u00e8re peut \u00eatre consid\u00e9r\u00e9e comme l'\u00e9quivalent du concept de hauteur du son. Les deux sont bas\u00e9s sur la longueur d'onde ou la fr\u00e9quence du signal voyageant dans l'espace (consultez la le\u00e7on sur l'interaction lumi\u00e8re-mati\u00e8re pour en savoir plus sur la longueur d'onde et la fr\u00e9quence de la lumi\u00e8re). Figure 1: a) the length a complete cycle of the wave is called the wavelength of the periodic function. The wavelength of visible light varies from 380 nm to 740 nm. b) white light passing through a prism is decomposed into a rainbow of colors. A wavelength is denoted with the greek letter \u03bb (lambda). Visible light is made of waves which frequency varies from 380 to about 740 nanometres (a nanometre is 1\u00d710\u22129 meter). Any waves which wavelength is below 380 nm or above 740 nm can not be perceived by the human eye. The following image shows the full spectrum of colors the visible light spectrum is made of (each color you see has a wavelength within the range of about 380 nm to 780 nm) Most people are also familiar with the Newton experiment which consists of using a prism to decompose white light into a rainbow of colors (figure 1b). This experiment shows that white light is made of all the visible colors from the visible light spectrum, mixed in some proportions. The prism experiment can also be carried out the other way around. If we take all the light colors from the visible light spectrum and add them up in the same proportions, then we can recreate white light (figure 2). White light as such doesn't exist. White light is the result of a light source, the sun or the screen of your computer, producing a mixture of light colors from the visible spectrum. If you examine your computer screen or television with a magnifying glass, you will see tiny dots, probably red, green and blue and by mixing these colors in different amounts, a large range of colors can be produced. Figure 2: a wooden mannequin lit by a green, red and blue light. The three colors are mixed on the background which appears white. Info Pointillism is a painting technique by which sensations of colors can be obtained by putting small dots of pure colors side by side in organised patterns rather than using the more traditional way of mixing colors on the palette. From the distance, the colored dots blend into one single color. Screens work in very similar fashion. Each pixel from the scene is actually made of three small components which emit red, green and blue light. By changing the amount of red, green and blue light emitted, we can create all the colors we need. From the distance these three separable elements are indistinguishable and the contribution of each light blend within each other to form one single light color. The next chapter will provide more information on the way screens actually work.","title":"Light"},{"location":"colors/introToLight/#introduction-to-light-color-and-color-space","text":"","title":"Introduction to Light, Color and Color Space"},{"location":"colors/introToLight/#introduction","text":"Simple en apparence et assez courante, la notion de couleur est en r\u00e9alit\u00e9 une question complexe. Ce n'est pas seulement quelque chose qui peut \u00eatre d\u00e9crit scientifiquement, auquel cas nous ne pourrions avoir qu'une point de vue objectif et rationnel sur la question. Les couleurs sont \u00e9galement le r\u00e9sultat d'un processus qui implique la vision, l'un des syst\u00e8mes sensoriels \u00e0 travers lesquels nous percevons et interagissons avec le monde qui nous entoure. En tant que tel, c'est aussi une question tr\u00e8s subjective avec une composante psychologique (la signification des couleurs) et physiologique (comment notre cerveau traite-t-il les couleurs) (avez-vous d\u00e9j\u00e0 v\u00e9cu une dispute avec une autre personne \u00e0 propos de la couleur d'un objet ?). Vous avez aussi probablement tous \u00e9t\u00e9 tromp\u00e9s par des illusions d'optique bien connues qui sont un autre exemple de l'influence de l'esprit sur la fa\u00e7on dont nous percevons les formes et les couleurs. Nous n'entrerons pas dans les d\u00e9tails et nous nous en tiendrons \u00e0 la fa\u00e7on dont nous pouvons repr\u00e9senter, stocker et afficher les couleurs dans le monde des ordinateurs. Cependant, c'est juste pour dire que le sujet est beaucoup plus complexe qu'il n'y para\u00eet \u00e0 premi\u00e8re vue. L'\u00e9tude de la couleur est g\u00e9n\u00e9ralement appel\u00e9e science des couleurs , qui comprend tous les \u00e9l\u00e9ments que nous avons mentionn\u00e9s ci-dessus : Comment le cerveau traite-t-il les stimuli visuels en ce que nous percevons comme des couleurs. L'utilisation des couleurs d'un point de vue artistique et l'\u00e9tude de ondes \u00e9lectromagn\u00e9tiques responsables de la lumi\u00e8re telle qu'elle existe dans le monde physique. Traiter les couleurs dans le domaine des \u00e9crans num\u00e9riques, qui est aussi une science \u00e0 part enti\u00e8re (ce que l'on appelle parfois la gestion des couleurs ).","title":"Introduction"},{"location":"colors/introToLight/#light","text":"Tout commence par la lumi\u00e8re. Avant de pouvoir nous pencher sur les couleurs, nous devons d'abord comprendre la lumi\u00e8re et comment la lumi\u00e8re interagit avec la mati\u00e8re. La lumi\u00e8re voyage dans l'espace sous forme d'ondes \u00e9lectromagn\u00e9tiques, mais peut \u00e9galement \u00eatre d\u00e9crite comme un flux de particules qu'Einstein appelait photons (c'est la raison pour laquelle nous disons que la lumi\u00e8re a une double nature onde-particule). Dans cette le\u00e7on cependant, nous ne consid\u00e9rerons la lumi\u00e8re que sous sa forme ondulatoire. Une onde p\u00e9riodique, comme nous le savons, est d\u00e9finie par sa fr\u00e9quence (le nombre de fois qu'un cycle est r\u00e9p\u00e9t\u00e9 par unit\u00e9 de temps) ou sa longueur d'onde (qui est l'inverse de la fr\u00e9quence) qui est la distance sur laquelle la forme de l'onde se r\u00e9p\u00e8te. La couleur de la lumi\u00e8re peut \u00eatre consid\u00e9r\u00e9e comme l'\u00e9quivalent du concept de hauteur du son. Les deux sont bas\u00e9s sur la longueur d'onde ou la fr\u00e9quence du signal voyageant dans l'espace (consultez la le\u00e7on sur l'interaction lumi\u00e8re-mati\u00e8re pour en savoir plus sur la longueur d'onde et la fr\u00e9quence de la lumi\u00e8re). Figure 1: a) the length a complete cycle of the wave is called the wavelength of the periodic function. The wavelength of visible light varies from 380 nm to 740 nm. b) white light passing through a prism is decomposed into a rainbow of colors. A wavelength is denoted with the greek letter \u03bb (lambda). Visible light is made of waves which frequency varies from 380 to about 740 nanometres (a nanometre is 1\u00d710\u22129 meter). Any waves which wavelength is below 380 nm or above 740 nm can not be perceived by the human eye. The following image shows the full spectrum of colors the visible light spectrum is made of (each color you see has a wavelength within the range of about 380 nm to 780 nm) Most people are also familiar with the Newton experiment which consists of using a prism to decompose white light into a rainbow of colors (figure 1b). This experiment shows that white light is made of all the visible colors from the visible light spectrum, mixed in some proportions. The prism experiment can also be carried out the other way around. If we take all the light colors from the visible light spectrum and add them up in the same proportions, then we can recreate white light (figure 2). White light as such doesn't exist. White light is the result of a light source, the sun or the screen of your computer, producing a mixture of light colors from the visible spectrum. If you examine your computer screen or television with a magnifying glass, you will see tiny dots, probably red, green and blue and by mixing these colors in different amounts, a large range of colors can be produced. Figure 2: a wooden mannequin lit by a green, red and blue light. The three colors are mixed on the background which appears white. Info Pointillism is a painting technique by which sensations of colors can be obtained by putting small dots of pure colors side by side in organised patterns rather than using the more traditional way of mixing colors on the palette. From the distance, the colored dots blend into one single color. Screens work in very similar fashion. Each pixel from the scene is actually made of three small components which emit red, green and blue light. By changing the amount of red, green and blue light emitted, we can create all the colors we need. From the distance these three separable elements are indistinguishable and the contribution of each light blend within each other to form one single light color. The next chapter will provide more information on the way screens actually work.","title":"Light"},{"location":"compositing/Basics/","text":"BASIC SENSITOMETRY AND CHARACTERISTICS OF FILM Documents Kodak - Basic Sensitometry and Characteristics of Film Kodak - Basic Photographic Sensitometry Workbook","title":"Basic"},{"location":"compositing/Basics/#basic-sensitometry-and-characteristics-of-film","text":"","title":"BASIC SENSITOMETRY AND CHARACTERISTICS OF FILM"},{"location":"compositing/Basics/#documents","text":"Kodak - Basic Sensitometry and Characteristics of Film Kodak - Basic Photographic Sensitometry Workbook","title":"Documents"},{"location":"compositing/Integration/","text":"The Beauty Grade The Beauty Grade is the first process to be applied to CGI in order to start modelling its Characteristic Curve that will recreate the way the sensor/film captured the footage. This is a chronological Step-by-Step guide to assist you through this vast process. In order to follow this guide correctly you should be familiar with photography basics of exposition, elements of optics relative to the behaviour of light intensities vs densities, film/sensor supports, NUKE\u2122 Grade & Toe mathematical operations and digital color fundamentals. Beauty Grade Guide Viewer Gamma high (recommended at 4). Grade node (attached to the resulting CG). (un)premultiply by rgba.alpha. Black Levels: Blackpoint: sample the darkest pixel from the CG. Remember: The intensity of light is relative to exposure but absence of light \u2013black\u2013 is absolute. Making a selection (rectangle) of the area containing the darkest pixels \u2013 please exclude any pixel from outside the premultiplied area. Using the Pixel Analyser use the Min colour chip and drag & drop into the colour chip of the Blackpoint knob. Lift: sample the darkest area from the Scan. Making an accurate selection (rectangle) of the area containing the darkest pixels. Make the selection as big as possible of just pure \u201cblack\u201d pixels. Using the Pixel Analyser use the Average (or the Median when you have an uneven selection of pixel values) colour chip and drag & drop into the colour chip of the Lift knob. Base & Fog: Toe node (attached to the Grade node). (un)premultiply by rgba.alpha. Link the Lift knob from the Grade node to the Toe (lift knob). *To create a link, drag & drop the curve icon by holding the Ctrl/Command key (if successful a green arrow will appear between the nodes). Back to the Grade node again: we push down the black levels to crash against the Toe levels of the base & fog by decreasing the value of the Offset knob. *This is something you do by eye, make sure you lose the same amount of detail in the black levels as your scan. Check any transfer unbalance (RGB) from the black levels to the lower midtones (that colour halo). >- TMI color controls of the Offset knob (start with the T [temperature] control (slider) and then move into the M [magenta] control (slider), don\u2019t touch the I [intensity] control (slider), because you already adjusted in previous operations).","title":"Integration"},{"location":"compositing/Integration/#the-beauty-grade","text":"The Beauty Grade is the first process to be applied to CGI in order to start modelling its Characteristic Curve that will recreate the way the sensor/film captured the footage. This is a chronological Step-by-Step guide to assist you through this vast process. In order to follow this guide correctly you should be familiar with photography basics of exposition, elements of optics relative to the behaviour of light intensities vs densities, film/sensor supports, NUKE\u2122 Grade & Toe mathematical operations and digital color fundamentals.","title":"The Beauty Grade"},{"location":"compositing/Integration/#beauty-grade-guide","text":"Viewer Gamma high (recommended at 4). Grade node (attached to the resulting CG). (un)premultiply by rgba.alpha. Black Levels: Blackpoint: sample the darkest pixel from the CG. Remember: The intensity of light is relative to exposure but absence of light \u2013black\u2013 is absolute. Making a selection (rectangle) of the area containing the darkest pixels \u2013 please exclude any pixel from outside the premultiplied area. Using the Pixel Analyser use the Min colour chip and drag & drop into the colour chip of the Blackpoint knob. Lift: sample the darkest area from the Scan. Making an accurate selection (rectangle) of the area containing the darkest pixels. Make the selection as big as possible of just pure \u201cblack\u201d pixels. Using the Pixel Analyser use the Average (or the Median when you have an uneven selection of pixel values) colour chip and drag & drop into the colour chip of the Lift knob. Base & Fog: Toe node (attached to the Grade node). (un)premultiply by rgba.alpha. Link the Lift knob from the Grade node to the Toe (lift knob). *To create a link, drag & drop the curve icon by holding the Ctrl/Command key (if successful a green arrow will appear between the nodes). Back to the Grade node again: we push down the black levels to crash against the Toe levels of the base & fog by decreasing the value of the Offset knob. *This is something you do by eye, make sure you lose the same amount of detail in the black levels as your scan. Check any transfer unbalance (RGB) from the black levels to the lower midtones (that colour halo). >- TMI color controls of the Offset knob (start with the T [temperature] control (slider) and then move into the M [magenta] control (slider), don\u2019t touch the I [intensity] control (slider), because you already adjusted in previous operations).","title":"Beauty Grade Guide"},{"location":"compositing/tracking/","text":"lift -> shadow, gamma -> midtones, Gain -> highlight lift == multiply 3 type of tracking 2D track -> Point track 2.5D tracks -> Planar Tracks 3D track -> Camera track 2d XY motion Color correction any parallax where i try to track myu element ? ANY ROTATION , 2.5 Track planar surfaces, anythin that can be put on a flat grid 3D track Shot that introduce parallax and dimentionality that need to be reflected in the element you adding Parallax: The relative speeds two separate objects move based on their distance from the camera. Creates the feeling of depth. Dimensionality: The relative speeds differents parts of a single object move on their distance from the Camera. Creates the feeling thaht an object is 3 dimensiontal and not a flat plane.","title":"Tracking"},{"location":"pipeline/OCIO/","text":"OCIO OpenColorIO OpenColorIO (OCIO) was initially developed (since 2003), open-sourced by Sony Pictures Imageworks and \u275d is an Academy Scientific and Technical Award winning color management solution for creating and displaying consistent images across multiple content creation applications during visual effects and animation production \u275e - It became the primary color management framework solution many software vendor started to support. The Academy Software Foundation (ASWF) was founded in 2018 to help foster and shepherd the development of open source software projects in the visual effects industry [ASWF 2018]. OpenColorIO was the second project accepted into the foundation [Olin 2019] (source). OCIO is open-source, free and is licensed under the BSD-3-Clause license. The Technical Steering Committee (\u201cTSC\u201d) is responsible for all technical oversight of the open source project. Last but not least, there are significant contributions that have also been made by Industrial Light & Magic, DNEG, and many individuals (contributors list). Version 2.0 is the second major version of OCIO, led by full-time Autodesk software engineers. Note Open Color IO (OCIO) is a color management framework: \u201cOCIO enables color transforms and image display to be handled in a consistent manner across multiple graphics applications. Unlike other color management solutions, OCIO is geared towards motion-picture post production, with an emphasis on visual effects and animation color pipelines. OpenColorIO has been used since 2003 to address the challenges of working with multiple commercial image-processing applications that have different approaches to color management\u201d - Sony Imageworks OCIO Configuration Files The configuration file \u201ccontrols\u201d OCIO (its file package), is usually named config.ocio and is a YAML document that can be opened in most text or code (e.g VSC) editors. OCIO Configuration Files Loading OCIO Package Environment Variable The operating-system environment-variables (shortened as env-var) allows all software (supporting OCIO) to automatically load the same OCIO configuration file (shortened to config-file) by setting a file path. https://www.elsksa.me/scientia/cgi-offline-rendering/ocio","title":"Intro"},{"location":"pipeline/OCIO/#ocio","text":"","title":"OCIO"},{"location":"pipeline/OCIO/#opencolorio","text":"OpenColorIO (OCIO) was initially developed (since 2003), open-sourced by Sony Pictures Imageworks and \u275d is an Academy Scientific and Technical Award winning color management solution for creating and displaying consistent images across multiple content creation applications during visual effects and animation production \u275e - It became the primary color management framework solution many software vendor started to support. The Academy Software Foundation (ASWF) was founded in 2018 to help foster and shepherd the development of open source software projects in the visual effects industry [ASWF 2018]. OpenColorIO was the second project accepted into the foundation [Olin 2019] (source). OCIO is open-source, free and is licensed under the BSD-3-Clause license. The Technical Steering Committee (\u201cTSC\u201d) is responsible for all technical oversight of the open source project. Last but not least, there are significant contributions that have also been made by Industrial Light & Magic, DNEG, and many individuals (contributors list). Version 2.0 is the second major version of OCIO, led by full-time Autodesk software engineers. Note Open Color IO (OCIO) is a color management framework: \u201cOCIO enables color transforms and image display to be handled in a consistent manner across multiple graphics applications. Unlike other color management solutions, OCIO is geared towards motion-picture post production, with an emphasis on visual effects and animation color pipelines. OpenColorIO has been used since 2003 to address the challenges of working with multiple commercial image-processing applications that have different approaches to color management\u201d - Sony Imageworks","title":"OpenColorIO"},{"location":"pipeline/OCIO/#ocio-configuration-files","text":"The configuration file \u201ccontrols\u201d OCIO (its file package), is usually named config.ocio and is a YAML document that can be opened in most text or code (e.g VSC) editors.","title":"OCIO Configuration Files"},{"location":"pipeline/OCIO/#ocio-configuration-files_1","text":"","title":"OCIO Configuration Files"},{"location":"pipeline/OCIO/#loading-ocio-package","text":"Environment Variable The operating-system environment-variables (shortened as env-var) allows all software (supporting OCIO) to automatically load the same OCIO configuration file (shortened to config-file) by setting a file path. https://www.elsksa.me/scientia/cgi-offline-rendering/ocio","title":"Loading OCIO Package"},{"location":"pipeline/OCIO_DT_misconceptions/","text":"Introduction I often think about this great question asked by Doug Walker at one of our OpenColorIO (OCIO) meetings : what are you trying to solve ? So in our case, what is this post about ? In the past year (2021), I have given several talks online about OCIO and the Academy Color Encoding System (ACES). I thought it would be useful to put those slides online for two reasons : If there is any inaccurracy, anyone can correct me and I\u2019ll be more than happy to update this page. With these slides online, I hope I can reach more people and raise awareness about certain topics. I do not consider myself an expert. I do not believe in this word anymore. We are only human beings, doing our best to understand things. We make mistakes, hopefully learn from them and keep going. So no, I\u2019m not an expert on anything ! The only thing I can offer is an artist\u2019s point of view on Color Management . Quite often, I have been lost on where to begin when it comes to OCIO and ACES. So by writing this post, I hope I can share with you some useful information. There is a reason my book starts with two chapters about Color Management. It will give you the fundation to create beautiful images. So I thought it would be interesting to compare various OCIO configs, with a series of visual examples in order to study their strength and flaws. Really the idea is to look back but NOT judge these historical OCIO configs. Not at all ! Huge respect to all the persons who more than ten years ago developed and shared these configs with the community. A great community Some people may find this post too obvious, boring or outdated. Probably ! But I was really interested to go back in time and write this historical analysis on OCIO and Display Transforms. This is post is primarly about my own misconceptions , but hopefully it will help others too ! Please note that I will only talk about OCIOv1 configs here. First of all, I would like to thank the Academy of Motion Picture Arts and Science s (AMPAS) because without them, I would have probably never discovered this crazy wonderful world of Color Management. ACES has been my entry point to Color Management and I will never be thankful enough for that. Thanks to them, I have discovered this amazing community of colour nerds and colour scientists, and I would like to personnally thank the following persons for their generosity, sharing and patience with my questions :","title":"Misconception"},{"location":"pipeline/OCIO_DT_misconceptions/#introduction","text":"I often think about this great question asked by Doug Walker at one of our OpenColorIO (OCIO) meetings : what are you trying to solve ? So in our case, what is this post about ? In the past year (2021), I have given several talks online about OCIO and the Academy Color Encoding System (ACES). I thought it would be useful to put those slides online for two reasons : If there is any inaccurracy, anyone can correct me and I\u2019ll be more than happy to update this page. With these slides online, I hope I can reach more people and raise awareness about certain topics. I do not consider myself an expert. I do not believe in this word anymore. We are only human beings, doing our best to understand things. We make mistakes, hopefully learn from them and keep going. So no, I\u2019m not an expert on anything ! The only thing I can offer is an artist\u2019s point of view on Color Management . Quite often, I have been lost on where to begin when it comes to OCIO and ACES. So by writing this post, I hope I can share with you some useful information. There is a reason my book starts with two chapters about Color Management. It will give you the fundation to create beautiful images. So I thought it would be interesting to compare various OCIO configs, with a series of visual examples in order to study their strength and flaws. Really the idea is to look back but NOT judge these historical OCIO configs. Not at all ! Huge respect to all the persons who more than ten years ago developed and shared these configs with the community.","title":"Introduction"},{"location":"pipeline/OCIO_DT_misconceptions/#a-great-community","text":"Some people may find this post too obvious, boring or outdated. Probably ! But I was really interested to go back in time and write this historical analysis on OCIO and Display Transforms. This is post is primarly about my own misconceptions , but hopefully it will help others too ! Please note that I will only talk about OCIOv1 configs here. First of all, I would like to thank the Academy of Motion Picture Arts and Science s (AMPAS) because without them, I would have probably never discovered this crazy wonderful world of Color Management. ACES has been my entry point to Color Management and I will never be thankful enough for that. Thanks to them, I have discovered this amazing community of colour nerds and colour scientists, and I would like to personnally thank the following persons for their generosity, sharing and patience with my questions :","title":"A great community"},{"location":"pipeline/Pipeline_usd/","text":"","title":"Pipeline usd"},{"location":"pipeline/USD/00_install_USD/","text":"Essayez d'utiliser USD SIGGRAPH2019 a constat\u00e9 que l'USD est tr\u00e8s chaud, donc Je vais t\u00e9l\u00e9charger un article de synth\u00e8se tout en v\u00e9rifiant diverses choses. Tout d'abord, nous allons t\u00e9l\u00e9charger et configurer USD. Construire Construisez d'abord l'USD. Auparavant, il fallait beaucoup de travail pour cr\u00e9er et utiliser USD, mais maintenant, cr\u00e9ez des scripts Il est pr\u00e9par\u00e9 et peut \u00eatre construit relativement facilement. Installez ce dont vous avez besoin https://git-scm.com/ https://visualstudio.microsoft.com/ja/downloads/ https://www.python.org/downloads/release/python-3712/ Tout d'abord, t\u00e9l\u00e9chargez et installez VisualStudio n\u00e9cessaire \u00e0 la construction, Git pour obtenir le code source et Python. Une fois Python install\u00e9 Ajoutez Python37 directement en dessous et Scripts \u00e0 la variable d'environnement PATH. De plus, installez PySide2 et PyOpenGL, qui sont requis lors de l'utilisation d'USDView. pip installer PyOpenGL PyOpenGL_accelerate PySide2 Les pr\u00e9paratifs sont maintenant termin\u00e9s. Cloner le d\u00e9p\u00f4t Une fois install\u00e9, clonez le r\u00e9f\u00e9rentiel \u00e0 partir du Github d'USD. \u00c0 l'invite de commande, acc\u00e9dez au r\u00e9pertoire dans lequel vous avez t\u00e9l\u00e9charg\u00e9 git clone https://github.com/PixarAnimationStudios/USD.git Cloner Une fois clon\u00e9, ouvrez l'invite de commande du d\u00e9veloppeur de VisualStudio. Une fois ouvert, acc\u00e9dez au dossier dans lequel vous avez clon\u00e9 le r\u00e9f\u00e9rentiel et ex\u00e9cutez la construction. python build_scripts\\build_usd.py <destination de sortie des artefacts de construction> \u00e0 travers le chemin Une fois le t\u00e9l\u00e9chargement termin\u00e9, placez-le dans le PATH requis. tu as besoin de deux nom de la variable CHEMIN PYTHONPATH /lib/python PATH /bin /lib Info Si votre PATH n'est pas sous lib, Notez qu'une erreur se produira lors de l'importation d'un fichier pyd. Passez par ces deux et vous \u00eates pr\u00eat \u00e0 partir essayez d'ouvrir des exemples de donn\u00e9es Lorsque vous \u00eates pr\u00eat, t\u00e9l\u00e9chargez l'exemple d'USD et ouvrez-le dans votre visionneuse. http://graphics.pixar.com/usd/downloads.html Des exemples de donn\u00e9es sont disponibles sur le site officiel de PIXAR, alors t\u00e9l\u00e9chargez ce KitchenSet. Apr\u00e8s le t\u00e9l\u00e9chargement, d\u00e9compressez et ouvrez l'invite de commande. usdview Kitchen_set.usd Ouvrez Kitchen_set.usd avec usdview. Utilisez USDView pour voir les graphiques de sc\u00e8ne, les couches, les propri\u00e9t\u00e9s, etc. des fichiers USD bo\u00eete. Il est \u00e9galement livr\u00e9 avec une console Python, donc Il est facile de comprendre (semble) d'utiliser cet usdview pour tester diverses choses. Cr\u00e9er un fichier USD \u00e0 partir de Python Lorsque vous \u00eates pr\u00eat, essayez d'ex\u00e9cuter le didacticiel officiel. https://graphics.pixar.com/usd/docs/Hello-World---Creating-Your-First-USD-Stage.html depuis l 'importation pxr Usd, UsdGeom stage = Usd . Stage . CreateNew ( 'HelloWorld.usda' ) xformPrim = UsdGeom . Xform . Define ( \u00e9tape , '/hello' ) spherePrim = UsdGeom . Sphere . Define ( stage , '/hello/world' ) \u00e9tape . GetRootLayer () . Save () Une fois ex\u00e9cut\u00e9, un fichier HelloWorld.usda sera g\u00e9n\u00e9r\u00e9 dans le dossier sp\u00e9cifi\u00e9. #usda 1.0 def Xform \"bonjour\" { def Sph\u00e8re \"monde\" { } } \u00c0 l'int\u00e9rieur se trouve un simple fichier USD (vide). Quand je l'ai ouvert dans usdeview, il a montr\u00e9 une simple sph\u00e8re. Pour le moment, nous avons maintenant un environnement o\u00f9 nous pouvons toucher l'USD. Comme petite pr\u00e9cision, car je n'ai pas mis PATH dans le dossier lib J'ai une erreur DLL introuvable Erreur d'autorisation lors de la sp\u00e9cification d'un dossier sur le NAS de Synology comme destination de sauvegarde o\u00f9 il ne pouvait pas \u00eatre \u00e9crit. Maintenant que nous sommes pr\u00eats, tout en explorant la structure de base de l'USD Je voudrais r\u00e9sumer comment l'utiliser.","title":"USD Install"},{"location":"pipeline/USD/00_install_USD/#essayez-dutiliser-usd","text":"SIGGRAPH2019 a constat\u00e9 que l'USD est tr\u00e8s chaud, donc Je vais t\u00e9l\u00e9charger un article de synth\u00e8se tout en v\u00e9rifiant diverses choses. Tout d'abord, nous allons t\u00e9l\u00e9charger et configurer USD.","title":"Essayez d'utiliser USD"},{"location":"pipeline/USD/00_install_USD/#construire","text":"Construisez d'abord l'USD. Auparavant, il fallait beaucoup de travail pour cr\u00e9er et utiliser USD, mais maintenant, cr\u00e9ez des scripts Il est pr\u00e9par\u00e9 et peut \u00eatre construit relativement facilement.","title":"Construire"},{"location":"pipeline/USD/00_install_USD/#installez-ce-dont-vous-avez-besoin","text":"https://git-scm.com/ https://visualstudio.microsoft.com/ja/downloads/ https://www.python.org/downloads/release/python-3712/ Tout d'abord, t\u00e9l\u00e9chargez et installez VisualStudio n\u00e9cessaire \u00e0 la construction, Git pour obtenir le code source et Python. Une fois Python install\u00e9 Ajoutez Python37 directement en dessous et Scripts \u00e0 la variable d'environnement PATH. De plus, installez PySide2 et PyOpenGL, qui sont requis lors de l'utilisation d'USDView. pip installer PyOpenGL PyOpenGL_accelerate PySide2 Les pr\u00e9paratifs sont maintenant termin\u00e9s.","title":"Installez ce dont vous avez besoin"},{"location":"pipeline/USD/00_install_USD/#cloner-le-d\u00e9p\u00f4t","text":"Une fois install\u00e9, clonez le r\u00e9f\u00e9rentiel \u00e0 partir du Github d'USD. \u00c0 l'invite de commande, acc\u00e9dez au r\u00e9pertoire dans lequel vous avez t\u00e9l\u00e9charg\u00e9 git clone https://github.com/PixarAnimationStudios/USD.git Cloner Une fois clon\u00e9, ouvrez l'invite de commande du d\u00e9veloppeur de VisualStudio. Une fois ouvert, acc\u00e9dez au dossier dans lequel vous avez clon\u00e9 le r\u00e9f\u00e9rentiel et ex\u00e9cutez la construction. python build_scripts\\build_usd.py <destination de sortie des artefacts de construction>","title":"Cloner le d\u00e9p\u00f4t"},{"location":"pipeline/USD/00_install_USD/#\u00e0-travers-le-chemin","text":"Une fois le t\u00e9l\u00e9chargement termin\u00e9, placez-le dans le PATH requis. tu as besoin de deux nom de la variable CHEMIN PYTHONPATH /lib/python PATH /bin /lib Info Si votre PATH n'est pas sous lib, Notez qu'une erreur se produira lors de l'importation d'un fichier pyd. Passez par ces deux et vous \u00eates pr\u00eat \u00e0 partir","title":"\u00e0 travers le chemin"},{"location":"pipeline/USD/00_install_USD/#essayez-douvrir-des-exemples-de-donn\u00e9es","text":"Lorsque vous \u00eates pr\u00eat, t\u00e9l\u00e9chargez l'exemple d'USD et ouvrez-le dans votre visionneuse. http://graphics.pixar.com/usd/downloads.html Des exemples de donn\u00e9es sont disponibles sur le site officiel de PIXAR, alors t\u00e9l\u00e9chargez ce KitchenSet. Apr\u00e8s le t\u00e9l\u00e9chargement, d\u00e9compressez et ouvrez l'invite de commande. usdview Kitchen_set.usd Ouvrez Kitchen_set.usd avec usdview. Utilisez USDView pour voir les graphiques de sc\u00e8ne, les couches, les propri\u00e9t\u00e9s, etc. des fichiers USD bo\u00eete. Il est \u00e9galement livr\u00e9 avec une console Python, donc Il est facile de comprendre (semble) d'utiliser cet usdview pour tester diverses choses.","title":"essayez d'ouvrir des exemples de donn\u00e9es"},{"location":"pipeline/USD/00_install_USD/#cr\u00e9er-un-fichier-usd-\u00e0-partir-de-python","text":"Lorsque vous \u00eates pr\u00eat, essayez d'ex\u00e9cuter le didacticiel officiel. https://graphics.pixar.com/usd/docs/Hello-World---Creating-Your-First-USD-Stage.html depuis l 'importation pxr Usd, UsdGeom stage = Usd . Stage . CreateNew ( 'HelloWorld.usda' ) xformPrim = UsdGeom . Xform . Define ( \u00e9tape , '/hello' ) spherePrim = UsdGeom . Sphere . Define ( stage , '/hello/world' ) \u00e9tape . GetRootLayer () . Save () Une fois ex\u00e9cut\u00e9, un fichier HelloWorld.usda sera g\u00e9n\u00e9r\u00e9 dans le dossier sp\u00e9cifi\u00e9. #usda 1.0 def Xform \"bonjour\" { def Sph\u00e8re \"monde\" { } } \u00c0 l'int\u00e9rieur se trouve un simple fichier USD (vide). Quand je l'ai ouvert dans usdeview, il a montr\u00e9 une simple sph\u00e8re. Pour le moment, nous avons maintenant un environnement o\u00f9 nous pouvons toucher l'USD. Comme petite pr\u00e9cision, car je n'ai pas mis PATH dans le dossier lib J'ai une erreur DLL introuvable Erreur d'autorisation lors de la sp\u00e9cification d'un dossier sur le NAS de Synology comme destination de sauvegarde o\u00f9 il ne pouvait pas \u00eatre \u00e9crit. Maintenant que nous sommes pr\u00eats, tout en explorant la structure de base de l'USD Je voudrais r\u00e9sumer comment l'utiliser.","title":"Cr\u00e9er un fichier USD \u00e0 partir de Python"},{"location":"pipeline/USD/01_start_USD/","text":"Cr\u00e9er un environnement pour diverses op\u00e9rations sur USD \u00e0 partir de Python Je pensais que j'expliquerais d'abord les bases autour de l'USD et l'explication autour de la structure Si j'\u00e9cris avec une compr\u00e9hension vaguement \u00e0 moiti\u00e9 cuite ~ J'ai l'impression que je vais \u00eatre l\u00e9g\u00e8rement g\u00ean\u00e9 ~ On dirait que \u00e7a va semer la confusion, alors je vais m\u00e9diter un peu plus dessus\u2026 J'ai pu ouvrir USD avec usdview la derni\u00e8re fois, donc Au lieu d'\u00e9crire directement le fichier USD tel quel, faites-le fonctionner du c\u00f4t\u00e9 Python Je voudrais cr\u00e9er un environnement pour v\u00e9rifier ce qui se passe. pr\u00e9paration Tout d'abord, pr\u00e9parez-vous. Usdview peut v\u00e9rifier le mod\u00e8le, mais ce n'est qu'une visionneuse Vous ne pouvez pas faire des choses comme le contr\u00f4le en entrant des valeurs num\u00e9riques dans AttributeEditor. Cependant, si vous voulez jouer avec des nombres, PythonInterpreter est attach\u00e9, donc Vous pouvez le contr\u00f4ler \u00e0 partir de l\u00e0. Mais\u2026 c'est impossible de faire de mon mieux avec cet interpr\u00e8te J'ai d\u00e9cid\u00e9 de cr\u00e9er un environnement en utilisant VSCode et Jupyter. Tout d'abord, utilisez la s\u00e9rie Python 3. Mais ici nous n'avons pas usdview T\u00e9l\u00e9chargez s\u00e9par\u00e9ment USD (build nvidia) pour Python2 Ouvrez usdview \u00e0 partir de l\u00e0. cd /d I:\\jupyter_notebook_root cahier jupyter Pour le moment, cr\u00e9ez un lot qui peut d\u00e9marrer Jupyter \u00e0 un emplacement fixe comme celui-ci D\u00e9marrer le bloc-notes en arri\u00e8re-plan. Mais lorsque j'utilise ce notebook depuis mon navigateur, Il peut \u00eatre utilis\u00e9 pour le moment, mais dans ce cas, la saisie semi-automatique ne fonctionne pas Comme le raccourci est difficile \u00e0 utiliser, je vais essayer de le frapper du c\u00f4t\u00e9 korewoVSCode. Ouvrez les param\u00e8tres URI du serveur Jupyter de Python, Entrez l'URL du Notebook ex\u00e9cut\u00e9 en arri\u00e8re-plan. Cependant, il \u00e9tait difficile d'ins\u00e9rer le jeton bloc-notes jupyter --generate-config Tout d'abord, cr\u00e9ez une configuration, C:/Users/ /.jupyter Ci-dessous, dans jupyter_notebook_config.py c . NotebookApp . token = '' Supprimez le jeton et c . NotebookApp . password = \"sha1:\uff5e\uff5e\uff5e\uff5e\" Tapez votre mot de passe. Le mot de passe est python -c \"importer IPython; imprimer (IPython.lib.passwd())\" Vous pouvez le g\u00e9n\u00e9rer avec cette commande. et. Une fois que tout est pr\u00eat, il est temps de tester les choses c\u00f4t\u00e9 VSCode. Faites quelque chose avec VSCode Dans la version actuelle de VSCode, lorsque j'ouvre .ipynb (au format JupyterNotebook) Vous pouvez modifier des cahiers sur VSCode. (Des raccourcis, etc. de VSCode peuvent \u00e9galement \u00eatre utilis\u00e9s) imprimer Tout d'abord, imprimons le contenu du fichier USD avant de l'ouvrir avec usdview. print ( stage . GetRootLayer () . ExportToString ()) Puisque vous voudrez le v\u00e9rifier souvent, divisez-le uniquement en cellules J'essaierai de l'imprimer si n\u00e9cessaire. Comme \u00e7a, le fichier USD actuel (couches pour \u00eatre pr\u00e9cis) peut \u00eatre imprim\u00e9. Par pr\u00e9caution, lors de l'impression, comme \"imprimer ( ~ )\" Que vous devez utiliser la commande d'impression correctement. Il peut \u00eatre affich\u00e9 sans lui, mais dans ce cas, les sauts de ligne ne sont pas possibles. enregistrer Exportez la sc\u00e8ne lors de l'enregistrement. Je le ferai fr\u00e9quemment, il est donc pratique de le garder dans une cellule. \u00e9tape . GetRootLayer () . Export ( USD_PATH_ROOT + \"/refTest.usda\" ) Il sortira comme ceci. USD peut \u00e9galement \u00eatre NewOpened et Saved lors de l'ouverture S'il y a d\u00e9j\u00e0 un fichier, ce sera une erreur, donc c'\u00e9tait un peu g\u00eanant # Cr\u00e9er un fichier une fois en m\u00e9moire stage = Usd . Stage . CreateInMemory () # Exporter \u00e9tape . GetRootLayer () . Export ( \"CHEMIN\" ) Il est pr\u00e9f\u00e9rable de cr\u00e9er une sc\u00e8ne en m\u00e9moire comme celle-ci et de l'exporter \u00e0 la fin Je pense qu'il est facile de tester avec une nouvelle sc\u00e8ne \u00e0 chaque fois. (peut-\u00eatre) ouvrir avec usdview Apr\u00e8s l'exportation, ouvrez le fichier avec usdview. usdview I :\\usd_test\\refTest.usda Est-ce un pi\u00e8ge lors de l'utilisation d'usdview sous Windows ? Le fichier usd pass\u00e9 en argument doit \u00eatre un chemin complet et le fichier usd doit \u00eatre pass\u00e9 en argument. De plus, comme le d\u00e9marrage de cet outil est inhabituellement lent Ouvrez-le avec un fichier appropri\u00e9 uniquement pour la premi\u00e8re fois Il est recommand\u00e9 d'ouvrir ou de recharger le fichier \u00e0 partir du menu de l'outil. Je viens d'ouvrir le fichier. Si vous \u00eates pr\u00eat jusqu'\u00e0 pr\u00e9sent, alors en \u00e9crivant le code du c\u00f4t\u00e9 VSCode Apr\u00e8s avoir enregistr\u00e9, rechargez la sc\u00e8ne avec Ctrl + R dans usdview et les propri\u00e9t\u00e9s, prims et apparence Assurez-vous qu'il ressemble \u00e0 ce que vous voulez. https://snippets.cacher.io/snippet/e4a461c3093c7ce7929f Apr\u00e8s cela, le r\u00e9sultat du test est t\u00e9l\u00e9charg\u00e9 sur Cacher sous forme de m\u00e9mo. petite histoire Affichage du r\u00e9sultat d'ex\u00e9cution PythonInteractive de VSCode \u00c7a devient de plus en plus. mais peut \u00eatre r\u00e9initialis\u00e9 en appuyant sur le bouton X dans le coin sup\u00e9rieur droit d'Interactif. Vous pouvez \u00e9galement le sortir sous forme de fichier Jupyter ipynb en appuyant sur l'ic\u00f4ne de disquette. https://snippets.cacher.io/snippet/90166b7fd86eb73d7d0e Je peux le sortir, mais je ne pense pas que je l'utiliserai beaucoup. Pour le moment, si vous avez fait jusqu'ici, vous pouvez jouer avec Python Nous avons cr\u00e9\u00e9 un environnement sans stress. Probablement, pour l'utiliser comme format de donn\u00e9es avec Exporter, etc. Je pense que tu n'as pas besoin d'aller aussi loin. Apr\u00e8s tout, pour g\u00e9rer la synth\u00e8se USD, le fonctionnement avec Python ou C ++ est essentiel. Parce qu'il est important de g\u00e9rer du c\u00f4t\u00e9 Python en termes de compr\u00e9hension de la structure des donn\u00e9es Je pense qu'il est important de rendre l'environnement de test sans stress.","title":"USD Start"},{"location":"pipeline/USD/01_start_USD/#cr\u00e9er-un-environnement-pour-diverses-op\u00e9rations-sur-usd-\u00e0-partir-de-python","text":"Je pensais que j'expliquerais d'abord les bases autour de l'USD et l'explication autour de la structure Si j'\u00e9cris avec une compr\u00e9hension vaguement \u00e0 moiti\u00e9 cuite ~ J'ai l'impression que je vais \u00eatre l\u00e9g\u00e8rement g\u00ean\u00e9 ~ On dirait que \u00e7a va semer la confusion, alors je vais m\u00e9diter un peu plus dessus\u2026 J'ai pu ouvrir USD avec usdview la derni\u00e8re fois, donc Au lieu d'\u00e9crire directement le fichier USD tel quel, faites-le fonctionner du c\u00f4t\u00e9 Python Je voudrais cr\u00e9er un environnement pour v\u00e9rifier ce qui se passe.","title":"Cr\u00e9er un environnement pour diverses op\u00e9rations sur USD \u00e0 partir de Python"},{"location":"pipeline/USD/01_start_USD/#pr\u00e9paration","text":"Tout d'abord, pr\u00e9parez-vous. Usdview peut v\u00e9rifier le mod\u00e8le, mais ce n'est qu'une visionneuse Vous ne pouvez pas faire des choses comme le contr\u00f4le en entrant des valeurs num\u00e9riques dans AttributeEditor. Cependant, si vous voulez jouer avec des nombres, PythonInterpreter est attach\u00e9, donc Vous pouvez le contr\u00f4ler \u00e0 partir de l\u00e0. Mais\u2026 c'est impossible de faire de mon mieux avec cet interpr\u00e8te J'ai d\u00e9cid\u00e9 de cr\u00e9er un environnement en utilisant VSCode et Jupyter. Tout d'abord, utilisez la s\u00e9rie Python 3. Mais ici nous n'avons pas usdview T\u00e9l\u00e9chargez s\u00e9par\u00e9ment USD (build nvidia) pour Python2 Ouvrez usdview \u00e0 partir de l\u00e0. cd /d I:\\jupyter_notebook_root cahier jupyter Pour le moment, cr\u00e9ez un lot qui peut d\u00e9marrer Jupyter \u00e0 un emplacement fixe comme celui-ci D\u00e9marrer le bloc-notes en arri\u00e8re-plan. Mais lorsque j'utilise ce notebook depuis mon navigateur, Il peut \u00eatre utilis\u00e9 pour le moment, mais dans ce cas, la saisie semi-automatique ne fonctionne pas Comme le raccourci est difficile \u00e0 utiliser, je vais essayer de le frapper du c\u00f4t\u00e9 korewoVSCode. Ouvrez les param\u00e8tres URI du serveur Jupyter de Python, Entrez l'URL du Notebook ex\u00e9cut\u00e9 en arri\u00e8re-plan. Cependant, il \u00e9tait difficile d'ins\u00e9rer le jeton bloc-notes jupyter --generate-config Tout d'abord, cr\u00e9ez une configuration, C:/Users/ /.jupyter Ci-dessous, dans jupyter_notebook_config.py c . NotebookApp . token = '' Supprimez le jeton et c . NotebookApp . password = \"sha1:\uff5e\uff5e\uff5e\uff5e\" Tapez votre mot de passe. Le mot de passe est python -c \"importer IPython; imprimer (IPython.lib.passwd())\" Vous pouvez le g\u00e9n\u00e9rer avec cette commande. et. Une fois que tout est pr\u00eat, il est temps de tester les choses c\u00f4t\u00e9 VSCode.","title":"pr\u00e9paration"},{"location":"pipeline/USD/01_start_USD/#faites-quelque-chose-avec-vscode","text":"Dans la version actuelle de VSCode, lorsque j'ouvre .ipynb (au format JupyterNotebook) Vous pouvez modifier des cahiers sur VSCode. (Des raccourcis, etc. de VSCode peuvent \u00e9galement \u00eatre utilis\u00e9s)","title":"Faites quelque chose avec VSCode"},{"location":"pipeline/USD/01_start_USD/#imprimer","text":"Tout d'abord, imprimons le contenu du fichier USD avant de l'ouvrir avec usdview. print ( stage . GetRootLayer () . ExportToString ()) Puisque vous voudrez le v\u00e9rifier souvent, divisez-le uniquement en cellules J'essaierai de l'imprimer si n\u00e9cessaire. Comme \u00e7a, le fichier USD actuel (couches pour \u00eatre pr\u00e9cis) peut \u00eatre imprim\u00e9. Par pr\u00e9caution, lors de l'impression, comme \"imprimer ( ~ )\" Que vous devez utiliser la commande d'impression correctement. Il peut \u00eatre affich\u00e9 sans lui, mais dans ce cas, les sauts de ligne ne sont pas possibles.","title":"imprimer"},{"location":"pipeline/USD/01_start_USD/#enregistrer","text":"Exportez la sc\u00e8ne lors de l'enregistrement. Je le ferai fr\u00e9quemment, il est donc pratique de le garder dans une cellule. \u00e9tape . GetRootLayer () . Export ( USD_PATH_ROOT + \"/refTest.usda\" ) Il sortira comme ceci. USD peut \u00e9galement \u00eatre NewOpened et Saved lors de l'ouverture S'il y a d\u00e9j\u00e0 un fichier, ce sera une erreur, donc c'\u00e9tait un peu g\u00eanant # Cr\u00e9er un fichier une fois en m\u00e9moire stage = Usd . Stage . CreateInMemory () # Exporter \u00e9tape . GetRootLayer () . Export ( \"CHEMIN\" ) Il est pr\u00e9f\u00e9rable de cr\u00e9er une sc\u00e8ne en m\u00e9moire comme celle-ci et de l'exporter \u00e0 la fin Je pense qu'il est facile de tester avec une nouvelle sc\u00e8ne \u00e0 chaque fois. (peut-\u00eatre)","title":"enregistrer"},{"location":"pipeline/USD/01_start_USD/#ouvrir-avec-usdview","text":"Apr\u00e8s l'exportation, ouvrez le fichier avec usdview. usdview I :\\usd_test\\refTest.usda Est-ce un pi\u00e8ge lors de l'utilisation d'usdview sous Windows ? Le fichier usd pass\u00e9 en argument doit \u00eatre un chemin complet et le fichier usd doit \u00eatre pass\u00e9 en argument. De plus, comme le d\u00e9marrage de cet outil est inhabituellement lent Ouvrez-le avec un fichier appropri\u00e9 uniquement pour la premi\u00e8re fois Il est recommand\u00e9 d'ouvrir ou de recharger le fichier \u00e0 partir du menu de l'outil. Je viens d'ouvrir le fichier. Si vous \u00eates pr\u00eat jusqu'\u00e0 pr\u00e9sent, alors en \u00e9crivant le code du c\u00f4t\u00e9 VSCode Apr\u00e8s avoir enregistr\u00e9, rechargez la sc\u00e8ne avec Ctrl + R dans usdview et les propri\u00e9t\u00e9s, prims et apparence Assurez-vous qu'il ressemble \u00e0 ce que vous voulez. https://snippets.cacher.io/snippet/e4a461c3093c7ce7929f Apr\u00e8s cela, le r\u00e9sultat du test est t\u00e9l\u00e9charg\u00e9 sur Cacher sous forme de m\u00e9mo.","title":"ouvrir avec usdview"},{"location":"pipeline/USD/01_start_USD/#petite-histoire","text":"Affichage du r\u00e9sultat d'ex\u00e9cution PythonInteractive de VSCode \u00c7a devient de plus en plus. mais peut \u00eatre r\u00e9initialis\u00e9 en appuyant sur le bouton X dans le coin sup\u00e9rieur droit d'Interactif. Vous pouvez \u00e9galement le sortir sous forme de fichier Jupyter ipynb en appuyant sur l'ic\u00f4ne de disquette. https://snippets.cacher.io/snippet/90166b7fd86eb73d7d0e Je peux le sortir, mais je ne pense pas que je l'utiliserai beaucoup. Pour le moment, si vous avez fait jusqu'ici, vous pouvez jouer avec Python Nous avons cr\u00e9\u00e9 un environnement sans stress. Probablement, pour l'utiliser comme format de donn\u00e9es avec Exporter, etc. Je pense que tu n'as pas besoin d'aller aussi loin. Apr\u00e8s tout, pour g\u00e9rer la synth\u00e8se USD, le fonctionnement avec Python ou C ++ est essentiel. Parce qu'il est important de g\u00e9rer du c\u00f4t\u00e9 Python en termes de compr\u00e9hension de la structure des donn\u00e9es Je pense qu'il est important de rendre l'environnement de test sans stress.","title":"petite histoire"},{"location":"python/PySide2/","text":"Using .qrc Files (pyside2-rcc) Note The Qt Resource System is a mechanism for storing binary files in an application. The most common uses are for custom images, icons, fonts, among others. The .qrc file Before running any command, add information about the resources to a .qrc file. In the following example, notice how the resources are listed in icons.qrc </ ui > <! DOCTYPE RCC >< RCC version = \"1.0\" > < qresource > < file > icons / play . png </ file > < file > icons / pause . png </ file > < file > icons / stop . png </ file > < file > icons / previous . png </ file > < file > icons / forward . png </ file > </ qresource > </ RCC > Generating a Python file Now that the icons.qrc file is ready, use the pyside2-rcc tool to generate a Python class containing the binary information about the resources pyside2-rcc icons.rc -o rc_icons.py from PySide2.QtGui import QIcon , QKeySequence , QPixmap playIcon = QIcon ( QPixmap ( \":/icons/play.png\" )) previousIcon = QIcon ( QPixmap ( \":/icons/previous.png\" )) pauseIcon = QIcon ( QPixmap ( \":/icons/pause.png\" )) nextIcon = QIcon ( QPixmap ( \":/icons/forward.png\" )) stopIcon = QIcon ( QPixmap ( \":/icons/stop.png\" )) Using .ui file We've created a very simple UI. The next step is to get this into Python and use it to construct a working application. Loading the .ui file directly To load .ui files in PySide we first create a QUiLoader instance and then call the loader.load() method to load the UI file. import sys from PySide2 import QtCore , QtGui , QtWidgets from PySide2.QtUiTools import QUiLoader loader = QUiLoader () app = QtWidgets . QApplication ( sys . argv ) window = loader . load ( \"mainwindow.ui\" , None ) window . show () app . exec_ () Warning but slot-signal bindings, which are set in the designer in *.ui file, are not working anyway. So, for full-function use of designer GUI and slot-signal bindings, the only way I found is to compile *.ui file to python module with pyside UI compiler: Converting your .ui file to Python Instead of importing your .uic files into your application directly, you can instead convert them into Python code and then import them like any other module. To generate a Python output file run pyside2-uic from the command line, passing the .ui file and the target file for output, with a -o parameter. The following will generate a Python file named MainWindow.py which contains our created UI. pyside2-uic mainwindow.ui -o MainWindow.py import sys from PySide2 import QtWidgets from MainWindow import Ui_MainWindow class MainWindow ( QtWidgets . QMainWindow , Ui_MainWindow ): def __init__ ( self ): super ( MainWindow , self ) . __init__ () self . setupUi ( self ) app = QtWidgets . QApplication ( sys . argv ) window = MainWindow () window . show () app . exec_ ()","title":"PySide"},{"location":"python/PySide2/#using-qrc-files-pyside2-rcc","text":"Note The Qt Resource System is a mechanism for storing binary files in an application. The most common uses are for custom images, icons, fonts, among others. The .qrc file Before running any command, add information about the resources to a .qrc file. In the following example, notice how the resources are listed in icons.qrc </ ui > <! DOCTYPE RCC >< RCC version = \"1.0\" > < qresource > < file > icons / play . png </ file > < file > icons / pause . png </ file > < file > icons / stop . png </ file > < file > icons / previous . png </ file > < file > icons / forward . png </ file > </ qresource > </ RCC > Generating a Python file Now that the icons.qrc file is ready, use the pyside2-rcc tool to generate a Python class containing the binary information about the resources pyside2-rcc icons.rc -o rc_icons.py from PySide2.QtGui import QIcon , QKeySequence , QPixmap playIcon = QIcon ( QPixmap ( \":/icons/play.png\" )) previousIcon = QIcon ( QPixmap ( \":/icons/previous.png\" )) pauseIcon = QIcon ( QPixmap ( \":/icons/pause.png\" )) nextIcon = QIcon ( QPixmap ( \":/icons/forward.png\" )) stopIcon = QIcon ( QPixmap ( \":/icons/stop.png\" ))","title":"Using .qrc Files (pyside2-rcc)"},{"location":"python/PySide2/#using-ui-file","text":"We've created a very simple UI. The next step is to get this into Python and use it to construct a working application. Loading the .ui file directly To load .ui files in PySide we first create a QUiLoader instance and then call the loader.load() method to load the UI file. import sys from PySide2 import QtCore , QtGui , QtWidgets from PySide2.QtUiTools import QUiLoader loader = QUiLoader () app = QtWidgets . QApplication ( sys . argv ) window = loader . load ( \"mainwindow.ui\" , None ) window . show () app . exec_ () Warning but slot-signal bindings, which are set in the designer in *.ui file, are not working anyway. So, for full-function use of designer GUI and slot-signal bindings, the only way I found is to compile *.ui file to python module with pyside UI compiler: Converting your .ui file to Python Instead of importing your .uic files into your application directly, you can instead convert them into Python code and then import them like any other module. To generate a Python output file run pyside2-uic from the command line, passing the .ui file and the target file for output, with a -o parameter. The following will generate a Python file named MainWindow.py which contains our created UI. pyside2-uic mainwindow.ui -o MainWindow.py import sys from PySide2 import QtWidgets from MainWindow import Ui_MainWindow class MainWindow ( QtWidgets . QMainWindow , Ui_MainWindow ): def __init__ ( self ): super ( MainWindow , self ) . __init__ () self . setupUi ( self ) app = QtWidgets . QApplication ( sys . argv ) window = MainWindow () window . show () app . exec_ ()","title":"Using .ui file"},{"location":"python/packages/","text":"Using Requirement.txt pip install -r . \\r equirements.txt --force-reinstall pip wheel -w dist","title":"Packages"},{"location":"python/packages/#using-requirementtxt","text":"pip install -r . \\r equirements.txt --force-reinstall pip wheel -w dist","title":"Using Requirement.txt"},{"location":"python/python_intro/","text":"icon :fa-coffee: fontawesome :fa-building: :fa-android: :fa-copy: :fa-folder: :fa-angle-double-right: :fa-angle-right:","title":"Intro"},{"location":"python/python_intro/#icon","text":":fa-coffee:","title":"icon"},{"location":"python/python_intro/#fontawesome","text":":fa-building: :fa-android: :fa-copy: :fa-folder: :fa-angle-double-right: :fa-angle-right:","title":"fontawesome"},{"location":"python/python_tips/","text":":maya: :python: By Dhruv Govil When I write a Python class that exposes the functionality of a Maya node , I override py__repr__ so that it returns the node's path so I can just pass the object to any Maya commands and it just works. :GitHub: By Thomas Masencal If you append .patch to a Github commit URL, you get the patch file: https://github.com/KelSolaar/colour/commit/e38b3e706e4e3581dd4e9c7806fe84422abadac2.patch :maya: By Yantor3d TIL Maya can't load audio if the file basename starts with a number. https://twitter.com/yantor3d/status/1433464047278575622 :python: By Lee Dunham Remember that if you really want to use the nasty from foo import * it is worth considering using the all variable in foo to control what is going to be imported. https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python/64130#64130 :houdini: By Paul Ambrosiussen Did you know you can comment one or multiple lines of code in #Houdini using CTRL+/ ? (in the VEXpression editor) https://twitter.com/ambrosiussen_p/status/1463177572766863374?s=20 :Maya: By Stuart Maya's internal angular units are radians, even if the settings are set to degrees? And that's why the unitConversion gets added when both input and output are meant to be degrees ? Confirmed: addAttr -ln \"rotTest2\" -at doubleAngle -dv 0 |nurbsCircle1; and then connecting to a rotate attribute doesn't create an unitConversion node. https://tech-artists.slack.com/archives/C0AN0KPMZ/p1643739680183619 :Maya: By Mark Jackson You can have a string of any length but if stored in an UI element and the element is selected, the tring become clamped to 16bit = 32,767 characters. http://markj3d.blogspot.com/2012/11/maya-string-attr-32k-limit.html :Houdini: By Richard C Thomas Hou-ple, Pulling Lops into Sops? Want to grab data using these VEX methods\u2026 https://sidefx.com/docs/houdini/solaris/vex.html Don't forget to use op: ! I hope you never know the frustration of the last 2 hours. string stage = \"op:/stage/OUT\" ; matrix mat = usd_primvar ( stage , root , \"xformOP:transform\" , 0 ); https://twitter.com/DoesCG/status/1502363843208622097 :python: By Thomas Mansencal Remember that you can use a semi-colon as a \"line break\" in python. It is super useful to pass commands to the interpreter, e.g. pypython -c \u201cimport sys;import pprint;pprint(dir(sys))\u201d :GitHub: By Oleksii Holub GitHub now supports special \"warning\" and \"note\" blockquotes for callouts in your markdown content. > **Warning** > Some text https://twitter.com/Tyrrrz/status/1554784140326748161","title":"Tips"},{"location":"python/venv/","text":"ENVIRONNEMENT VIRTUEL Un environnement virtuel est un environnement d'ex\u00e9cution isol\u00e9. Les environnements virtuels sont utilis\u00e9s afin d'isoler les paquets utilis\u00e9s pour un projet. On peut ainsi avoir sur le m\u00eame ordinateur deux projets Python (ou plus) qui utilisent chacun une version diff\u00e9rente d'un m\u00eame paquet. Pour cr\u00e9er un environnement virtuel, on peut utiliser le module venv qui est inclus dans la biblioth\u00e8que standard de Python : python - m venv nom_de_lenvironnement L'environnement virtuel contient plusieurs dossiers et fichiers : \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 Activate . ps1 \u2502 \u251c\u2500\u2500 activate \u2502 \u251c\u2500\u2500 activate . csh \u2502 \u251c\u2500\u2500 activate . fish \u2502 \u251c\u2500\u2500 easy_install \u2502 \u251c\u2500\u2500 easy_install - 3.8 \u2502 \u251c\u2500\u2500 pip \u2502 \u251c\u2500\u2500 pip3 \u2502 \u251c\u2500\u2500 pip3 .8 \u2502 \u251c\u2500\u2500 python -> python3 \u2502 \u2514\u2500\u2500 python3 -> / Library / Developer / CommandLineTools / usr / bin / python3 \u251c\u2500\u2500 include \u251c\u2500\u2500 lib \u2502 \u2514\u2500\u2500 python3 .8 \u2502 \u2514\u2500\u2500 site - packages \u2514\u2500\u2500 pyvenv . cfg Le dossier bin contient l'interpr\u00e9teur Python et tous les ex\u00e9cutables dont vous pourriez avoir besoin (comme easy_install ou pip). C'est \u00e9galement dans ce dossier que vous trouverez les fichiers qui vous permettent d'activer votre environnement virtuel (activate). Pour activer votre environnement virtuel, il suffit donc de \u00ab sourcer \u00bb le fichier activate dans votre terminal : $ source bin/activate windows venvName/Scripts/activate.bat Le dossier include ne contient par d\u00e9faut aucun fichier. Ce dossier sert dans le cas de la cr\u00e9ation de biblioth\u00e8ques utilisant le langage C. Le dossier lib contient tous les paquets que vous installerez avec pip dans cet environnement virtuel (\u00e0 l'int\u00e9rieur du sous-dossier site-packages). Le fichier pyvenv.cfg contient des variables qui d\u00e9finissent certains param\u00e8tres de votre environnement virtuel, comme le chemin vers l'interpr\u00e9teur Python syst\u00e8me ou la version de Python de votre environnement virtuel.","title":"Virtual Environment"},{"location":"python/venv/#environnement-virtuel","text":"Un environnement virtuel est un environnement d'ex\u00e9cution isol\u00e9. Les environnements virtuels sont utilis\u00e9s afin d'isoler les paquets utilis\u00e9s pour un projet. On peut ainsi avoir sur le m\u00eame ordinateur deux projets Python (ou plus) qui utilisent chacun une version diff\u00e9rente d'un m\u00eame paquet. Pour cr\u00e9er un environnement virtuel, on peut utiliser le module venv qui est inclus dans la biblioth\u00e8que standard de Python : python - m venv nom_de_lenvironnement L'environnement virtuel contient plusieurs dossiers et fichiers : \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 Activate . ps1 \u2502 \u251c\u2500\u2500 activate \u2502 \u251c\u2500\u2500 activate . csh \u2502 \u251c\u2500\u2500 activate . fish \u2502 \u251c\u2500\u2500 easy_install \u2502 \u251c\u2500\u2500 easy_install - 3.8 \u2502 \u251c\u2500\u2500 pip \u2502 \u251c\u2500\u2500 pip3 \u2502 \u251c\u2500\u2500 pip3 .8 \u2502 \u251c\u2500\u2500 python -> python3 \u2502 \u2514\u2500\u2500 python3 -> / Library / Developer / CommandLineTools / usr / bin / python3 \u251c\u2500\u2500 include \u251c\u2500\u2500 lib \u2502 \u2514\u2500\u2500 python3 .8 \u2502 \u2514\u2500\u2500 site - packages \u2514\u2500\u2500 pyvenv . cfg Le dossier bin contient l'interpr\u00e9teur Python et tous les ex\u00e9cutables dont vous pourriez avoir besoin (comme easy_install ou pip). C'est \u00e9galement dans ce dossier que vous trouverez les fichiers qui vous permettent d'activer votre environnement virtuel (activate). Pour activer votre environnement virtuel, il suffit donc de \u00ab sourcer \u00bb le fichier activate dans votre terminal : $ source bin/activate windows venvName/Scripts/activate.bat Le dossier include ne contient par d\u00e9faut aucun fichier. Ce dossier sert dans le cas de la cr\u00e9ation de biblioth\u00e8ques utilisant le langage C. Le dossier lib contient tous les paquets que vous installerez avec pip dans cet environnement virtuel (\u00e0 l'int\u00e9rieur du sous-dossier site-packages). Le fichier pyvenv.cfg contient des variables qui d\u00e9finissent certains param\u00e8tres de votre environnement virtuel, comme le chemin vers l'interpr\u00e9teur Python syst\u00e8me ou la version de Python de votre environnement virtuel.","title":"ENVIRONNEMENT VIRTUEL"},{"location":"rendering/LPE/","text":"Introduction to Light Path Expressions Light Path Expressions (LPEs) are useful for outputting light into specific AOVs. LPEs describe the transport of light through the scene, starting from a source of light, bouncing between objects, and finally ending up at the camera. LPEs can be used to extract specific light contributions from Arnold into separate built-in or custom AOVs which can be output and recombined in various ways in a compositing package. A light path expression (LPE) is a type of regular expression that describes a specific light path (or set of paths) based on the scattering events. The expression can be used to \u201cselect\u201d the contributions of those kinds of paths to the output. You can use this to isolate very specific parts of the output when compositing, without having to write a custom shader or render that part as a separate pass. Light Path Expressions or LPEs are a very powerful and advanced tool for extracting specific lighting events from the scene to a separate channel. This allows for a very fine control of the image in compositing. For example, LPEs allow breaking the GI down to different types of light sources like lights, the environment and self-illuminating objects, or breaking down the GI to its separate bounces, or capturing only self-reflections, or the SSS that's only seen behind glass and similar for compositing control of only this aspect of the image. Light Path Expressions or LPEs are regular expressions describing light transport paths. They define paths initiated from camera that are bouncing all around the scene until they reach light sources. LPEs are extremely powerful as it is a mean to extract specific paths to output their results in custom AOVs without of relying on specific AOVs predefined by materials. This is extremely useful to extract all per light AOVs to relight in compositing for example or to output specific AOVs that are not available by materials. A light path expression is a regular expression representing the different vertices of a path between the camera and a light. Guerilla comes with many presets, but you can create your own expression. For example, CDL matches the direct lighting on a diffuse surface: a path of three events, each represented by a capital letter: Camera, Diffuse and a Light. This section is a formal definition of the light path expression language. Ones may be interested by the the default expressions available in Guerilla from nvidia iray docs 1Light path expressions 1.1Introduction Light path expressions (LPEs) describe the propagation of light through a scene, for example starting from a source of light, bouncing around between the objects of the scene and ultimately ending up at the eye. The paths that light takes through a scene are called light transport paths. LPEs may be used for example, to extract only specific light contributions from a renderer into separate image buffers. Light path expressions were first proposed as a description of light transport by Paul Heckbert [Heckbert90]. Heckbert suggested that regular expressions \u2014 typically used to describe patterns of characters in text \u2014 could also describe light transport events and paths. The alphabet of LPEs consists of event descriptions, that is, of interactions between light particles and the scene. 1.2Events Each event of a path, that is, each interaction of light with the scene objects and materials, is described by its type (for example, emission or reflection), the mode of scattering (for example, diffuse or specular), and an optional handle. A full event is described as < t m h >, where t is the event type, either R (reflection), T (transmission), or V (volume interaction), m is the scattering mode, either D (diffuse), G (glossy), or S (specular), and h is a handle in single quotes, for example. 'foo'. This position may be omitted from the specification. In that case, any handle is accepted. See below for details. Spaces are ignored unless they occur inside a handle string. The dot character (.) may be used as a wildcard in any position. It accepts any valid input. For example, a diffuse reflection event may be specified as , or, omitting the handle, . A specular transmission event identified with the handle \"window\" may be specified as . 1.3Handles Handles are strings of ASCII characters, enclosed in single quotes ('). The following characters must be escaped by prefixing them with a backslash inside handles: , ', and \". The assignment of a handle as a name for a scene element is typically made possible through the graphical interface of an application. 1.4Sets and exclusion As an alternative to the type, mode, and handle specifiers described above, each position of the event triple may contain a character set enclosed in square brackets. Any element of the set will be accepted. For example, <[RT]..> matches all reflection events and all transmission events. The complementary set is specified by first including the caret (^) character. For example, <.[^S]> matches any non-specular event and <..[^'ground']> matches any event that is not identified with the handle \"ground\". Event sets also work on full events. For instance, [ ] matches glossy reflection and specular transmission events. Note that this is different from <[RT][GS]>, which accepts glossy transmission and specular reflection in addition to the events accepted by the previous expression. 1.5Abbreviations In order to make specification of LPEs simpler, event descriptions may be abbreviated. An event in which only one of type, mode, or handle is explicitly specified may be replaced by that item. For example, may be abbreviated as R. Likewise, <..'foo'> may be abbreviated as 'foo'. Note the difference between , which accepts a single specular transmission event, and TS, which accepts an arbitrary transmission event followed by an arbitrary specular event. Finally, . matches any event except the special events described below. Abbreviation rules also apply to event sets, that is, [ <.S.>] reduces to [TS]. Again note that this is different from TS (without brackets). 1.6Constructing expressions LPEs may be constructed by combining event specifications through concatenation, alternation, and quantification. The following operators are supported and have the same semantics as they would for standard regular expressions. For expressions A and B and integers n and m with m >= n: AB Accepts first A, then B A|B Accepts A or B A? Optionally accepts A, that is, A may or may not be present A* Accepts any number of occurrences of A in sequence, including zero times A+ Accepts any non-empty sequence of A's. It is equivalent to AA* A{n} Accepts exactly n consecutive occurrences of A. For example, A{3} is equivalent to AAA. A{n,m} Accepts from n to m, inclusively, occurrences of A A{n,} Equivalent to A{n}A* The precedence from high to low is quantifiers (?, *, +, {}), concatenation, alternatives. Items can be grouped using normal parentheses ( and ). 1.7Special events Each LPE is delimited by special events for the eye (or camera) and light vertices. These events serve as markers and must be the first and last symbols in a LPE. LPEs may be specified starting either at the light or at the eye. All expressions must be constructed in such a way that every possible match has exactly one eye and one light vertex. This is due to the nature of LPEs: They describe light transport paths between one light and the eye. Note that this does not mean that light and eye markers must each show up exactly once. For example, \"E (D La | G Le)\" is correct, because either alternative has exactly one light and one eye marker: \"E D La\" and \"E G Le\". On the other hand \"E D La?\", \"E (D | La)\", and \"E (D | La) Le\" are ill-formed, because they would match paths with zero or two light markers. In the abbreviated form, the eye marker is simply E and the light marker is L. These items are special in that they represent two distinct characteristics: the shape of the light (or camera lens) and the mode of the emission distribution function. The full notation therefore differs from that of the standard events. In the full form, a light source as the first vertex of a transport path is described as < L h m h >, where L is the light type, and m and h are as before. The first pair of type and handle describes the light source itself. The type L can be one of Lp (point shape), La (area light), Le (environment or background), Lm (matte lookup), or L (any type). In the case of alpha expressions, Lms (matte shadows) is supported in addition to the aforementioned types. The second pair describes the light's directional characteristics, that is, its EDF. (This form loosely corresponds to the full-path notation introduced by Eric Veach in [Veach97], Section 8.3.2.) As before, the handles are optional. Furthermore, the EDF specification may be omitted. Thus, is equivalent to and La. Especially when dealing with irradiance (rather than radiance) render targets, it is convenient to use a special form of LPE, called irradiance expression. Such expressions contain an irradiance marker, rather than an eye marker. Using this marker, it is possible to describe light transport up to the point of the irradiance measurement, rather than up to the camera. The full form of the irradiance marker is , the abbreviated form is simply I. As before, h represents an optional handle. If set, irradiance will only be computed on those surfaces that have a matching handle. 1.8Advanced operations Several operations exist in order to make specifying the right expression easier. These operations do not add expressive power, but simplify certain tasks. When an application provides a means for multiple output images (or canvases) to be rendered at the same time, expressions may be re-used in subsequent canvas names. This is achieved by assigning a name to the expressions that should be re-used, for example: caustics: L.*SDE LE | $caustics In this example, the second canvas will receive both caustics and directly visible light sources. As illustrated above, variables are introduced by specifying the desired name, followed by a colon and the desired expression. Variable names may contain any positive number of alphanumeric characters and the underscore character, with the limitation that they may not start with a terminal symbol. Since all terminals of the LPE grammar start with capital letters, it is good practice to start variable names with lowercase letters. Note that sub-expressions cannot be captured by variable names. Variables are referenced by applying the dollar (or value-of) operator to the variable name. Expressions may be turned into their complement by prefixing them with the caret symbol. An expression of type ^A will yield all light transport paths that are not matched by A. Note that the complement operator cannot be applied to sub-expressions: \"^(L. E)\" is valid, but \"L^(. )E\" is not. It is possible to compute the intersection of two or more expressions by combining them with the ampersand symbol. Expressions of type A & B will match only those paths which are matched by both A and B. 1.9Matte object interaction The color of matte objects is determined by two types of interaction. The first is the lookup into the environment or backplate. This contribution is potentially shadowed by other objects in the scene and may be selected by expressions ending in Lm. Selection of such contributions can be further refined by specifying the handle of the matte object, for example, . The second type of contributions is made up of effects that synthetic objects and lights have on matte objects. This includes effects like synthetic lights illuminating matte objects and synthetic objects reflected by matte objects. For these contributions, matte objects behave exactly like synthetic objects. With regards to LPEs, matte lights illuminating synthetic objects behave exactly as if they were synthetic lights. 1.10LPEs for alpha control Some additional considerations are necessary when using LPEs to control alpha output. By definition, alpha is transparent (alpha 0) only for paths that match the provided expression. Note that this means that light transport paths which do not reach a light source or the environment because they are terminated prematurely (for whatever reason) are opaque. This is necessary to avoid undesired semi-transparent areas in the standard case. This has implications for the creation of object masks. Since the mask is supposed to be completely transparent also when undesired objects are hit by camera rays, these paths have to be captured by the expression even if they are terminated. This requires a special type of LPE, that is, one that captures terminated paths. Such LPEs are only allowed for alpha channels. For example, the expression \"E ([^'crate'] .*)? L?\" will render a mask for the object 'crate'. Shadows received by matte objects may also affect the opacity of the alpha channel. This opacity can be removed by capturing paths which end in Lms.\u203b Adding opacity in areas of matte shadow to the previously shown mask expression may be achieved by slightly changing the LPE to \"E ([^'crate'] .*) L? | E [^Lms]\". Note that presence or absence of Lms controls whether shadows which are received by a certain matte object make the alpha channel opaque. This affects all matte shadow, regardless of how it was cast, and by which objects. The API provides functions mi::neuraylib::IRendering_configuration::make_alpha_expression() and mi::neuraylib::IRendering_configuration::make_alpha_mask_expression() to generate various common alpha expressions, including masks. 1.11Example LPEs The universal light path expression, \"L .* E\", accepts all light transport paths. By default, this expression yields the same result as not using LPEs at all. Remember that this is equivalent to \"L. E\" (whitespace is ignored) and \"E. L\" (the expression can be reversed). Direct illumination can be requested by specifying \"L .? E\", or \"L . E\" if directly visible light sources are not desired. Indirect illumination is then specified by \"L .{2,} E\". Compositing workflows often use the concept of diffuse and reflection passes. They can be specified with the LPEs \"E L\" and \"E L\", respectively. Note that these passes as specified above do not contain indirect illumination. \"E .* L\" extends this to global illumination where the visible surfaces are diffuse. If only diffuse interactions are desired, \"E * L\" can be used. Caustics are usually described as \"E D S .* L\". The expression \"E D (S|G) .* L\" or \"E D [GS] .* L\" also considers glossy objects as caustics casters. If, for example, only specular reflection caustics cast by an object identified with a handle \"crate\" are desired, the expression is changed to \"E D .* L\". This can further be restricted to caustics cast onto \"ground\" from a spot light by changing the expression to \"E 'ground' .* \". Assuming an expression variable called caustics was defined in a previous expression, \"L.{2,5}E & ^$caustics\" will match any path that has the specified length and is not a caustic. 1.12Summary of typical LPEs Typical production workflow structures in digital compositing often employ a set of standard elements that can be represented by light path expressions. The following expressions define the color (RGB) component of rendering: LPE Description E D .* L Diffuse pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a diffuse event. E G .* L Glossy pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a glossy event. E S .* L Specular pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a specular event, that is, a mirror reflection or specular refraction. E D S .* L Diffuse part of caustics cast by a mirror reflection or specular refraction on another surface. E .* All direct and indirect light contribution coming from the key light group, which are all lights in the scene with the handle attribute set to key. E'crate'.*L All direct and indirect light falling onto any object in the scene with the handle attribute set to crate. The alpha channel can also be specified by light path expressions: LPE Description E [LmLe] Alpha is based solely on primary visibility. This is the approach used traditionally by many renderers. ET [LmLe] Transmitting objects make the alpha channel transparent. This is the default behavior. E [LmLe] Only specular transmission makes the alpha channel transparent. This avoids unexpected results in scenes with materials that have a diffuse transmission component. The graphical interface of an application may provide a way of naming and storing LPEs for reuse. Common LPEs like the above may also be part of a standard set of named LPEs in an application interface. 1.13Light path expression grammar L light E eye R reflection type T transmission type V volume interaction type D diffuse mode G glossy mode S specular mode 'h' handle h < type mode handle > event Lp point light type La area light type Le environment or background light type Lm matte lookup type Lms shadows cast onto matte objects (alpha expressions only) < light-type light-handle mode handle > light source full form < I h > irradiance marker type abbreviation for < type ..> mode abbreviation for <. mode .> handle abbreviation for <.. handle > I abbreviation for . match anything (in context) [ A \u2026 ] match any element in set [ ^A ] match all but A AB A followed by B A|B A or B A? zero or one A A* zero or more As A+ one or more As A{n} a sequence of n As A{n,m} n to m occurences of A A{n,} equivalent to A{n}A* ( \u2026 ) grouping ^ expression complement of expression expression-1 & expression-2 match both expressions name: expression assign expression to name $name use value of name","title":"LPE"},{"location":"rendering/LPE/#introduction-to-light-path-expressions","text":"Light Path Expressions (LPEs) are useful for outputting light into specific AOVs. LPEs describe the transport of light through the scene, starting from a source of light, bouncing between objects, and finally ending up at the camera. LPEs can be used to extract specific light contributions from Arnold into separate built-in or custom AOVs which can be output and recombined in various ways in a compositing package. A light path expression (LPE) is a type of regular expression that describes a specific light path (or set of paths) based on the scattering events. The expression can be used to \u201cselect\u201d the contributions of those kinds of paths to the output. You can use this to isolate very specific parts of the output when compositing, without having to write a custom shader or render that part as a separate pass. Light Path Expressions or LPEs are a very powerful and advanced tool for extracting specific lighting events from the scene to a separate channel. This allows for a very fine control of the image in compositing. For example, LPEs allow breaking the GI down to different types of light sources like lights, the environment and self-illuminating objects, or breaking down the GI to its separate bounces, or capturing only self-reflections, or the SSS that's only seen behind glass and similar for compositing control of only this aspect of the image. Light Path Expressions or LPEs are regular expressions describing light transport paths. They define paths initiated from camera that are bouncing all around the scene until they reach light sources. LPEs are extremely powerful as it is a mean to extract specific paths to output their results in custom AOVs without of relying on specific AOVs predefined by materials. This is extremely useful to extract all per light AOVs to relight in compositing for example or to output specific AOVs that are not available by materials. A light path expression is a regular expression representing the different vertices of a path between the camera and a light. Guerilla comes with many presets, but you can create your own expression. For example, CDL matches the direct lighting on a diffuse surface: a path of three events, each represented by a capital letter: Camera, Diffuse and a Light. This section is a formal definition of the light path expression language. Ones may be interested by the the default expressions available in Guerilla from nvidia iray docs 1Light path expressions 1.1Introduction Light path expressions (LPEs) describe the propagation of light through a scene, for example starting from a source of light, bouncing around between the objects of the scene and ultimately ending up at the eye. The paths that light takes through a scene are called light transport paths. LPEs may be used for example, to extract only specific light contributions from a renderer into separate image buffers. Light path expressions were first proposed as a description of light transport by Paul Heckbert [Heckbert90]. Heckbert suggested that regular expressions \u2014 typically used to describe patterns of characters in text \u2014 could also describe light transport events and paths. The alphabet of LPEs consists of event descriptions, that is, of interactions between light particles and the scene. 1.2Events Each event of a path, that is, each interaction of light with the scene objects and materials, is described by its type (for example, emission or reflection), the mode of scattering (for example, diffuse or specular), and an optional handle. A full event is described as < t m h >, where t is the event type, either R (reflection), T (transmission), or V (volume interaction), m is the scattering mode, either D (diffuse), G (glossy), or S (specular), and h is a handle in single quotes, for example. 'foo'. This position may be omitted from the specification. In that case, any handle is accepted. See below for details. Spaces are ignored unless they occur inside a handle string. The dot character (.) may be used as a wildcard in any position. It accepts any valid input. For example, a diffuse reflection event may be specified as , or, omitting the handle, . A specular transmission event identified with the handle \"window\" may be specified as . 1.3Handles Handles are strings of ASCII characters, enclosed in single quotes ('). The following characters must be escaped by prefixing them with a backslash inside handles: , ', and \". The assignment of a handle as a name for a scene element is typically made possible through the graphical interface of an application. 1.4Sets and exclusion As an alternative to the type, mode, and handle specifiers described above, each position of the event triple may contain a character set enclosed in square brackets. Any element of the set will be accepted. For example, <[RT]..> matches all reflection events and all transmission events. The complementary set is specified by first including the caret (^) character. For example, <.[^S]> matches any non-specular event and <..[^'ground']> matches any event that is not identified with the handle \"ground\". Event sets also work on full events. For instance, [ ] matches glossy reflection and specular transmission events. Note that this is different from <[RT][GS]>, which accepts glossy transmission and specular reflection in addition to the events accepted by the previous expression. 1.5Abbreviations In order to make specification of LPEs simpler, event descriptions may be abbreviated. An event in which only one of type, mode, or handle is explicitly specified may be replaced by that item. For example, may be abbreviated as R. Likewise, <..'foo'> may be abbreviated as 'foo'. Note the difference between , which accepts a single specular transmission event, and TS, which accepts an arbitrary transmission event followed by an arbitrary specular event. Finally, . matches any event except the special events described below. Abbreviation rules also apply to event sets, that is, [ <.S.>] reduces to [TS]. Again note that this is different from TS (without brackets). 1.6Constructing expressions LPEs may be constructed by combining event specifications through concatenation, alternation, and quantification. The following operators are supported and have the same semantics as they would for standard regular expressions. For expressions A and B and integers n and m with m >= n: AB Accepts first A, then B A|B Accepts A or B A? Optionally accepts A, that is, A may or may not be present A* Accepts any number of occurrences of A in sequence, including zero times A+ Accepts any non-empty sequence of A's. It is equivalent to AA* A{n} Accepts exactly n consecutive occurrences of A. For example, A{3} is equivalent to AAA. A{n,m} Accepts from n to m, inclusively, occurrences of A A{n,} Equivalent to A{n}A* The precedence from high to low is quantifiers (?, *, +, {}), concatenation, alternatives. Items can be grouped using normal parentheses ( and ). 1.7Special events Each LPE is delimited by special events for the eye (or camera) and light vertices. These events serve as markers and must be the first and last symbols in a LPE. LPEs may be specified starting either at the light or at the eye. All expressions must be constructed in such a way that every possible match has exactly one eye and one light vertex. This is due to the nature of LPEs: They describe light transport paths between one light and the eye. Note that this does not mean that light and eye markers must each show up exactly once. For example, \"E (D La | G Le)\" is correct, because either alternative has exactly one light and one eye marker: \"E D La\" and \"E G Le\". On the other hand \"E D La?\", \"E (D | La)\", and \"E (D | La) Le\" are ill-formed, because they would match paths with zero or two light markers. In the abbreviated form, the eye marker is simply E and the light marker is L. These items are special in that they represent two distinct characteristics: the shape of the light (or camera lens) and the mode of the emission distribution function. The full notation therefore differs from that of the standard events. In the full form, a light source as the first vertex of a transport path is described as < L h m h >, where L is the light type, and m and h are as before. The first pair of type and handle describes the light source itself. The type L can be one of Lp (point shape), La (area light), Le (environment or background), Lm (matte lookup), or L (any type). In the case of alpha expressions, Lms (matte shadows) is supported in addition to the aforementioned types. The second pair describes the light's directional characteristics, that is, its EDF. (This form loosely corresponds to the full-path notation introduced by Eric Veach in [Veach97], Section 8.3.2.) As before, the handles are optional. Furthermore, the EDF specification may be omitted. Thus, is equivalent to and La. Especially when dealing with irradiance (rather than radiance) render targets, it is convenient to use a special form of LPE, called irradiance expression. Such expressions contain an irradiance marker, rather than an eye marker. Using this marker, it is possible to describe light transport up to the point of the irradiance measurement, rather than up to the camera. The full form of the irradiance marker is , the abbreviated form is simply I. As before, h represents an optional handle. If set, irradiance will only be computed on those surfaces that have a matching handle. 1.8Advanced operations Several operations exist in order to make specifying the right expression easier. These operations do not add expressive power, but simplify certain tasks. When an application provides a means for multiple output images (or canvases) to be rendered at the same time, expressions may be re-used in subsequent canvas names. This is achieved by assigning a name to the expressions that should be re-used, for example: caustics: L.*SDE LE | $caustics In this example, the second canvas will receive both caustics and directly visible light sources. As illustrated above, variables are introduced by specifying the desired name, followed by a colon and the desired expression. Variable names may contain any positive number of alphanumeric characters and the underscore character, with the limitation that they may not start with a terminal symbol. Since all terminals of the LPE grammar start with capital letters, it is good practice to start variable names with lowercase letters. Note that sub-expressions cannot be captured by variable names. Variables are referenced by applying the dollar (or value-of) operator to the variable name. Expressions may be turned into their complement by prefixing them with the caret symbol. An expression of type ^A will yield all light transport paths that are not matched by A. Note that the complement operator cannot be applied to sub-expressions: \"^(L. E)\" is valid, but \"L^(. )E\" is not. It is possible to compute the intersection of two or more expressions by combining them with the ampersand symbol. Expressions of type A & B will match only those paths which are matched by both A and B. 1.9Matte object interaction The color of matte objects is determined by two types of interaction. The first is the lookup into the environment or backplate. This contribution is potentially shadowed by other objects in the scene and may be selected by expressions ending in Lm. Selection of such contributions can be further refined by specifying the handle of the matte object, for example, . The second type of contributions is made up of effects that synthetic objects and lights have on matte objects. This includes effects like synthetic lights illuminating matte objects and synthetic objects reflected by matte objects. For these contributions, matte objects behave exactly like synthetic objects. With regards to LPEs, matte lights illuminating synthetic objects behave exactly as if they were synthetic lights. 1.10LPEs for alpha control Some additional considerations are necessary when using LPEs to control alpha output. By definition, alpha is transparent (alpha 0) only for paths that match the provided expression. Note that this means that light transport paths which do not reach a light source or the environment because they are terminated prematurely (for whatever reason) are opaque. This is necessary to avoid undesired semi-transparent areas in the standard case. This has implications for the creation of object masks. Since the mask is supposed to be completely transparent also when undesired objects are hit by camera rays, these paths have to be captured by the expression even if they are terminated. This requires a special type of LPE, that is, one that captures terminated paths. Such LPEs are only allowed for alpha channels. For example, the expression \"E ([^'crate'] .*)? L?\" will render a mask for the object 'crate'. Shadows received by matte objects may also affect the opacity of the alpha channel. This opacity can be removed by capturing paths which end in Lms.\u203b Adding opacity in areas of matte shadow to the previously shown mask expression may be achieved by slightly changing the LPE to \"E ([^'crate'] .*) L? | E [^Lms]\". Note that presence or absence of Lms controls whether shadows which are received by a certain matte object make the alpha channel opaque. This affects all matte shadow, regardless of how it was cast, and by which objects. The API provides functions mi::neuraylib::IRendering_configuration::make_alpha_expression() and mi::neuraylib::IRendering_configuration::make_alpha_mask_expression() to generate various common alpha expressions, including masks. 1.11Example LPEs The universal light path expression, \"L .* E\", accepts all light transport paths. By default, this expression yields the same result as not using LPEs at all. Remember that this is equivalent to \"L. E\" (whitespace is ignored) and \"E. L\" (the expression can be reversed). Direct illumination can be requested by specifying \"L .? E\", or \"L . E\" if directly visible light sources are not desired. Indirect illumination is then specified by \"L .{2,} E\". Compositing workflows often use the concept of diffuse and reflection passes. They can be specified with the LPEs \"E L\" and \"E L\", respectively. Note that these passes as specified above do not contain indirect illumination. \"E .* L\" extends this to global illumination where the visible surfaces are diffuse. If only diffuse interactions are desired, \"E * L\" can be used. Caustics are usually described as \"E D S .* L\". The expression \"E D (S|G) .* L\" or \"E D [GS] .* L\" also considers glossy objects as caustics casters. If, for example, only specular reflection caustics cast by an object identified with a handle \"crate\" are desired, the expression is changed to \"E D .* L\". This can further be restricted to caustics cast onto \"ground\" from a spot light by changing the expression to \"E 'ground' .* \". Assuming an expression variable called caustics was defined in a previous expression, \"L.{2,5}E & ^$caustics\" will match any path that has the specified length and is not a caustic. 1.12Summary of typical LPEs Typical production workflow structures in digital compositing often employ a set of standard elements that can be represented by light path expressions. The following expressions define the color (RGB) component of rendering: LPE Description E D .* L Diffuse pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a diffuse event. E G .* L Glossy pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a glossy event. E S .* L Specular pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a specular event, that is, a mirror reflection or specular refraction. E D S .* L Diffuse part of caustics cast by a mirror reflection or specular refraction on another surface. E .* All direct and indirect light contribution coming from the key light group, which are all lights in the scene with the handle attribute set to key. E'crate'.*L All direct and indirect light falling onto any object in the scene with the handle attribute set to crate. The alpha channel can also be specified by light path expressions: LPE Description E [LmLe] Alpha is based solely on primary visibility. This is the approach used traditionally by many renderers. ET [LmLe] Transmitting objects make the alpha channel transparent. This is the default behavior. E [LmLe] Only specular transmission makes the alpha channel transparent. This avoids unexpected results in scenes with materials that have a diffuse transmission component. The graphical interface of an application may provide a way of naming and storing LPEs for reuse. Common LPEs like the above may also be part of a standard set of named LPEs in an application interface. 1.13Light path expression grammar L light E eye R reflection type T transmission type V volume interaction type D diffuse mode G glossy mode S specular mode 'h' handle h < type mode handle > event Lp point light type La area light type Le environment or background light type Lm matte lookup type Lms shadows cast onto matte objects (alpha expressions only) < light-type light-handle mode handle > light source full form < I h > irradiance marker type abbreviation for < type ..> mode abbreviation for <. mode .> handle abbreviation for <.. handle > I abbreviation for . match anything (in context) [ A \u2026 ] match any element in set [ ^A ] match all but A AB A followed by B A|B A or B A? zero or one A A* zero or more As A+ one or more As A{n} a sequence of n As A{n,m} n to m occurences of A A{n,} equivalent to A{n}A* ( \u2026 ) grouping ^ expression complement of expression expression-1 & expression-2 match both expressions name: expression assign expression to name $name use value of name","title":"Introduction to Light Path Expressions"},{"location":"rendering/Lexique/","text":"Rendering Irradiance L'irradiance est une mesure de la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9. Elle est g\u00e9n\u00e9ralement exprim\u00e9e en watts par m\u00e8tre carr\u00e9 (W/m\u00b2) et peut \u00eatre mesur\u00e9e \u00e0 l'aide d'un radiom\u00e8tre ou d'un spectrophotom\u00e8tre. L'irradiance est diff\u00e9rente de l'intensit\u00e9 lumineuse, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par une source lumineuse en un temps donn\u00e9 et est exprim\u00e9e en candelas (cd). L'irradiance est \u00e9galement diff\u00e9rente de l'illuminance, qui mesure la quantit\u00e9 de lumi\u00e8re qui atteint une surface en un temps donn\u00e9 et est exprim\u00e9e en lux (lx). Radiance La radiance est une mesure de la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par un corps en un temps donn\u00e9, per\u00e7ue dans une direction donn\u00e9e. Elle est g\u00e9n\u00e9ralement exprim\u00e9e en watts par m\u00e8tre carr\u00e9 par steradian (W/m\u00b2/sr) et peut \u00eatre mesur\u00e9e \u00e0 l'aide d'un radiom\u00e8tre ou d'un spectrophotom\u00e8tre. La radiance est diff\u00e9rente de l'intensit\u00e9 lumineuse, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par une source lumineuse en un temps donn\u00e9 et est exprim\u00e9e en candelas (cd). La radiance est \u00e9galement diff\u00e9rente de l'irradiance, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9 et est exprim\u00e9e en watts par m\u00e8tre carr\u00e9 (W/m\u00b2). La radiance est un concept important en optique, en astrophysique et en m\u00e9t\u00e9orologie, car elle permet de quantifier la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par des corps c\u00e9lestes, des nuages, des atmosph\u00e8res, etc. Flux Le flux est une mesure de la quantit\u00e9 d'\u00e9nergie, de mati\u00e8re, de particules ou d'autres grandeurs qui traversent une surface en un temps donn\u00e9. Le flux est g\u00e9n\u00e9ralement exprim\u00e9 en unit\u00e9s de mesure appropri\u00e9es (par exemple, en watts pour l'\u00e9nergie, en kilogrammes pour la mati\u00e8re, en particules par seconde pour les particules, etc.) et peut \u00eatre mesur\u00e9 \u00e0 l'aide d'un instrument appropri\u00e9 (comme un wattm\u00e8tre pour l'\u00e9nergie, une balance pour la mati\u00e8re, un compteur de particules pour les particules, etc.). Le flux peut \u00eatre un concept important en physique, en chimie, en biologie, en ing\u00e9nierie, en m\u00e9t\u00e9orologie, etc., car il permet de quantifier la quantit\u00e9 de diff\u00e9rentes grandeurs qui traversent une surface en un temps donn\u00e9. Par exemple, le flux solaire mesure la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9, le flux de chaleur mesure la quantit\u00e9 de chaleur qui traverse une surface en un temps donn\u00e9, le flux de particules radioactive mesure la quantit\u00e9 de particules radioactive qui traversent une surface en un temps donn\u00e9, etc.","title":"Lexique"},{"location":"rendering/Lexique/#rendering","text":"","title":"Rendering"},{"location":"rendering/Lexique/#irradiance","text":"L'irradiance est une mesure de la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9. Elle est g\u00e9n\u00e9ralement exprim\u00e9e en watts par m\u00e8tre carr\u00e9 (W/m\u00b2) et peut \u00eatre mesur\u00e9e \u00e0 l'aide d'un radiom\u00e8tre ou d'un spectrophotom\u00e8tre. L'irradiance est diff\u00e9rente de l'intensit\u00e9 lumineuse, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par une source lumineuse en un temps donn\u00e9 et est exprim\u00e9e en candelas (cd). L'irradiance est \u00e9galement diff\u00e9rente de l'illuminance, qui mesure la quantit\u00e9 de lumi\u00e8re qui atteint une surface en un temps donn\u00e9 et est exprim\u00e9e en lux (lx).","title":"Irradiance"},{"location":"rendering/Lexique/#radiance","text":"La radiance est une mesure de la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par un corps en un temps donn\u00e9, per\u00e7ue dans une direction donn\u00e9e. Elle est g\u00e9n\u00e9ralement exprim\u00e9e en watts par m\u00e8tre carr\u00e9 par steradian (W/m\u00b2/sr) et peut \u00eatre mesur\u00e9e \u00e0 l'aide d'un radiom\u00e8tre ou d'un spectrophotom\u00e8tre. La radiance est diff\u00e9rente de l'intensit\u00e9 lumineuse, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par une source lumineuse en un temps donn\u00e9 et est exprim\u00e9e en candelas (cd). La radiance est \u00e9galement diff\u00e9rente de l'irradiance, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9 et est exprim\u00e9e en watts par m\u00e8tre carr\u00e9 (W/m\u00b2). La radiance est un concept important en optique, en astrophysique et en m\u00e9t\u00e9orologie, car elle permet de quantifier la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par des corps c\u00e9lestes, des nuages, des atmosph\u00e8res, etc.","title":"Radiance"},{"location":"rendering/Lexique/#flux","text":"Le flux est une mesure de la quantit\u00e9 d'\u00e9nergie, de mati\u00e8re, de particules ou d'autres grandeurs qui traversent une surface en un temps donn\u00e9. Le flux est g\u00e9n\u00e9ralement exprim\u00e9 en unit\u00e9s de mesure appropri\u00e9es (par exemple, en watts pour l'\u00e9nergie, en kilogrammes pour la mati\u00e8re, en particules par seconde pour les particules, etc.) et peut \u00eatre mesur\u00e9 \u00e0 l'aide d'un instrument appropri\u00e9 (comme un wattm\u00e8tre pour l'\u00e9nergie, une balance pour la mati\u00e8re, un compteur de particules pour les particules, etc.). Le flux peut \u00eatre un concept important en physique, en chimie, en biologie, en ing\u00e9nierie, en m\u00e9t\u00e9orologie, etc., car il permet de quantifier la quantit\u00e9 de diff\u00e9rentes grandeurs qui traversent une surface en un temps donn\u00e9. Par exemple, le flux solaire mesure la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9, le flux de chaleur mesure la quantit\u00e9 de chaleur qui traverse une surface en un temps donn\u00e9, le flux de particules radioactive mesure la quantit\u00e9 de particules radioactive qui traversent une surface en un temps donn\u00e9, etc.","title":"Flux"},{"location":"rendering/OSL_samples/","text":"Simple Skin shader shader my_shader ( vector I , vector N , vector T , float sigma_a , float sigma_s , float g , float roughness , color albedo = color ( 1 , 1 , 1 ), float ior = 1.4 , float multiscattering = 0.5 , float melanin = 1.0 , float subsurface_scale = 1.0 , float specular_scale = 1.0 , float diffuse_scale = 1.0 , float melanin_scale = 1.0 , float scattering_model = 0 , color sigma_prime_s_coeff = color ( 0.74 , 0.88 , 1.01 ), color sigma_prime_a_coeff = color ( 0.0014 , 0.0025 , 0.0142 ), output color result = color ( 0 , 0 , 0 ) ) { // Calculate the halfway vector vector H = normalize ( I + N ); // Calculate the diffuse term color diffuse = albedo / M_PI ; // Calculate the specular term color specular = pow ( max ( dot ( H , N ), 0.0 ), 5 ); // Calculate the Beckmann term color beckmann = exp ( - pow ( dot ( N , H ), 2 ) / ( pow ( tan ( acos ( dot ( N , H ))), 2 ) * pow ( roughness , 2 ))) / ( M_PI * pow ( roughness , 2 ) * pow ( dot ( N , H ), 4 )); // Calculate the absorption term color absorption = exp ( -2 * ( sigma_a + sigma_s ) * dot ( N , H )); // Calculate the reduced scattering term color reduced_scattering = ( 1 - g ) / ( 1 + ( 1 - g ) * pow ( dot ( N , H ), 2 )); // Calculate the phase function term color phase_function = 0.25 / M_PI * ( 1 + pow ( dot ( N , H ), 2 )); // Calculate the Fresnel term color fresnel = ( 1 - pow ( ior , 2 )) / ( 1 + pow ( ior , 2 ) - 2 * ior * dot ( I , H )); // Calculate the melanin term color melanin_term = melanin * ( sigma_a + sigma_s ); // Calculate the spectral model for skin scattering color spectral_model = 0 ; if ( scattering_model == 0 ) { // Calculate the spectral model using the skin_scattering function spectral_model = skin_scattering ( N , H , sigma_prime_s_coeff , sigma_prime_a_coeff ); } else if ( scattering_model == 1 ) { // Calculate the spectral model using the Henyey-Greenstein model spectral_model = henyey_greenstein ( N , H , g ); } else if ( scattering_model == 2 ) { // Calculate the spectral model using the Schlick model spectral_model = schlick ( N , H , g ); } // Calculate the multiple scattering term color multiple_scattering = exp ( - multiscattering * ( sigma_a + sigma_s )); // Calculate the final color result = ( diffuse + specular * beckmann * absorption * reduced_scattering * phase_function * fresnel * spectral_model * multiple_scattering ) / sigma_t ; // Scale the diffuse, specular, subsurface, and melanin contributions result *= diffuse_scale ; result *= specular_scale ; result += melanin_term * melanin_scale ; result += melanin_term * subsurface_scale ; // Calculate the tangent space basis vectors vector U = normalize ( T - dot ( T , N ) * N ); vector V = normalize ( cross ( N , U )); // Calculate the tangent space direction of the incident light vector I_tangent = vector ( dot ( I , U ), dot ( I , V ), dot ( I , N )); // Calculate the BRDF in tangent space color brdf_tangent = ( diffuse + specular * beckmann * absorption * reduced_scattering * phase_function * fresnel * spectral_model * multiple_scattering ) / sigma_t ; // Transform the BRDF back to world space result = color ( dot ( brdf_tangent , vector ( U . x , V . x , N . x )), dot ( brdf_tangent , vector ( U . y , V . y , N . y )), dot ( brdf_tangent , vector ( U . z , V . z , N . z )) ); } // Function to calculate the spectral model for skin scattering color skin_scattering ( vector N , vector H , color sigma_prime_s_coeff , color sigma_prime_a_coeff ) { // Calculate the skin scattering coefficients color sigma_prime_s = sigma_prime_s_coeff * ( 1 + 0.01 * pow ( dot ( N , H ), 2 )); color sigma_prime_a = sigma_prime_a_coeff * ( 1 + 0.0001 * pow ( dot ( N , H ), 2 )); // Calculate the reduced scattering term color reduced_scattering = ( 1 - g ) / ( 1 + ( 1 - g ) * pow ( dot ( N , H ), 2 )); // Calculate the spectral model for skin scattering return sigma_prime_s / ( sigma_prime_s + sigma_prime_a ); } // Function to calculate the spectral model using the Henyey-Greenstein model color henyey_greenstein ( vector N , vector H , float g ) { return 1 / ( 4 * M_PI ) * ( 1 - pow ( g , 2 )) / pow ( 1 + pow ( g , 2 ) - 2 * g * dot ( N , H ), 1.5 ); } // Function to calculate the spectral model using the Schlick model color schlick ( vector N , vector H , float g ) { return 1 / ( 4 * M_PI ) * ( 1 + pow ( g , 2 )) / pow ( 1 + pow ( g , 2 ) - 2 * g * dot ( N , H ), 1.5 ); } In this updated implementation, the scattering_model parameter allows the user to choose between three different models for calculating the spectral model: the default skin scattering model, the Henyey-Greenstein model, or the Schlick model. The sigma_prime_s_coeff and sigma_prime_a_coeff parameters allow the user to adjust the coefficients for the sigma prime s and sigma prime a values in the skin scattering model. Additionally, the multiscattering parameter allows the user to adjust the amount of multiple scattering applied to the skin, and the subsurface_scale, specular_scale, diffuse_scale, and melanin_scale parameters allow the user to adjust the relative contributions of the subsurface, specular, diffuse, and melanin terms in the final color calculation. A Spectral BSSRDF for Shading Human Skin\" and \"Practical and Controllable Subsurface Scattering for Production Path Tracing\" Probability shader shader my_shader ( output color result = color ( 0 , 0 , 0 ) ) { // Use the rand() function to generate a random value between 0 and 1 float random = rand (); // Use the random value to determine the output color if ( random < 0.33 ) { // Output red result = color ( 1 , 0 , 0 ); } else if ( random < 0.66 ) { // Output green result = color ( 0 , 1 , 0 ); } else { // Output blue result = color ( 0 , 0 , 1 ); } }","title":"OSLSamples"},{"location":"rendering/OSL_samples/#simple-skin-shader","text":"shader my_shader ( vector I , vector N , vector T , float sigma_a , float sigma_s , float g , float roughness , color albedo = color ( 1 , 1 , 1 ), float ior = 1.4 , float multiscattering = 0.5 , float melanin = 1.0 , float subsurface_scale = 1.0 , float specular_scale = 1.0 , float diffuse_scale = 1.0 , float melanin_scale = 1.0 , float scattering_model = 0 , color sigma_prime_s_coeff = color ( 0.74 , 0.88 , 1.01 ), color sigma_prime_a_coeff = color ( 0.0014 , 0.0025 , 0.0142 ), output color result = color ( 0 , 0 , 0 ) ) { // Calculate the halfway vector vector H = normalize ( I + N ); // Calculate the diffuse term color diffuse = albedo / M_PI ; // Calculate the specular term color specular = pow ( max ( dot ( H , N ), 0.0 ), 5 ); // Calculate the Beckmann term color beckmann = exp ( - pow ( dot ( N , H ), 2 ) / ( pow ( tan ( acos ( dot ( N , H ))), 2 ) * pow ( roughness , 2 ))) / ( M_PI * pow ( roughness , 2 ) * pow ( dot ( N , H ), 4 )); // Calculate the absorption term color absorption = exp ( -2 * ( sigma_a + sigma_s ) * dot ( N , H )); // Calculate the reduced scattering term color reduced_scattering = ( 1 - g ) / ( 1 + ( 1 - g ) * pow ( dot ( N , H ), 2 )); // Calculate the phase function term color phase_function = 0.25 / M_PI * ( 1 + pow ( dot ( N , H ), 2 )); // Calculate the Fresnel term color fresnel = ( 1 - pow ( ior , 2 )) / ( 1 + pow ( ior , 2 ) - 2 * ior * dot ( I , H )); // Calculate the melanin term color melanin_term = melanin * ( sigma_a + sigma_s ); // Calculate the spectral model for skin scattering color spectral_model = 0 ; if ( scattering_model == 0 ) { // Calculate the spectral model using the skin_scattering function spectral_model = skin_scattering ( N , H , sigma_prime_s_coeff , sigma_prime_a_coeff ); } else if ( scattering_model == 1 ) { // Calculate the spectral model using the Henyey-Greenstein model spectral_model = henyey_greenstein ( N , H , g ); } else if ( scattering_model == 2 ) { // Calculate the spectral model using the Schlick model spectral_model = schlick ( N , H , g ); } // Calculate the multiple scattering term color multiple_scattering = exp ( - multiscattering * ( sigma_a + sigma_s )); // Calculate the final color result = ( diffuse + specular * beckmann * absorption * reduced_scattering * phase_function * fresnel * spectral_model * multiple_scattering ) / sigma_t ; // Scale the diffuse, specular, subsurface, and melanin contributions result *= diffuse_scale ; result *= specular_scale ; result += melanin_term * melanin_scale ; result += melanin_term * subsurface_scale ; // Calculate the tangent space basis vectors vector U = normalize ( T - dot ( T , N ) * N ); vector V = normalize ( cross ( N , U )); // Calculate the tangent space direction of the incident light vector I_tangent = vector ( dot ( I , U ), dot ( I , V ), dot ( I , N )); // Calculate the BRDF in tangent space color brdf_tangent = ( diffuse + specular * beckmann * absorption * reduced_scattering * phase_function * fresnel * spectral_model * multiple_scattering ) / sigma_t ; // Transform the BRDF back to world space result = color ( dot ( brdf_tangent , vector ( U . x , V . x , N . x )), dot ( brdf_tangent , vector ( U . y , V . y , N . y )), dot ( brdf_tangent , vector ( U . z , V . z , N . z )) ); } // Function to calculate the spectral model for skin scattering color skin_scattering ( vector N , vector H , color sigma_prime_s_coeff , color sigma_prime_a_coeff ) { // Calculate the skin scattering coefficients color sigma_prime_s = sigma_prime_s_coeff * ( 1 + 0.01 * pow ( dot ( N , H ), 2 )); color sigma_prime_a = sigma_prime_a_coeff * ( 1 + 0.0001 * pow ( dot ( N , H ), 2 )); // Calculate the reduced scattering term color reduced_scattering = ( 1 - g ) / ( 1 + ( 1 - g ) * pow ( dot ( N , H ), 2 )); // Calculate the spectral model for skin scattering return sigma_prime_s / ( sigma_prime_s + sigma_prime_a ); } // Function to calculate the spectral model using the Henyey-Greenstein model color henyey_greenstein ( vector N , vector H , float g ) { return 1 / ( 4 * M_PI ) * ( 1 - pow ( g , 2 )) / pow ( 1 + pow ( g , 2 ) - 2 * g * dot ( N , H ), 1.5 ); } // Function to calculate the spectral model using the Schlick model color schlick ( vector N , vector H , float g ) { return 1 / ( 4 * M_PI ) * ( 1 + pow ( g , 2 )) / pow ( 1 + pow ( g , 2 ) - 2 * g * dot ( N , H ), 1.5 ); } In this updated implementation, the scattering_model parameter allows the user to choose between three different models for calculating the spectral model: the default skin scattering model, the Henyey-Greenstein model, or the Schlick model. The sigma_prime_s_coeff and sigma_prime_a_coeff parameters allow the user to adjust the coefficients for the sigma prime s and sigma prime a values in the skin scattering model. Additionally, the multiscattering parameter allows the user to adjust the amount of multiple scattering applied to the skin, and the subsurface_scale, specular_scale, diffuse_scale, and melanin_scale parameters allow the user to adjust the relative contributions of the subsurface, specular, diffuse, and melanin terms in the final color calculation. A Spectral BSSRDF for Shading Human Skin\" and \"Practical and Controllable Subsurface Scattering for Production Path Tracing\"","title":"Simple Skin shader"},{"location":"rendering/OSL_samples/#probability-shader","text":"shader my_shader ( output color result = color ( 0 , 0 , 0 ) ) { // Use the rand() function to generate a random value between 0 and 1 float random = rand (); // Use the random value to determine the output color if ( random < 0.33 ) { // Output red result = color ( 1 , 0 , 0 ); } else if ( random < 0.66 ) { // Output green result = color ( 0 , 1 , 0 ); } else { // Output blue result = color ( 0 , 0 , 1 ); } }","title":"Probability shader"},{"location":"rendering/Raytracing101/","text":"","title":"Raytracing"},{"location":"vex/Snippets/","text":"Note Point Wrangle Bisects a vector fixing the first and last points // Get the next points N vector prevPointP = point ( 0 , 'P' , @ ptnum + 1 ); vector nextPointP = point ( 0 , 'P' , @ ptnum - 1 ); // Create the bisected vector v @ N = cross ( normalize ( nextPointP - prevPointP ), v @ up ); // Check if it's the fist point if ( @ ptnum == 0 ){ // Create the bisected vector v @ N = - cross ( normalize ( prevPointP - @ P ), v @ up ); } // Check if it's the last point if ( @ ptnum == @ numpt - 1 ){ vector vec = normalize ( - cross ( @ P - point ( 0 , 'P' , @ ptnum - 1 ), { 0 , 1 , 0 })); @ N = vec ; } Note Prim Wrangle Makes a primitive polywire sag good idea to resample it first int Count , Pt_List []; float Fitted_Pt_Num , Ramped_Pt_Num ; vector Modifier ; Pt_List = primpoints ( geoself (), 0 ); for ( Count = 0 ; Count < len ( Pt_List ); Count ++ ){ Modifier = set ( 0 , 0 , 0 ); Fitted_Pt_Num = efit ( Count , 0 , ( len ( Pt_List ) - 1 ), 0 , 1 ); Ramped_Pt_Num = chramp ( \"sag_control\" , Fitted_Pt_Num ); Modifier . y = - ( Ramped_Pt_Num * chf ( \"sag_multiplier\" )); //Modifier = set(1,Ramped_Pt_Num,1); setpointattrib ( geoself (), \"P\" , Pt_List [ Count ], Modifier , \"add\" ); } Note Prim wrangle (Points Line N at next point) Get Point Numbers (Should only ever be 2) int points [] = primpoints ( 0 , @ primnum ); // resize(points, 2); // Get P of each point vector points_P []; foreach ( int point ; points ) { vector P = point ( 0 , 'P' , point ); push ( points_P , P ); } setpointattrib ( 0 , 'N' , points [ 0 ], ( points_P [ 1 ] - points_P [ 0 ])); setpointattrib ( 0 , 'N' , points [ 1 ], ( points_P [ 0 ] - points_P [ 1 ])); Note Look for a value in an array You should use find() instead int contains ( int array []; int value ){ foreach ( int check_val ; array ){ if ( check_val == value ){ return 1 ; } } return 0 ; } Note Prim Wrangle This vex wrangle will find all the connected prim neighbors of an object int prim_edge , edge , prim , i , n , num ; string neighbours = \"\" ; i = 0 ; prim_edge = primhedge ( @ OpInput1 , @ primnum ); while ( i < primvertexcount ( @ OpInput1 , @ primnum )) { num = hedge_equivcount ( @ OpInput1 , prim_edge ); n = 0 ; while ( n < num ) { edge = hedge_nextequiv ( @ OpInput1 , prim_edge ); prim = hedge_prim ( @ OpInput1 , edge ); if ( prim != @ primnum ) neighbours += sprintf ( \"%g \" , prim ); prim_edge = edge ; n ++ ; } prim_edge = hedge_next ( @ OpInput1 , prim_edge ); i ++ ; } s @ neighbours = neighbours ; Note If a prim @Cd is red return its primnum int has_red_neighbor ( string primnums ) { foreach ( string s_primnum ; split ( primnums )) { int primnum = atoi ( s_primnum ); if ( prim ( 0 , ' Cd ' , primnum ) == set ( 1 , 0 , 0 )) return primnum ; } return -1 ; } int find_non_outerwall_neighbor ( string primnums ) { foreach ( string s_primnum ; split ( primnums )) { int primnum = atoi ( s_primnum ); if ( prim ( 0 , ' group_outer_walls ' , primnum ) == 0 ) return primnum ; } return -1 ; } Note Get an interger list of the primitive attribute neighbors int [] get_neighbors_from_primnum ( int primnum ) { int int_neighbors [] = array (); string neighbors [] = split ( prim ( 0 , ' neighbors ' , primnum )); foreach ( string n_primnum ; neighbors ) { push ( int_neighbors , atoi ( n_primnum )); } return int_neighbors ; } Note If a primitive has any neighbor primitive points that share a point with any other neighbor that piece is an elbow piece int is_elbow ( int primnum ) { int seen_points [] = array (); // Get the neighbors int neighbors [] = get_neighbors_from_primnum ( primnum ); foreach ( int n_primnum ; neighbors ) { // get the neigbor points int neighbor_points [] = primpoints ( 0 , n_primnum ); foreach ( int pointnum ; neighbor_points ) { // If we've seen the point return immediatly if ( find ( seen_points , pointnum ) >= 0 ) { return 1 ; } // add the seen points to the arry push ( seen_points , pointnum ); } } return 0 ; } int [] split_neighbors_to_int_array ( string neighbors ) { int ret_arr [] = array (); foreach ( string primnum ; split ( neighbors )) { push ( ret_arr , atoi ( primnum )); } return ret_arr ; } int [] get_blue_neighbors ( string str_neighbors ) { int blue_neighbors [] = array (); int neighbors [] = split_neighbors_to_int_array ( str_neighbors ); foreach ( int primnum ; neighbors ) { if ( prim ( 0 , ' Cd ' , primnum ) == set ( 0 , 0 , 1 )) { push ( blue_neighbors , primnum ); } } return blue_neighbors ; } vector get_first_blue_neighbors_vector ( int blue_neighbors []) { vector facing_arr [] = array (); foreach ( int primnum ; blue_neighbors ) { vector facing = prim ( 0 , ' facing ' , primnum ); push ( facing_arr , facing ); } return facing_arr [ 1 ]; } int find_first_unseen ( int seen_prims []; int neighbors []) { foreach ( int neighbor ; neighbors ) { if ( find ( seen_prims , neighbor ) >= 0 ) {} else { return neighbor ; } } } Note OpInput1: is usually just @OpInput1 and primnum is @primnum string neighbor_prims ( string OpInput1 ; int primnum ) { int prim_edge , edge , prim , i , n , num ; int neighbours_arr [] = array (); string neighbors = \"\" ; i = 0 ; prim_edge = primhedge ( OpInput1 , primnum ); while ( i < primvertexcount ( OpInput1 , primnum )) { num = hedge_equivcount ( OpInput1 , prim_edge ); n = 0 ; while ( n < num ) { edge = hedge_nextequiv ( OpInput1 , prim_edge ); prim = hedge_prim ( OpInput1 , edge ); if ( prim != primnum ) { neighbors += sprintf ( \"%s \" , prim ); push ( neighbours_arr , prim );; } prim_edge = edge ; n ++ ; } prim_edge = hedge_next ( OpInput1 , prim_edge ); i ++ ; } return neighbors ; } Note OpInput1: is usually just @OpInput1 and primnum is @primnum int [] neighbor_prims_i ( string OpInput1 ; int primnum ) { int prim_edge , edge , prim , i , n , num ; int neighbours_arr [] = array (); i = 0 ; prim_edge = primhedge ( OpInput1 , primnum ); while ( i < primvertexcount ( OpInput1 , primnum )) { num = hedge_equivcount ( OpInput1 , prim_edge ); n = 0 ; while ( n < num ) { edge = hedge_nextequiv ( OpInput1 , prim_edge ); prim = hedge_prim ( OpInput1 , edge ); if ( prim != primnum ) { push ( neighbours_arr , prim );; } prim_edge = edge ; n ++ ; } prim_edge = hedge_next ( OpInput1 , prim_edge ); i ++ ; } return neighbours_arr ; } vector get_direction_from ( int primnum1 ; int primnum2 ){ vector direction = prim ( 0 , 'P' , primnum2 ) - prim ( 0 , 'P' , primnum1 ); return normalize ( set ( direction . x , direction . y , direction . z )); } int is_left ( int primnum ; int neighbor_primnum ){ if ( get_direction_from ( primnum , neighbor_primnum ) == set ( 1 , 0 , 0 ) ){ return 1 ; } return 0 ; } int is_right ( int primnum ; int neighbor_primnum ){ if ( get_direction_from ( primnum , neighbor_primnum ) == set ( -1 , 0 , 0 ) ){ return 1 ; } return 0 ; } ``` ``` c int is_above ( int primnum ; int neighbor_primnum ){ if ( get_direction_from ( primnum , neighbor_primnum ) == set ( 0 , 0 , 1 ) ){ return 1 ; } return 0 ; } int is_below ( int primnum ; int neighbor_primnum ){ vector dir = get_direction_from ( primnum , neighbor_primnum ); if ( dir == set ( 0 , 0 , -1 )){ return 1 ; } return 0 ; } int has_neighbors_above ( int primnum ){ int neighbors [] = get_neighbors_from_primnum ( primnum ); foreach ( int neighbor_prim ; neighbors ){ if ( is_above ( primnum , neighbor_prim )){ return 1 ; } } return 0 ; } int has_neighbors_left ( int primnum ){ int neighbors [] = get_neighbors_from_primnum ( primnum ); foreach ( int neighbor_prim ; neighbors ){ if ( is_left ( primnum , neighbor_prim )){ return 1 ; } } return 0 ; } int has_neighbors_right ( int primnum ){ int neighbors [] = get_neighbors_from_primnum ( primnum ); foreach ( int neighbor_prim ; neighbors ){ if ( is_right ( primnum , neighbor_prim )){ return 1 ; } } return 0 ; } int has_neighbors_below ( int primnum ){ int neighbors [] = get_neighbors_from_primnum ( primnum ); foreach ( int neighbor_prim ; neighbors ){ if ( is_below ( primnum , neighbor_prim )){ return 1 ; } } return 0 ; } int [] get_shape_array ( int primnum ){ int shape_array [] = array (); push ( shape_array , has_neighbors_left ( primnum )); push ( shape_array , has_neighbors_right ( primnum )); push ( shape_array , has_neighbors_above ( primnum )); push ( shape_array , has_neighbors_below ( primnum )); return shape_array ; } int array_equivelant ( int arr1 []; int arr2 []){ int i = 0 ; foreach ( int val ; arr2 ) { // If a value doesn't match return back false if ( val != arr1 [ i ]){ return 0 ; } i += 1 ; } // All the values match return true return 1 ; }","title":"Snippets"},{"location":"vex/vex/","text":"Welcome to Vex Global Variables A list of variables available in wrangles. Note The type indicator isn't necessary, but included as a reminder. Available in all SOP wrangles f @ Frame //The current floating frame number, equivalent to the $FF Hscript variable f @ Time //The current time in seconds, equivalent to the $T Hscript variable i @ SimFrame //The integer simulation timestep number ($SF), only present in DOP contexts. f @ SimTime //The simulation time in seconds ($ST), only present in DOP contexts. f @ TimeInc //The timestep currently being used for simulation or playback. Available in Attribute Wrangle ( point , vertex , primitive and detail ) v @ P //The position of the current element. i @ ptnum //The point number attached to the currently processed element. i @ vtxnum //The linear number of the currently processed vertex. i @ primnum //The primitive number attached to the currently processed element. i @ elemnum //The index number of the currently processed element. i @ numpt //The total number of points in the geometry. i @ numvtx //The number of vertices in the primitive of the currently processed element. i @ numprim //The total number of primitives in the geometry. i @ numelem //The total number of elements being processed. Available in Volume Wrangle v @ P //The position of the current voxel. f @ density //The value of the density field at the current voxel location. v @ center //The center of the current volume. v @ dPdx , v @ dPdy , v @ dPdz //These vectors store the change in P that occurs in the x, y, and z voxel indices. i @ ix , i @ iy , i @ iz //Voxel indices. For dense volumes (non-VDB) these range from 0 to resolution-1. i @ resx , i @ resy , i @ resz //The resolution of the current volume. Common Geometry Attributes Note Houdini knows to cast these to the appropriate data type. Int @ id // A unique number that remains the same throughout a simulation. // Float @ pscale // Particle radius size. Uniform scale. Set display particles as 'Discs' to visualize. @ width // Thickness of curves. Enable 'Shade Open Curves In Viewport' on the object node to visualize. @ Alpha // Alpha transparency override. The viewport uses this to set the alpha of OpenGL geometry. @ Pw // Spline weight. Vector3 @ P // Point position. Used this to lay out points in 3D space. @ Cd // Diffuse color override. The viewport uses this to color OpenGL geometry. @ N // Surface or curve normal. Houdini will compute the normal if this attribute does not exist. @ scale // Vector scale. Allows directional scaling or stretching (in one direction). @ rest // Used by procedural patterns and textures to stick on deforming and animated surfaces. @ up // Up vector. The up direction for local space, typically (0, 1, 0). @ uv // UV texture coordinates for this point/vertex. @ v // Point velocity. The direction and speed of movement in units per second. Vector4 @ orient // The local orientation of the point (represented as a quaternion). @ rot // Additional rotation to be applied after orient, N, and up attributes. String @ name // A unique name identifying which primitives belong to which piece. Also used to label volumes. @ instance // Path of an object node to be instanced at render time. Specifying VEX Data Types Note The following characters are used to cast to the corresponding data type. float f @ name // Floating point scalar values. vector2 u @ name // Two floating point values. Could be used to store 2D positions. vector3 v @ name // Three floating point values. Usually positions, directions, normals, UVW or colors. vector4 p @ name // Four floating point values. Usually rotation quaternions, or color and alpha (RGBA). int i @ name // Integer values (VEX uses 32 bit integers). matrix2 2 @ name // Four floating point values representing a 2D rotation matrix. matrix3 3 @ name // Nine floating point values representing a 3D rotation matrix or 2D transform matrix. matrix 4 @ name // Sixteen floating point values representing a 3D transform matrix. string s @ name // A string of characters. Channel Shortcut Syntax Note This is used to hint at the data type of auto generated wrangle parameters. ch ( ' flt1 ' ); // Float chf ( ' flt2 ' ); // Float chi ( ' int ' ); // Integer chv ( ' vecparm ' ); // Vector 3 chp ( ' quat ' ); // Vector 4 / Quaternion ch3 ( ' m3 ' ); // 3x3 Matrix ch4 ( ' m4 ' ); // 4x4 Matrix chs ( ' str ' ); // String chramp ( 'r' , x ); // Spline Ramp vector ( chramp ( 'c' , x )); // RGB Ramp DOP Particle Attributes Note Particle systems are driven by attributes, here are some of the attributes used. f @ age // Time in seconds since the particle was born. f @ life // Time in seconds the particle is allowed to live. When f@age>f@life, i@dead will be set to 1. f @ nage // Normalized age, f@age divided by f@life. Implicit attribute, you cannot write to this. i @ dead // Whether a particle is living (0) or dead (1). A dead particle is deleted in the Reaping stage. i @ id // A unique id for the particle that remains the same throughout a single simulation. i @ stopped // Whether a particle is moving (0) or stopped (1). i @ stuck // Whether a particle is free (0) or stuck (1). i @ sliding // Whether a particle is free (0) or sliding along a surface (1). f @ cling // Force applied to sliding particles inwards (according to the collision's surface normal). s @ pospath // The path to the object that the particle is colliding with. i @ posprim // Which collision primitive in the path geometry whose position we wish to refer to. v @ posuv // Parametric uv on the collision primitive. i @ hittotal // The cumulative total of all hits for the particle (only incremented once per timestep). i @ has_pprevious // This is set to 1 if v@pprevious contains valid values. v @ pprevious // Stores the position of the particle on the previous frame. Used for collision detection. i @ hitnum // The number of times the particle collided in the last POP Collision Detect. s @ hitpath // The path to the object that was hit. A path to a file on disk or an op: path. i @ hitprim // The primitive hit. Could be -1 if it the collision detector couldn\u2019t figure out which prim. v @ hituv // The parametric UV space on the primitive. v @ hitpos // Where the hit actually occurred. Useful if the colliding object was moving. v @ hitnml // The normal of the surface at the time of the collision. v @ hitv // The velocity of the surface at the time of the collision. f @ hittime // When the collision occurred, that could be within a frame. f @ hitimpulse // Records how much of an impulse was needed for the collision resolution. varies with timestep. f @ bounce // When particles bounce off another object, this controls how much energy they keep. f @ bounceforward // Controls how much energy they keep in the tangential direction. f @ friction // When particles bounce, they are slowed down proportional to how hard they hit. s @ collisionignore // Objects that match this pattern will not be collided. f @ force // Forces on the particle for this frame. f @ mass // Inertia of the particle. v @ spinshape // This is multiplied by f@pscale to determine the shape of the particle for rotational inertia. f @ drag // How much the particle is effected by any wind effects. f @ dragexp // Ranges from 1 to 2, default is set on the solver. Used for both angular and linear drag. v @ dragshape // How much the particle is dragged in each of its local axes. v @ dragcenter // If specified, drag forces will also generate torques on the particle. v @ targetv // The local wind speed. Thought of as the goal, or target, velocity for the particle. f @ airresist // How important it is to match the wind speed. Thickness of the air. f @ speedmin // Minimum speed, in units per second, that a particle can move. f @ speedmax // Maximum speed, in units per second, that a particle can move. p @ orient // Orientation of the particle. Used for figuring out 'local' forces. v @ w // Angular speed of the particle. A vector giving the rotation axis. v @ torque // The equivalent of force for spins. No inertial tensor (the equivalent of mass) is supported. v @ targetw // The goal spin direction and speed for this particle. f @ spinresist // How important it is to match the targetw. f @ spinmin // Minimum speed in radians per second that a particle can spin. f @ spinmax // Maximum speed in radians per second that a particle can spin. DOP Grains Attributes Note Particles under control of POP Grains have the 'ispbd' attribute set to 1. This causes them to bypass movement update in the POP Solver, as the actual motion update is done by the POP Grains node. i @ ispbd // A value of 1 causes the particle to behave as grains. f @ pscale // Used to determine the radius of each particle. f @ repulsionweight // How much the particle collision forces are weighted. f @ repulsionstiffness // How strongly particles are kept apart. Higher values result in less bouncy repulsion. f @ attractionweight // How much the particles will naturally stick together when close. f @ attractionstiffness // How strongly nearby particles stick to each other. v @ targetP // Particles are constrained to this location. f @ targetweight // The weight of the v@targetP constraints. f @ targetstiffness // The stiffness with which particles are fixed to their v@targetP attribute. f @ restlength // Particles connected by polylines will be forced to maintain this distance (prim attribute). f @ constraintweight // Scale, on a per-particle basis of the constraint force. f @ constraintstiffness // This controls the stiffness on a per-particle basis. f @ strain // This primitive attribute records how much the constraint is stretched. f @ strength // If f@strain exceeds this primitive attribute, the constraint will be removed. DOP Packed RBD Attributes Note The Bullet Solver uses several point attributes to store the properties of each piece of a packed object. i @ active // Specifies whether the object is able to react to other objects. i @ animated // Specifies whether the transform should be updated from its SOP geometry at each timestep. i @ deforming // Specifies whether the collision shape should be rebuilt from its SOP geometry each timestep. f @ bounce // The elasticity of the object. i @ bullet_add_impact // Impacts that occur during the sim will be recorded in the Impacts or Feedback data. i @ bullet_ignore // Specifies whether the object should be completely ignored by the Bullet solver. f @ bullet_angular_sleep_threshold // The sleeping threshold for the object\u2019s angular velocity. f @ bullet_linear_sleep_threshold // The sleeping threshold for the object\u2019s linear velocity. i @ bullet_want_deactivate // Disables simulation of a non-moving object until the object moves again. i @ computecom // Specifies whether the center of mass should be computed from the collision shape. i @ computemass // Specifies whether the mass should be computed from the collision shape and density. f @ creationtime // Stores the simulation time at which the object was created. i @ dead // Specifies whether the object should be deleted during the next solve. f @ density // The mass of an object is its volume times its density. f @ friction // The coefficient of friction of the object. f @ inertialtensorstiffness // Rotational stiffness. A scale factor applied to the inertial tensor. i @ inheritvelocity // v and w point attributes from the SOP geometry will override the initial velocity. f @ mass // The mass of the object. s @ name // A unique name for the object. Used by Constraint Networks. p @ orient // The orientation of the object. v @ P // The current position of the object\u2019s center of mass. v @ pivot // The pivot that the orientation applies to. If i@computecom is non-zero, this is auto-computed. v @ v // Linear velocity of the object. v @ w // Angular velocity of the object, in radians per second. i @ bullet_adjust_geometry // Shrinks the collision geometry. i @ bullet_autofit // Use the bounds of the object for Box, Capsule, Cylinder, Sphere, or Plane. f @ bullet_collision_margin // Padding distance between collision shapes. s @ bullet_georep // Can be convexhull, concave, box, capsule, cylinder, compound, sphere, or plane. i @ bullet_groupconnected // Create convex hull per set of connected primitives. f @ bullet_length // The length of the Capsule or Cylinder collision shape in the Y direction. v @ bullet_primR // Orientation of the Box, Capsule, Cylinder, or Plane collision shape. v @ bullet_primS // Size of the Box collision shape. v @ bullet_primT // Position of the Box, Sphere, Capsule, Cylinder, or Plane collision shape. f @ bullet_radius // Radius of the Sphere, Capsule, or Cylinder collision shape. f @ bullet_shrink_amount // Specifies the amount of resizing done by Shrink Collision Geometry. s @ activationignore // Won't be activated by collisions with any objects that match this pattern. s @ collisiongroup // Specifies the name of a collision group that this object belongs to. s @ collisionignore // The object will not collide against any objects that match this pattern. f @ min_activation_impulse // Minimum impulse that will cause the object to switch from inactive to active. f @ speedmin // Minimum speed, in units per second, that a particle can move. f @ speedmax // Maximum speed, in units per second, that a particle can move. f @ spinmin // Minimum speed in radians per second that a particle can spin. f @ spinmax // Maximum speed in radians per second that a particle can spin. f @ accelmax // Limits the change in the object\u2019s speed that is caused by enforcing constraints. f @ angaccelmax // Limits the change in the object\u2019s angular speed that is caused by enforcing constraints. f @ airresist // Specifies how important it is to match the target velocity (v@targetv). f @ drag // How much the the v@targetv and f@airresist attributes effect the object. f @ dragexp // Ranges from 1 to 2, default is set on the solver. Used for both angular and linear drag. v @ force // Specifies a force that will be applied to the center of mass of the object. f @ spinresist // Specifies how important it is to match the target angular velocity (v@targetw). v @ targetv // Target velocity for the object. Used in combination with the f@airresist attribute. v @ targetw // Target angular velocity for the object. Used in combination with the f@spinresist attribute. v @ torque // Specifies a torque that will be applied to the object. i @ bullet_autofit_valid // Stores whether the solver has already computed collision shape attributes. i @ bullet_sleeping // Tracks whether the object has been put to sleep by the solver. f @ deactivation_time // Amount of time the speed has been below the Linear Threshold or Angular Threshold. i @ found_overlap // Used by the solver to determine whether it has performed the overlap test. i @ id // A unique identifier for the object. i @ nextid // Stores the i@id the solver will assign to the next new object. DOP RBD Constraint Attributes Note Attributes on the geometry to customize each constraint behavior and type. If a primitive attribute with the same name as a constraint property (such as damping) is present, the attribute value will be multiplied with the value from the constraint sub-data. s @ constraint_name // Specifies a piece of relationship data by name, such as 'Glue' or 'Spring'. s @ constraint_type // Specifies whether the constraint affects 'position', 'rotation' or 'all' degrees of freedom. f @ restlength // Specifies the desired length of the constraint to enforce. f @ width // Width of each edge. f @ density // Density of each point. p @ orient // Initial orientation of each point. Value stored as a quaternion. v @ v // Initial velocity of each point. v @ w // Initial angular velocity of each point measured in radians per second. f @ friction // Friction of each point. f @ klinear // Defines how strongly the wire resists stretching. f @ damplinear // Defines how strongly the wire resists oscillation due to stretching forces. f @ kangular // Defines how strongly the wire resists bending. f @ dampangular // Defines how strongly the wire resists oscillation due to bending forces. f @ targetstiffness // Defines how strongly the wire resists deforming from the animated position. f @ targetdamping // Defines how strongly the wire resists oscillation due to stretch forces. f @ normaldrag // The component of drag in the directions normal to the wire. f @ tangentdrag // The component of drag in the direction tangent to the wire. i @ nocollide // Collision detection for the edge is disabled (Only used if Collision Handling is SDF). v @ restP // Rest position of each point. p @ restorient // Rest orientation of each point. i @ gluetoanimation // Causes a point\u2019s position and orientation to be constrained to the input geometry. i @ pintoanimation // Causes a point\u2019s position to be constrained to the input geometry. v @ animationP // Target position of each point. p @ animationorient // Target orientation of each point. v @ animationv // Target velocity of each point. v @ animationw // Target angular velocity of each point. i @ independentcollisionallowed // Toggle external collisions (Only non-SDF Geometric Collision). i @ independentcollisionresolved // Unresolved external collisions (Only non-SDF Geometric Collision). i @ codependentcollisionallowed // Toggle soft body collisions (Only non-SDF Geometric Collision). i @ codependentcollisionresolved // Unresolved toggle soft body collisions (Only non-SDF Geometric Collision). i @ selfcollisionallowed // Toggle self collisions (Only non-SDF Geometric Collision). i @ selfcollisionresolved // Unresolved toggle self collisions (Only non-SDF Geometric Collision). DOP FLIP Attributes Note The FLIP Solver contains an embedded POP Solver, so all of POP Attributes listed above apply. f @ pscale // Particle scale v @ v // Particle velocity f @ viscosity // The \"thickness\" of a fluid. f @ density // The mass per unit volume. f @ temperature // The temperature of the fluid. f @ vorticity // Measures the amount of circulation in the fluid. f @ divergence // Positive values cause particles to spread out, negative cause them to clump together. v @ rest // Used to track the position of the fluid over time. v @ rest2 // Used for blending dual rest attributes, avoids stretching. f @ droplet // Identifies particles that separate from the main body of fluid. f @ underresolved // Particles that haven't fully resolved on the grid. i @ ballistic // Specifies particles which will be ignored by the fluid solve. v @ Lx // Angular momentum X axis v @ Ly // Angular momentum Y axis v @ Lz // Angular momentum Z axis DOP Vellum Point Attributes Note Vellum geometry is also considered particles, so all of POP Attributes listed above apply. i @ isgrain // A value of 1 causes the particle to behave as grains, 0 behaves as cloth. f @ attractionweight // How much the particles will naturally stick together when close, zero disables clumping. f @ friction // How much to scale the static friction. f @ dynamicfriction // How much to scale the dynamic friction. f @ inertia // Resistance of a particle to rotational constraints. If zero, the particle won't rotate. v @ v // Point velocity. p @ w // (Hair or Wire) angular velocity. p @ orient // (Hair or Wire) orientations. i @ stopped // Used to pin points (0=free, 1=no motion, 2=no rotation, 3=no rotate or move). i @ pintoanimation // If 1, the pinned points' position will be updated to match the target point. i @ gluetoanimation // If 1, both the position and orientation will be updated s @ target_path // target path for any pins (when the Target parameter is set in Vellum Source) i @ target_pt // target point number for any pins (when the Target parameter is set in Vellum Source) f @ targetweight // Affect the strength of the pinned points using a 0..1 weighting value. i @ weld // Weld this point to a point number. If there is an @id attribute, then to a point id. i @ branchweld // built by hair constraints when it is forced to split points for hair simulation. i @ collisionweld // generated on demand to provide a single weld to the detangle algorithm. f @ breakthreshold // The threshold for breaking welds and branch welds s @ breaktype // 'stretchstress', 'bendstress', 'stretchdistance', 'stretchratio', or 'bendangle'. // Collisions f @ pscale // Used to determine the thickness of cloth or radius of each particle. f @ overlap_self // Stores how much of the original pscale is overlapped. f @ overlap_external // Stores how much of the original pscale is overlapped. i @ layer // Indicates belonging to different layers of cloth. Higher numbers refer to higher layers. i @ disableself // A value of 0 means this point will use self collisions. i @ disableexternal // A value of 0 means this point will use external collisions. s @ collisionignore // Stores a pattern for the objects and collision groups to not collide with. s @ collisiongroup // Gives the collision group that this point belongs to. // Internal worker variables (Kept to avoid removing/adding attributes every frame) v @ pprevious // For 1st order integration, the previous frames position (beginning of timestep). v @ plast // For 2nd order integration, the position from two frames earlier. v @ vprevious // For 1st order integration, the previous frames velocity (beginning of timestep). v @ vlast // For 2nd order integration, the velocity from two frames earlier. p @ orientprevious // For 1st order integration, the previous frames orientation (beginning of timestep). p @ orientlast // For 2nd order integration, the orientation from two frames earlier. p @ wprevious // For 1st order integration, the previous frames angular velocity (beginning of timestep). p @ wlast // For 2nd order integration, the angular velocity from two frames earlier. f @ dP // Constraint displacements. Likely of last iteration. f @ dPw // Constraint weights. Likely of last iteration. s @ patchname // Identifies each generated patch in a simulation so it can be updated/replaced. // When a point is part of a Pressure constraint, these attrs hold values computed during constraint update. v @ pressuregradient // a vector pointing outwards along the direction of greatest volume gain. i [] @ volumepts // contains array of the points needed to compute the volume attribute. i @ volume // compare against the Pressure constraint\u2019s restlength value. DOP Vellum Constraint Attributes Note There are many types of constraints, so the meaning of these variables is often dependent on the constraint type. They usually live on the primitive. s @ type // Type of the constraint. s @ type = ' distance ' // Each edge in the display geometry is turned into a distance constraint maintaining that edge length. s @ type = ' stitch ' // Stitch points within the same geometry together using distance constraints. The points do not need to actually be connected by geometry. This is useful for keeping jackets closed or preventing pockets from flapping s @ type = ' branchstitch ' // s @ type = ' ptprim ' // s @ type = ' bend ' // Each pair of triangles (or implied triangles if input is quads or higher) creates a constraint maintaining the initial dihedral angle between the triangles. s @ type = ' trianglebend ' // Each pair of triangles (or implied triangles if input is quads or higher) creates a constraint maintaining the initial dihedral angle between the triangles. angle tetvolume pressure // Each piece, as determined by the Define Pieces parameter, stores its original volume and a many-point constraint is built to maintain it. The enforcement is global, so squishing one place will expand another, like a balloon. attach , pin attachnormal pinorient bendtwist stretchshear tetfiber triarap tetarap * f @ stiffness // The stiffness of the constraint, which controls how strongly the constraint pulls. f @ restlength f @ restlengthorig f @ dampingratio // Damping reduces jitter by bleeding energy when evaluating the constraint. Too much damping can prevent the constraint from being satisfied. Values less than 1 must be used. f @ stress // Estimate of work done by the constraint (updated by Vellum solver). s @ constraint_tag // The name of the node which created the constraint KineFX Attributes Note A KineFX hierarchy or skeleton is represented by a collection of points connected by polygon lines. The parent-child relationship between joints in a hierarchy is determined by vertex order. s @ name // joint name attribute 3 @ transform // World space 3\u00d73 transform of the point (rotation, scale, and shear). 4 @ localtransform // The transform of the point relative to its parent. i @ scaleinheritance // Determines how a point inherits the local scale from its parent. Viewport Display Attributes Note Override the viewport display mode attributes i @ gl_wireframe // Detail attribute, force wireframe (1) or shaded (-1) i @ gl_lit // Detail attribute, draw with lighting (1) or no lighting (0) i @ gl_showallpoints // Detail attribute, draw points even if connected to geometry f @ vm_cuspangle // Detail attribute, angle in degrees for generating cusped normals i @ gl_spherepoints // Detail attribute, (1) causes unconnected points to be drawn as spheres i @ gl_xray // Detail attribute, (1) geometry will visible even when it is hidden behind other geometry v @ Cd // Color Diffuse f @ Alpha // Surface opacity v @ N // Surface Normal for lighting f @ width // Curve width f @ pscale // If no pscale exists the viewport defaults to 1.0, while Mantra defaults to 0.1 s @ spritepath s @ shop_materialpath i @ group__3d_hidden_primitives // Add primitives to this group to hide them from the 3D viewport f @ intrinsic : volumevisualdensity // Primitive intrinsic attribute controlling the opacity of volumes. f @ volvis_shadowscale // Detail attribute controlling the shadow strength for volumes. Copying and Instancing Attributes Note When copying or instancing, Houdini looks for these point attributes to transform each copy/instance. instanceattrs instanceattrs p @ orient // Orientation of the copy. f @ pscale // Uniform scale. v @ scale // Non-uniform scale. v @ N // Normal (+Z axis of the copy, if no p@orient). v @ up // Up vector of the copy (+Y axis of the copy, if no p@orient). v @ v // Velocity of the copy (motion blur), used as +Z axis if no p@orient or v@N. p @ rot // Additional rotation (applied after the orientation attributes above). v @ P // Translation of the copy. v @ trans // Translation of the copy, in addition to v@P. v @ pivot // Local pivot point for the copy. 3 @ transform or 4 @ transform // Transform matrix overriding everything except v@P, v@pivot, and v@trans. s @ shop_materialpath // The instanced object uses this material. s @ material_override // A serialized Python dictionary mapping material parameter names to values. s @ instance s @ instancefile // File path indicating what geometry to instance. s @ instancepath // Geometry to instance. This is either a path to a file on disk or an op: path. Accessing Other Inputs Note This syntax is used to refer to nodes wired into the inputs of the wrangle, or other nodes in the network. ' opinput : X ' is the most legible and always works ( the first input is input 0 ). point ( ' opinput : 0 ' , 'P' , i @ ptnum ) point ( ' opinput : 1 ' , 'P' , i @ ptnum ) point ( ' opinput : 2 ' , 'P' , i @ ptnum ) point ( ' opinput : 3 ' , 'P' , i @ ptnum ) The integer input number ( the first input is 0 ). Some functions don ' t support this but it ' s easy to type . point ( 0 , 'P' , i @ ptnum ) point ( 1 , 'P' , i @ ptnum ) point ( 2 , 'P' , i @ ptnum ) point ( 3 , 'P' , i @ ptnum ) @ OpInputX works as well , but be careful as it isn ' t 0 based , instead it starts at 1 which is confusing point ( @ OpInput1 , 'P' , i @ ptnum ) point ( @ OpInput2 , 'P' , i @ ptnum ) point ( @ OpInput3 , 'P' , i @ ptnum ) point ( @ OpInput4 , 'P' , i @ ptnum ) v @ opinputX_ * reads an attribute from the same element on the numbered input ( first input is input 0 ). v @ opinput0_P v @ opinput1_P v @ opinput2_P v @ opinput3_P Absolute and relative paths to other nodes look like this . point ( ' op :/ obj / geo1 / OUT ' , 'P' , i @ ptnum ) point ( ' op : .. / .. / OUT ' , 'P' , i @ ptnum ) For Loop Metadata Note Detail attributes, You can get this information with a looping_metadata \u201cFetch metadata\u201d Block Begin node]. i @ numiterations // The expected total number of iterations i @ iteration // The current iteration number, always starting at 0 and increasing by 1 each loop. f @ value // In piece-wise loops, this is the current value of the attribute i @ ivalue // In simple repetition, this is an integer version of value. Mantra Shader Note Mantra shader global variables. See a primitive_spaces detailed explanation of implicit parametric UVs] v @ Cf // Surface Color. v @ Of // Surface Opacity. f @ Af // Surface Alpha. v @ P // Surface Position (camera space). f @ Pz // Surface Depth. v @ I // Direction from Eye (camera) to Surface. v @ dPds // Directions or Derivatives of surface implicit s coordinate. v @ dPdt // Directions or Derivatives of surface implicit t coordinate. v @ N // Surface Normal. v @ Ng // Surface Geometric Normal. v @ Eye // Position of Eye (camera). f @ s // implicit parametric s coordinate (u). f @ t // implicit parametric t coordinate (v). f @ Time // Shading Time. f @ dPdz // Change in Position with depth. i @ SID // Sample Identifier. A sample id to be used with the nextsample() VEX function to generate consistent random samples that don\u2019t change when re-rendering or between frames. wireframeInViewport Note Showing a geometry as wireframes. // Set true (1) for ON and false (0) for OFF. Need Detail in Run Over [Attribute Wrangle] @ gl_wireframe = 1 ; GroupExpand Note Uniformly expanding group by a specified distance. // Need a group filter ('group1') [Point Wrangle] int pc = pcopen ( 0 , 'P' , @ P , ch ( ' radius ' ), chi ( ' maxpts ' )); while ( pciterate ( pc ) > 0 ) { int currentpt ; pcimport ( pc , ' point . number ' , currentpt ); setpointgroup ( 0 , ' group1 ' , currentpt , 1 ); } https://mrkunz.com/blog/08_22_2018_VEX_Wrangle_Cheat_Sheet.html f@Frame The current floating frame number, equivalent to the $FF Hscript variable f@Time The current time in seconds, equivalent to the $T Hscript variable i@SimFrame The integer simulation timestep number ($SF), only present in DOP contexts f@SimTime The simulation time in seconds ($ST), only present in DOP contexts f@TimeInc The timestep currently being used for simulation or playback - - Attribute Wrangle v@P The position of the current element i@ptnum The point number attached to the currently processed element i@vtxnum The linear number of the currently processed vertex i@primnum The primitive number attached to the currently processed element i@elemnum The index number of the currently processed element i@numpt The total number of points in the geometry i@numvtx The number of vertices in the primitive of the currently processed element i@numprim The total number of primitives in the geometry i@numelem The total number of elements being processed - - Volume Wrangle v@P The position of the current voxel f@density The value of the density field at the current voxel location v@center The center of the current volume v@dPdx , v@dPdy , v@dPdz These vectors store the change in P that occurs in the x, y, and z voxel indices i@ix , i@iy , i@iz Voxel indices For dense volumes (non-VDB) these range from 0 to resolution-1 i@resx , i@resy , i@resz The resolution of the current volume - - Common Geometry Attributes i@id A unique indexing number that remains the same throughout time Used to match elements between frames f@pscale Particle radius size Uniform scale Set display particles as 'Discs' to visualize f@width Thickness of curves Enable 'Shade Open Curves In Viewport' on the object node to visualize f@Alpha Alpha transparency override The viewport uses this to set the alpha of OpenGL geometry f@Pw Spline weight Mostly depreciated at this point v@P Point position Used this to lay out points in 3D space v@Cd Diffuse color override The viewport uses this to color OpenGL geometry v@N Surface or curve normal Houdini will compute the normal if this attribute does not exist v@scale Vector scale Allows directional scaling or stretching (in one direction) v@rest Used by procedural patterns and textures to stick on deforming and animated surfaces v@up Up vector The up direction for local space, typically (0, 1, 0) v@uv UV texture coordinates for this point/vertex v@v Point velocity The direction and speed of movement in units per second p@orient The local orientation of the point (represented as a quaternion) p@rot Additional rotation to be applied after orient, N, and up attributes s@name A unique name identifying which primitives belong to which piece Also used to label volumes s@instance Path of an object node to be instanced at render time - - DOP Particle Attributes f@age Time in seconds since the particle was born f@life Time in seconds the particle is allowed to live When f@age>f@life, i@dead will be set to 1 f@nage Normalized age, f@age divided by f@life Implicit attribute, you cannot write to this i@dead Whether a particle is living (0) or dead (1) A dead particle is deleted in the Reaping stage i@id A unique id for the particle that remains the same throughout a single simulation i@stopped Whether a particle is moving (0) or stopped (1) i@stuck Whether a particle is free (0) or stuck (1) i@sliding Whether a particle is free (0) or sliding along a surface (1) f@cling Force applied to sliding paritcles inwards (according to the collision's surface normal) s@pospath The path to the object that the particle is colliding with i@posprim Which collision primitive in the path geometry whose position we wish to refer to v@posuv Parametric uv on the collision primitive i@hittotal The cumulative total of all hits for the particle (only incremented once per timestep) i@has_pprevious This is set to 1 if v@pprevious contains valid values v@pprevious Stores the position of the particle on the previous frame Used for collision detection i@hitnum The number of times the particle collided in the last POP Collision Detect s@hitpath The path to the object that was hit A path to a file on disk or an op: path i@hitprim The primitive hit Could be -1 if it the collision detector couldn\u2019t figure out which prim v@hituv The parametric UV space on the primitive v@hitpos Where the hit actually occurred Useful if the colliding object was moving v@hitnml The normal of the surface at the time of the collision v@hitv The velocity of the surface at the time of the collision f@hittime When the collision occurred, that could be within a frame f@hitimpulse Records how much of an impulse was needed for the collision resolution varies with timestep f@bounce When particles bounce off another object, this controls how much energy they keep f@bounceforward Controls how much energy they keep in the tangential direction f@friction When particles bounce, they are slowed down proportional to how hard they hit s@collisionignore Objects that match this pattern will not be collided f@force Forces on the particle for this frame f@mass Inertia of the particle v@spinshape This is multiplied by f@pscale to determine the shape of the particle for rotational inertia f@drag How much the particle is effected by any wind effects f@dragexp Ranges from 1 to 2, default is set on the solver Used for both angular and linear drag v@dragshape How much the particle is dragged in each of its local axes v@dragcenter If specified, drag forces will also generate torques on the particle v@targetv The local wind speed Thought of as the goal, or target, velocity for the particle f@airresist How important it is to match the wind speed Thickness of the air f@speedmin Minumum speed, in units per second, that a particle can move f@speedmax Maximum speed, in units per second, that a particle can move p@orient Orientation of the particle Used for figuring out 'local' forces v@w Angular speed of the particle A vector giving the rotation axis v@torque The equivalent of force for spins No inertial tensor (the equivalent of mass) is supported v@targetw The goal spin direction and speed for this particle f@spinresist How important it is to match the targetw f@spinmin Minumum speed in radians per second that a particle can spin f@spinmax Maximum speed in radians per second that a particle can spin - - DOP Grains Attributes i@ispbd A value of 1 causes the particle to behave as grains f@pscale Used to determine the radius of each particle f@repulsionweight How much the particle collision forces are weighted f@repulsionstiffness How strongly particles are kept apart Higher values result in less bouncy repulsion f@attractionweight How much the particles will naturally stick together when close f@attractionstiffness How strongly nearby particles stick to each other v@targetP Particles are constrained to this location f@targetweight The weight of the v@targetP constraints f@targetstiffness The stiffness with which particles are fixed to their v@targetP attribute f@restlength Particles connected by polylines will be forced to maintain this distance (prim attribute) f@constraintweight Scale, on a per-particle basis of the constraint force f@constraintstiffness This controls the stiffness on a per-particle basis f@strain This primitive attribute records how much the constraint is stretched f@strength If f@strain exceeds this primitive attribute, the constraint will be removed - - DOP Packed RBD Attributes i@active Specifies whether the object is able to react to other objects i@animated Specifies whether the transform should be updated from its SOP geometry at each timestep i@deforming Specifies whether the collision shape should be rebuilt from its SOP geometry each timestep f@bounce The elasticity of the object i@bullet_add_impact Impacts that occur during the sim will be recorded in the Impacts or Feedback data i@bullet_ignore Specifies whether the object should be completely ignored by the Bullet solver f@bullet_angular_sleep_threshold The sleeping threshold for the object\u2019s angular velocity f@bullet_linear_sleep_threshold The sleeping threshold for the object\u2019s linear velocity i@bullet_want_deactivate Disables simulation of a non-moving object until the object moves again i@computecom Specifies whether the center of mass should be computed from the collision shape i@computemass Specifies whether the mass should be computedfrom the collision shape and density f@creationtime Stores the simulation time at which the object was created i@dead Specifies whether the object should be deleted during the next solve f@density The mass of an object is its volume times its density f@friction The coefficient of friction of the object f@inertialtensorstiffness Rotational stiffness A scale factor applied to the inertial tensor i@inheritvelocity v and w point attributes from the SOP geometry will override the initial velocity f@mass The mass of the object s@name A unique name for the object Used by Constraint Networks p@orient The orientation of the object v@P The current position of the object\u2019s center of mass v@pivot The pivot that the orientation applies to If i@computecom is non-zero, this is auto-computed v@v Linear velocity of the object v@w Angular velocity of the object, in radians per second i@bullet_adjust_geometry Shrinks the collision geometry i@bullet_autofit Use the bounds of the object for Box, Capsule, Cylinder, Sphere, or Plane f@bullet_collision_margin Padding distance between collision shapes s@bullet_georep Can be convexhull, concave, box, capsule, cylinder, compound, sphere, or plane i@bullet_groupconnected Create convex hull per set of connected primitives f@bullet_length The length of the Capsule or Cylinder collision shape in the Y direction v@bullet_primR Orientation of the Box, Capsule, Cylinder, or Plane collision shape v@bullet_primS Size of the Box collision shape v@bullet_primT Position of the Box, Sphere, Capsule, Cylinder, or Plane collision shape f@bullet_radius Radius of the Sphere, Capsule, or Cylinder collision shape f@bullet_shrink_amount Specifies the amount of resizing done by Shrink Collision Geometry s@activationignore Won't be activated by collisions with any objects that match this pattern s@collisiongroup Specifies the name of a collision group that this object belongs to s@collisionignore The object will not collide against any objects that match this pattern f@min_activation_impulse Minimum impulse that will cause the object to switch from inactive to active f@speedmin Minumum speed, in units per second, that a particle can move f@speedmax Maximum speed, in units per second, that a particle can move f@spinmin Minumum speed in radians per second that a particle can spin f@spinmax Maximum speed in radians per second that a particle can spin f@accelmax Limits the change in the object\u2019s speed that is caused by enforcing constraints f@angaccelmax Limits the change in the object\u2019s angular speed that is caused by enforcing constraints f@airresist Specifies how important it is to match the target velocity (v@targetv) f@drag How much the the v@targetv and f@airresist attributes effect the object f@dragexp Ranges from 1 to 2, default is set on the solver Used for both angular and linear drag v@force Specifies a force that will be applied to the center of mass of the object f@spinresist Specifies how important it is to match the target angular velocity (v@targetw) v@targetv Target velocity for the object Used in combination with the f@airresist attribute v@targetw Target angular velocity for the object Used in combination with the f@spinresist attribute v@torque Specifies a torque that will be applied to the object i@bullet_autofit_valid Stores whether the solver has already computed collision shape attributes i@bullet_sleeping Tracks whether the object has been put to sleep by the solver f@deactivation_time Amount of time the speed has been below the Linear Threshold or Angular Threshold i@found_overlap Used by the solver to determine whether it has performed the overlap test i@id A unique identifier for the object i@nextid Stores the i@id the solver will assign to the next new object - - DOP Constraint Network Attributes f@width Width of each edge f@density Density of each point p@orient Initial orientation of each point This value is stored as a quaternion v@v Initial velocity of each point v@w Initial angular velocity of each point measured in radians per second f@friction Friction of each point f@klinear Defines how strongly the wire resists stretching f@damplinear Defines how strongly the wire resists oscillation due to stretching forces f@kangular Defines how strongly the wire resists bending f@dampangular Defines how strongly the wire resists oscillation due to bending forces f@targetstiffness Defines how strongly the wire resists deforming from the animated position f@targetdamping Defines how strongly the wire resists oscillation due to stretch forces f@normaldrag The component of drag in the directions normal to the wire f@tangentdrag The component of drag in the direction tangent to the wire i@nocollide Collision detection for the edge is disabled (Only used if Collision Handling is SDF) v@restP Rest position of each point p@restorient Rest orientation of each point i@gluetoanimation Causes a point\u2019s position and orientation to be constrained to the input geometry i@pintoanimation Causes a point\u2019s position to be constrained to the input geometry v@animationP Target position of each point p@animationorient Target orientation of each point v@animationv Target velocity of each point v@animationw Target angular velocity of each point i@independentcollisionallowed Toggle external collisions (Only non-SDF Geometric Collision) i@independentcollisionresolved Unresolved external collisions (Only non-SDF Geometric Collision) i@codependentcollisionallowed Toggle soft body collisions (Only non-SDF Geometric Collision) i@codependentcollisionresolved Unresolved toggle soft body collisions (Only non-SDF Geometric Collision) i@selfcollisionallowed Toggle self collisions (Only non-SDF Geometric Collision) i@selfcollisionresolved Unresolved toggle self collisions (Only non-SDF Geometric Collision) - - DOP FLIP Attributes f@pscale Particle scale v@v Particle velocity f@viscosity The \"thickness\" of a fluid f@density The mass per unit volume f@temperature The temperature of the fluid f@vorticity Measures the amount of circulation in the fluid f@divergence Positive values cause particles to spread out, negative cause them to clump together v@rest Used to track the position of the fluid over time v@rest2 Used for blending dual rest attributes, avoids stretching f@droplet Identifies particles that separate from the main body of fluid f@underresolved Particles that haven't fully resolved on the grid i@ballistic Specifies particles which will be ignored by the fluid solve v@Lx Angular momentum X axis v@Ly Angular momentum Y axis v@Lz Angular momentum Z axis","title":"Variables"},{"location":"vex/vex/#welcome-to-vex","text":"","title":"Welcome to Vex"},{"location":"vex/vex/#global-variables","text":"A list of variables available in wrangles. Note The type indicator isn't necessary, but included as a reminder. Available in all SOP wrangles f @ Frame //The current floating frame number, equivalent to the $FF Hscript variable f @ Time //The current time in seconds, equivalent to the $T Hscript variable i @ SimFrame //The integer simulation timestep number ($SF), only present in DOP contexts. f @ SimTime //The simulation time in seconds ($ST), only present in DOP contexts. f @ TimeInc //The timestep currently being used for simulation or playback. Available in Attribute Wrangle ( point , vertex , primitive and detail ) v @ P //The position of the current element. i @ ptnum //The point number attached to the currently processed element. i @ vtxnum //The linear number of the currently processed vertex. i @ primnum //The primitive number attached to the currently processed element. i @ elemnum //The index number of the currently processed element. i @ numpt //The total number of points in the geometry. i @ numvtx //The number of vertices in the primitive of the currently processed element. i @ numprim //The total number of primitives in the geometry. i @ numelem //The total number of elements being processed. Available in Volume Wrangle v @ P //The position of the current voxel. f @ density //The value of the density field at the current voxel location. v @ center //The center of the current volume. v @ dPdx , v @ dPdy , v @ dPdz //These vectors store the change in P that occurs in the x, y, and z voxel indices. i @ ix , i @ iy , i @ iz //Voxel indices. For dense volumes (non-VDB) these range from 0 to resolution-1. i @ resx , i @ resy , i @ resz //The resolution of the current volume.","title":"Global Variables"},{"location":"vex/vex/#common-geometry-attributes","text":"Note Houdini knows to cast these to the appropriate data type. Int @ id // A unique number that remains the same throughout a simulation. // Float @ pscale // Particle radius size. Uniform scale. Set display particles as 'Discs' to visualize. @ width // Thickness of curves. Enable 'Shade Open Curves In Viewport' on the object node to visualize. @ Alpha // Alpha transparency override. The viewport uses this to set the alpha of OpenGL geometry. @ Pw // Spline weight. Vector3 @ P // Point position. Used this to lay out points in 3D space. @ Cd // Diffuse color override. The viewport uses this to color OpenGL geometry. @ N // Surface or curve normal. Houdini will compute the normal if this attribute does not exist. @ scale // Vector scale. Allows directional scaling or stretching (in one direction). @ rest // Used by procedural patterns and textures to stick on deforming and animated surfaces. @ up // Up vector. The up direction for local space, typically (0, 1, 0). @ uv // UV texture coordinates for this point/vertex. @ v // Point velocity. The direction and speed of movement in units per second. Vector4 @ orient // The local orientation of the point (represented as a quaternion). @ rot // Additional rotation to be applied after orient, N, and up attributes. String @ name // A unique name identifying which primitives belong to which piece. Also used to label volumes. @ instance // Path of an object node to be instanced at render time.","title":"Common Geometry Attributes"},{"location":"vex/vex/#specifying-vex-data-types","text":"Note The following characters are used to cast to the corresponding data type. float f @ name // Floating point scalar values. vector2 u @ name // Two floating point values. Could be used to store 2D positions. vector3 v @ name // Three floating point values. Usually positions, directions, normals, UVW or colors. vector4 p @ name // Four floating point values. Usually rotation quaternions, or color and alpha (RGBA). int i @ name // Integer values (VEX uses 32 bit integers). matrix2 2 @ name // Four floating point values representing a 2D rotation matrix. matrix3 3 @ name // Nine floating point values representing a 3D rotation matrix or 2D transform matrix. matrix 4 @ name // Sixteen floating point values representing a 3D transform matrix. string s @ name // A string of characters.","title":"Specifying VEX Data Types"},{"location":"vex/vex/#channel-shortcut-syntax","text":"Note This is used to hint at the data type of auto generated wrangle parameters. ch ( ' flt1 ' ); // Float chf ( ' flt2 ' ); // Float chi ( ' int ' ); // Integer chv ( ' vecparm ' ); // Vector 3 chp ( ' quat ' ); // Vector 4 / Quaternion ch3 ( ' m3 ' ); // 3x3 Matrix ch4 ( ' m4 ' ); // 4x4 Matrix chs ( ' str ' ); // String chramp ( 'r' , x ); // Spline Ramp vector ( chramp ( 'c' , x )); // RGB Ramp","title":"Channel Shortcut Syntax"},{"location":"vex/vex/#dop-particle-attributes","text":"Note Particle systems are driven by attributes, here are some of the attributes used. f @ age // Time in seconds since the particle was born. f @ life // Time in seconds the particle is allowed to live. When f@age>f@life, i@dead will be set to 1. f @ nage // Normalized age, f@age divided by f@life. Implicit attribute, you cannot write to this. i @ dead // Whether a particle is living (0) or dead (1). A dead particle is deleted in the Reaping stage. i @ id // A unique id for the particle that remains the same throughout a single simulation. i @ stopped // Whether a particle is moving (0) or stopped (1). i @ stuck // Whether a particle is free (0) or stuck (1). i @ sliding // Whether a particle is free (0) or sliding along a surface (1). f @ cling // Force applied to sliding particles inwards (according to the collision's surface normal). s @ pospath // The path to the object that the particle is colliding with. i @ posprim // Which collision primitive in the path geometry whose position we wish to refer to. v @ posuv // Parametric uv on the collision primitive. i @ hittotal // The cumulative total of all hits for the particle (only incremented once per timestep). i @ has_pprevious // This is set to 1 if v@pprevious contains valid values. v @ pprevious // Stores the position of the particle on the previous frame. Used for collision detection. i @ hitnum // The number of times the particle collided in the last POP Collision Detect. s @ hitpath // The path to the object that was hit. A path to a file on disk or an op: path. i @ hitprim // The primitive hit. Could be -1 if it the collision detector couldn\u2019t figure out which prim. v @ hituv // The parametric UV space on the primitive. v @ hitpos // Where the hit actually occurred. Useful if the colliding object was moving. v @ hitnml // The normal of the surface at the time of the collision. v @ hitv // The velocity of the surface at the time of the collision. f @ hittime // When the collision occurred, that could be within a frame. f @ hitimpulse // Records how much of an impulse was needed for the collision resolution. varies with timestep. f @ bounce // When particles bounce off another object, this controls how much energy they keep. f @ bounceforward // Controls how much energy they keep in the tangential direction. f @ friction // When particles bounce, they are slowed down proportional to how hard they hit. s @ collisionignore // Objects that match this pattern will not be collided. f @ force // Forces on the particle for this frame. f @ mass // Inertia of the particle. v @ spinshape // This is multiplied by f@pscale to determine the shape of the particle for rotational inertia. f @ drag // How much the particle is effected by any wind effects. f @ dragexp // Ranges from 1 to 2, default is set on the solver. Used for both angular and linear drag. v @ dragshape // How much the particle is dragged in each of its local axes. v @ dragcenter // If specified, drag forces will also generate torques on the particle. v @ targetv // The local wind speed. Thought of as the goal, or target, velocity for the particle. f @ airresist // How important it is to match the wind speed. Thickness of the air. f @ speedmin // Minimum speed, in units per second, that a particle can move. f @ speedmax // Maximum speed, in units per second, that a particle can move. p @ orient // Orientation of the particle. Used for figuring out 'local' forces. v @ w // Angular speed of the particle. A vector giving the rotation axis. v @ torque // The equivalent of force for spins. No inertial tensor (the equivalent of mass) is supported. v @ targetw // The goal spin direction and speed for this particle. f @ spinresist // How important it is to match the targetw. f @ spinmin // Minimum speed in radians per second that a particle can spin. f @ spinmax // Maximum speed in radians per second that a particle can spin.","title":"DOP Particle Attributes"},{"location":"vex/vex/#dop-grains-attributes","text":"Note Particles under control of POP Grains have the 'ispbd' attribute set to 1. This causes them to bypass movement update in the POP Solver, as the actual motion update is done by the POP Grains node. i @ ispbd // A value of 1 causes the particle to behave as grains. f @ pscale // Used to determine the radius of each particle. f @ repulsionweight // How much the particle collision forces are weighted. f @ repulsionstiffness // How strongly particles are kept apart. Higher values result in less bouncy repulsion. f @ attractionweight // How much the particles will naturally stick together when close. f @ attractionstiffness // How strongly nearby particles stick to each other. v @ targetP // Particles are constrained to this location. f @ targetweight // The weight of the v@targetP constraints. f @ targetstiffness // The stiffness with which particles are fixed to their v@targetP attribute. f @ restlength // Particles connected by polylines will be forced to maintain this distance (prim attribute). f @ constraintweight // Scale, on a per-particle basis of the constraint force. f @ constraintstiffness // This controls the stiffness on a per-particle basis. f @ strain // This primitive attribute records how much the constraint is stretched. f @ strength // If f@strain exceeds this primitive attribute, the constraint will be removed.","title":"DOP Grains Attributes"},{"location":"vex/vex/#dop-packed-rbd-attributes","text":"Note The Bullet Solver uses several point attributes to store the properties of each piece of a packed object. i @ active // Specifies whether the object is able to react to other objects. i @ animated // Specifies whether the transform should be updated from its SOP geometry at each timestep. i @ deforming // Specifies whether the collision shape should be rebuilt from its SOP geometry each timestep. f @ bounce // The elasticity of the object. i @ bullet_add_impact // Impacts that occur during the sim will be recorded in the Impacts or Feedback data. i @ bullet_ignore // Specifies whether the object should be completely ignored by the Bullet solver. f @ bullet_angular_sleep_threshold // The sleeping threshold for the object\u2019s angular velocity. f @ bullet_linear_sleep_threshold // The sleeping threshold for the object\u2019s linear velocity. i @ bullet_want_deactivate // Disables simulation of a non-moving object until the object moves again. i @ computecom // Specifies whether the center of mass should be computed from the collision shape. i @ computemass // Specifies whether the mass should be computed from the collision shape and density. f @ creationtime // Stores the simulation time at which the object was created. i @ dead // Specifies whether the object should be deleted during the next solve. f @ density // The mass of an object is its volume times its density. f @ friction // The coefficient of friction of the object. f @ inertialtensorstiffness // Rotational stiffness. A scale factor applied to the inertial tensor. i @ inheritvelocity // v and w point attributes from the SOP geometry will override the initial velocity. f @ mass // The mass of the object. s @ name // A unique name for the object. Used by Constraint Networks. p @ orient // The orientation of the object. v @ P // The current position of the object\u2019s center of mass. v @ pivot // The pivot that the orientation applies to. If i@computecom is non-zero, this is auto-computed. v @ v // Linear velocity of the object. v @ w // Angular velocity of the object, in radians per second. i @ bullet_adjust_geometry // Shrinks the collision geometry. i @ bullet_autofit // Use the bounds of the object for Box, Capsule, Cylinder, Sphere, or Plane. f @ bullet_collision_margin // Padding distance between collision shapes. s @ bullet_georep // Can be convexhull, concave, box, capsule, cylinder, compound, sphere, or plane. i @ bullet_groupconnected // Create convex hull per set of connected primitives. f @ bullet_length // The length of the Capsule or Cylinder collision shape in the Y direction. v @ bullet_primR // Orientation of the Box, Capsule, Cylinder, or Plane collision shape. v @ bullet_primS // Size of the Box collision shape. v @ bullet_primT // Position of the Box, Sphere, Capsule, Cylinder, or Plane collision shape. f @ bullet_radius // Radius of the Sphere, Capsule, or Cylinder collision shape. f @ bullet_shrink_amount // Specifies the amount of resizing done by Shrink Collision Geometry. s @ activationignore // Won't be activated by collisions with any objects that match this pattern. s @ collisiongroup // Specifies the name of a collision group that this object belongs to. s @ collisionignore // The object will not collide against any objects that match this pattern. f @ min_activation_impulse // Minimum impulse that will cause the object to switch from inactive to active. f @ speedmin // Minimum speed, in units per second, that a particle can move. f @ speedmax // Maximum speed, in units per second, that a particle can move. f @ spinmin // Minimum speed in radians per second that a particle can spin. f @ spinmax // Maximum speed in radians per second that a particle can spin. f @ accelmax // Limits the change in the object\u2019s speed that is caused by enforcing constraints. f @ angaccelmax // Limits the change in the object\u2019s angular speed that is caused by enforcing constraints. f @ airresist // Specifies how important it is to match the target velocity (v@targetv). f @ drag // How much the the v@targetv and f@airresist attributes effect the object. f @ dragexp // Ranges from 1 to 2, default is set on the solver. Used for both angular and linear drag. v @ force // Specifies a force that will be applied to the center of mass of the object. f @ spinresist // Specifies how important it is to match the target angular velocity (v@targetw). v @ targetv // Target velocity for the object. Used in combination with the f@airresist attribute. v @ targetw // Target angular velocity for the object. Used in combination with the f@spinresist attribute. v @ torque // Specifies a torque that will be applied to the object. i @ bullet_autofit_valid // Stores whether the solver has already computed collision shape attributes. i @ bullet_sleeping // Tracks whether the object has been put to sleep by the solver. f @ deactivation_time // Amount of time the speed has been below the Linear Threshold or Angular Threshold. i @ found_overlap // Used by the solver to determine whether it has performed the overlap test. i @ id // A unique identifier for the object. i @ nextid // Stores the i@id the solver will assign to the next new object.","title":"DOP Packed RBD Attributes"},{"location":"vex/vex/#dop-rbd-constraint-attributes","text":"Note Attributes on the geometry to customize each constraint behavior and type. If a primitive attribute with the same name as a constraint property (such as damping) is present, the attribute value will be multiplied with the value from the constraint sub-data. s @ constraint_name // Specifies a piece of relationship data by name, such as 'Glue' or 'Spring'. s @ constraint_type // Specifies whether the constraint affects 'position', 'rotation' or 'all' degrees of freedom. f @ restlength // Specifies the desired length of the constraint to enforce. f @ width // Width of each edge. f @ density // Density of each point. p @ orient // Initial orientation of each point. Value stored as a quaternion. v @ v // Initial velocity of each point. v @ w // Initial angular velocity of each point measured in radians per second. f @ friction // Friction of each point. f @ klinear // Defines how strongly the wire resists stretching. f @ damplinear // Defines how strongly the wire resists oscillation due to stretching forces. f @ kangular // Defines how strongly the wire resists bending. f @ dampangular // Defines how strongly the wire resists oscillation due to bending forces. f @ targetstiffness // Defines how strongly the wire resists deforming from the animated position. f @ targetdamping // Defines how strongly the wire resists oscillation due to stretch forces. f @ normaldrag // The component of drag in the directions normal to the wire. f @ tangentdrag // The component of drag in the direction tangent to the wire. i @ nocollide // Collision detection for the edge is disabled (Only used if Collision Handling is SDF). v @ restP // Rest position of each point. p @ restorient // Rest orientation of each point. i @ gluetoanimation // Causes a point\u2019s position and orientation to be constrained to the input geometry. i @ pintoanimation // Causes a point\u2019s position to be constrained to the input geometry. v @ animationP // Target position of each point. p @ animationorient // Target orientation of each point. v @ animationv // Target velocity of each point. v @ animationw // Target angular velocity of each point. i @ independentcollisionallowed // Toggle external collisions (Only non-SDF Geometric Collision). i @ independentcollisionresolved // Unresolved external collisions (Only non-SDF Geometric Collision). i @ codependentcollisionallowed // Toggle soft body collisions (Only non-SDF Geometric Collision). i @ codependentcollisionresolved // Unresolved toggle soft body collisions (Only non-SDF Geometric Collision). i @ selfcollisionallowed // Toggle self collisions (Only non-SDF Geometric Collision). i @ selfcollisionresolved // Unresolved toggle self collisions (Only non-SDF Geometric Collision).","title":"DOP RBD Constraint Attributes"},{"location":"vex/vex/#dop-flip-attributes","text":"Note The FLIP Solver contains an embedded POP Solver, so all of POP Attributes listed above apply. f @ pscale // Particle scale v @ v // Particle velocity f @ viscosity // The \"thickness\" of a fluid. f @ density // The mass per unit volume. f @ temperature // The temperature of the fluid. f @ vorticity // Measures the amount of circulation in the fluid. f @ divergence // Positive values cause particles to spread out, negative cause them to clump together. v @ rest // Used to track the position of the fluid over time. v @ rest2 // Used for blending dual rest attributes, avoids stretching. f @ droplet // Identifies particles that separate from the main body of fluid. f @ underresolved // Particles that haven't fully resolved on the grid. i @ ballistic // Specifies particles which will be ignored by the fluid solve. v @ Lx // Angular momentum X axis v @ Ly // Angular momentum Y axis v @ Lz // Angular momentum Z axis","title":"DOP FLIP Attributes"},{"location":"vex/vex/#dop-vellum-point-attributes","text":"Note Vellum geometry is also considered particles, so all of POP Attributes listed above apply. i @ isgrain // A value of 1 causes the particle to behave as grains, 0 behaves as cloth. f @ attractionweight // How much the particles will naturally stick together when close, zero disables clumping. f @ friction // How much to scale the static friction. f @ dynamicfriction // How much to scale the dynamic friction. f @ inertia // Resistance of a particle to rotational constraints. If zero, the particle won't rotate. v @ v // Point velocity. p @ w // (Hair or Wire) angular velocity. p @ orient // (Hair or Wire) orientations. i @ stopped // Used to pin points (0=free, 1=no motion, 2=no rotation, 3=no rotate or move). i @ pintoanimation // If 1, the pinned points' position will be updated to match the target point. i @ gluetoanimation // If 1, both the position and orientation will be updated s @ target_path // target path for any pins (when the Target parameter is set in Vellum Source) i @ target_pt // target point number for any pins (when the Target parameter is set in Vellum Source) f @ targetweight // Affect the strength of the pinned points using a 0..1 weighting value. i @ weld // Weld this point to a point number. If there is an @id attribute, then to a point id. i @ branchweld // built by hair constraints when it is forced to split points for hair simulation. i @ collisionweld // generated on demand to provide a single weld to the detangle algorithm. f @ breakthreshold // The threshold for breaking welds and branch welds s @ breaktype // 'stretchstress', 'bendstress', 'stretchdistance', 'stretchratio', or 'bendangle'. // Collisions f @ pscale // Used to determine the thickness of cloth or radius of each particle. f @ overlap_self // Stores how much of the original pscale is overlapped. f @ overlap_external // Stores how much of the original pscale is overlapped. i @ layer // Indicates belonging to different layers of cloth. Higher numbers refer to higher layers. i @ disableself // A value of 0 means this point will use self collisions. i @ disableexternal // A value of 0 means this point will use external collisions. s @ collisionignore // Stores a pattern for the objects and collision groups to not collide with. s @ collisiongroup // Gives the collision group that this point belongs to. // Internal worker variables (Kept to avoid removing/adding attributes every frame) v @ pprevious // For 1st order integration, the previous frames position (beginning of timestep). v @ plast // For 2nd order integration, the position from two frames earlier. v @ vprevious // For 1st order integration, the previous frames velocity (beginning of timestep). v @ vlast // For 2nd order integration, the velocity from two frames earlier. p @ orientprevious // For 1st order integration, the previous frames orientation (beginning of timestep). p @ orientlast // For 2nd order integration, the orientation from two frames earlier. p @ wprevious // For 1st order integration, the previous frames angular velocity (beginning of timestep). p @ wlast // For 2nd order integration, the angular velocity from two frames earlier. f @ dP // Constraint displacements. Likely of last iteration. f @ dPw // Constraint weights. Likely of last iteration. s @ patchname // Identifies each generated patch in a simulation so it can be updated/replaced. // When a point is part of a Pressure constraint, these attrs hold values computed during constraint update. v @ pressuregradient // a vector pointing outwards along the direction of greatest volume gain. i [] @ volumepts // contains array of the points needed to compute the volume attribute. i @ volume // compare against the Pressure constraint\u2019s restlength value.","title":"DOP Vellum Point Attributes"},{"location":"vex/vex/#dop-vellum-constraint-attributes","text":"Note There are many types of constraints, so the meaning of these variables is often dependent on the constraint type. They usually live on the primitive. s @ type // Type of the constraint. s @ type = ' distance ' // Each edge in the display geometry is turned into a distance constraint maintaining that edge length. s @ type = ' stitch ' // Stitch points within the same geometry together using distance constraints. The points do not need to actually be connected by geometry. This is useful for keeping jackets closed or preventing pockets from flapping s @ type = ' branchstitch ' // s @ type = ' ptprim ' // s @ type = ' bend ' // Each pair of triangles (or implied triangles if input is quads or higher) creates a constraint maintaining the initial dihedral angle between the triangles. s @ type = ' trianglebend ' // Each pair of triangles (or implied triangles if input is quads or higher) creates a constraint maintaining the initial dihedral angle between the triangles. angle tetvolume pressure // Each piece, as determined by the Define Pieces parameter, stores its original volume and a many-point constraint is built to maintain it. The enforcement is global, so squishing one place will expand another, like a balloon. attach , pin attachnormal pinorient bendtwist stretchshear tetfiber triarap tetarap * f @ stiffness // The stiffness of the constraint, which controls how strongly the constraint pulls. f @ restlength f @ restlengthorig f @ dampingratio // Damping reduces jitter by bleeding energy when evaluating the constraint. Too much damping can prevent the constraint from being satisfied. Values less than 1 must be used. f @ stress // Estimate of work done by the constraint (updated by Vellum solver). s @ constraint_tag // The name of the node which created the constraint","title":"DOP Vellum Constraint Attributes"},{"location":"vex/vex/#kinefx-attributes","text":"Note A KineFX hierarchy or skeleton is represented by a collection of points connected by polygon lines. The parent-child relationship between joints in a hierarchy is determined by vertex order. s @ name // joint name attribute 3 @ transform // World space 3\u00d73 transform of the point (rotation, scale, and shear). 4 @ localtransform // The transform of the point relative to its parent. i @ scaleinheritance // Determines how a point inherits the local scale from its parent.","title":"KineFX Attributes"},{"location":"vex/vex/#viewport-display-attributes","text":"Note Override the viewport display mode attributes i @ gl_wireframe // Detail attribute, force wireframe (1) or shaded (-1) i @ gl_lit // Detail attribute, draw with lighting (1) or no lighting (0) i @ gl_showallpoints // Detail attribute, draw points even if connected to geometry f @ vm_cuspangle // Detail attribute, angle in degrees for generating cusped normals i @ gl_spherepoints // Detail attribute, (1) causes unconnected points to be drawn as spheres i @ gl_xray // Detail attribute, (1) geometry will visible even when it is hidden behind other geometry v @ Cd // Color Diffuse f @ Alpha // Surface opacity v @ N // Surface Normal for lighting f @ width // Curve width f @ pscale // If no pscale exists the viewport defaults to 1.0, while Mantra defaults to 0.1 s @ spritepath s @ shop_materialpath i @ group__3d_hidden_primitives // Add primitives to this group to hide them from the 3D viewport f @ intrinsic : volumevisualdensity // Primitive intrinsic attribute controlling the opacity of volumes. f @ volvis_shadowscale // Detail attribute controlling the shadow strength for volumes.","title":"Viewport Display Attributes"},{"location":"vex/vex/#copying-and-instancing-attributes","text":"Note When copying or instancing, Houdini looks for these point attributes to transform each copy/instance. instanceattrs instanceattrs p @ orient // Orientation of the copy. f @ pscale // Uniform scale. v @ scale // Non-uniform scale. v @ N // Normal (+Z axis of the copy, if no p@orient). v @ up // Up vector of the copy (+Y axis of the copy, if no p@orient). v @ v // Velocity of the copy (motion blur), used as +Z axis if no p@orient or v@N. p @ rot // Additional rotation (applied after the orientation attributes above). v @ P // Translation of the copy. v @ trans // Translation of the copy, in addition to v@P. v @ pivot // Local pivot point for the copy. 3 @ transform or 4 @ transform // Transform matrix overriding everything except v@P, v@pivot, and v@trans. s @ shop_materialpath // The instanced object uses this material. s @ material_override // A serialized Python dictionary mapping material parameter names to values. s @ instance s @ instancefile // File path indicating what geometry to instance. s @ instancepath // Geometry to instance. This is either a path to a file on disk or an op: path.","title":"Copying and Instancing Attributes"},{"location":"vex/vex/#accessing-other-inputs","text":"Note This syntax is used to refer to nodes wired into the inputs of the wrangle, or other nodes in the network. ' opinput : X ' is the most legible and always works ( the first input is input 0 ). point ( ' opinput : 0 ' , 'P' , i @ ptnum ) point ( ' opinput : 1 ' , 'P' , i @ ptnum ) point ( ' opinput : 2 ' , 'P' , i @ ptnum ) point ( ' opinput : 3 ' , 'P' , i @ ptnum ) The integer input number ( the first input is 0 ). Some functions don ' t support this but it ' s easy to type . point ( 0 , 'P' , i @ ptnum ) point ( 1 , 'P' , i @ ptnum ) point ( 2 , 'P' , i @ ptnum ) point ( 3 , 'P' , i @ ptnum ) @ OpInputX works as well , but be careful as it isn ' t 0 based , instead it starts at 1 which is confusing point ( @ OpInput1 , 'P' , i @ ptnum ) point ( @ OpInput2 , 'P' , i @ ptnum ) point ( @ OpInput3 , 'P' , i @ ptnum ) point ( @ OpInput4 , 'P' , i @ ptnum ) v @ opinputX_ * reads an attribute from the same element on the numbered input ( first input is input 0 ). v @ opinput0_P v @ opinput1_P v @ opinput2_P v @ opinput3_P Absolute and relative paths to other nodes look like this . point ( ' op :/ obj / geo1 / OUT ' , 'P' , i @ ptnum ) point ( ' op : .. / .. / OUT ' , 'P' , i @ ptnum )","title":"Accessing Other Inputs"},{"location":"vex/vex/#for-loop-metadata","text":"Note Detail attributes, You can get this information with a looping_metadata \u201cFetch metadata\u201d Block Begin node]. i @ numiterations // The expected total number of iterations i @ iteration // The current iteration number, always starting at 0 and increasing by 1 each loop. f @ value // In piece-wise loops, this is the current value of the attribute i @ ivalue // In simple repetition, this is an integer version of value.","title":"For Loop Metadata"},{"location":"vex/vex/#mantra-shader","text":"Note Mantra shader global variables. See a primitive_spaces detailed explanation of implicit parametric UVs] v @ Cf // Surface Color. v @ Of // Surface Opacity. f @ Af // Surface Alpha. v @ P // Surface Position (camera space). f @ Pz // Surface Depth. v @ I // Direction from Eye (camera) to Surface. v @ dPds // Directions or Derivatives of surface implicit s coordinate. v @ dPdt // Directions or Derivatives of surface implicit t coordinate. v @ N // Surface Normal. v @ Ng // Surface Geometric Normal. v @ Eye // Position of Eye (camera). f @ s // implicit parametric s coordinate (u). f @ t // implicit parametric t coordinate (v). f @ Time // Shading Time. f @ dPdz // Change in Position with depth. i @ SID // Sample Identifier. A sample id to be used with the nextsample() VEX function to generate consistent random samples that don\u2019t change when re-rendering or between frames.","title":"Mantra Shader"},{"location":"vex/vex/#wireframeinviewport","text":"Note Showing a geometry as wireframes. // Set true (1) for ON and false (0) for OFF. Need Detail in Run Over [Attribute Wrangle] @ gl_wireframe = 1 ;","title":"wireframeInViewport"},{"location":"vex/vex/#groupexpand","text":"Note Uniformly expanding group by a specified distance. // Need a group filter ('group1') [Point Wrangle] int pc = pcopen ( 0 , 'P' , @ P , ch ( ' radius ' ), chi ( ' maxpts ' )); while ( pciterate ( pc ) > 0 ) { int currentpt ; pcimport ( pc , ' point . number ' , currentpt ); setpointgroup ( 0 , ' group1 ' , currentpt , 1 ); } https://mrkunz.com/blog/08_22_2018_VEX_Wrangle_Cheat_Sheet.html f@Frame The current floating frame number, equivalent to the $FF Hscript variable f@Time The current time in seconds, equivalent to the $T Hscript variable i@SimFrame The integer simulation timestep number ($SF), only present in DOP contexts f@SimTime The simulation time in seconds ($ST), only present in DOP contexts f@TimeInc The timestep currently being used for simulation or playback - - Attribute Wrangle v@P The position of the current element i@ptnum The point number attached to the currently processed element i@vtxnum The linear number of the currently processed vertex i@primnum The primitive number attached to the currently processed element i@elemnum The index number of the currently processed element i@numpt The total number of points in the geometry i@numvtx The number of vertices in the primitive of the currently processed element i@numprim The total number of primitives in the geometry i@numelem The total number of elements being processed - - Volume Wrangle v@P The position of the current voxel f@density The value of the density field at the current voxel location v@center The center of the current volume v@dPdx , v@dPdy , v@dPdz These vectors store the change in P that occurs in the x, y, and z voxel indices i@ix , i@iy , i@iz Voxel indices For dense volumes (non-VDB) these range from 0 to resolution-1 i@resx , i@resy , i@resz The resolution of the current volume - - Common Geometry Attributes i@id A unique indexing number that remains the same throughout time Used to match elements between frames f@pscale Particle radius size Uniform scale Set display particles as 'Discs' to visualize f@width Thickness of curves Enable 'Shade Open Curves In Viewport' on the object node to visualize f@Alpha Alpha transparency override The viewport uses this to set the alpha of OpenGL geometry f@Pw Spline weight Mostly depreciated at this point v@P Point position Used this to lay out points in 3D space v@Cd Diffuse color override The viewport uses this to color OpenGL geometry v@N Surface or curve normal Houdini will compute the normal if this attribute does not exist v@scale Vector scale Allows directional scaling or stretching (in one direction) v@rest Used by procedural patterns and textures to stick on deforming and animated surfaces v@up Up vector The up direction for local space, typically (0, 1, 0) v@uv UV texture coordinates for this point/vertex v@v Point velocity The direction and speed of movement in units per second p@orient The local orientation of the point (represented as a quaternion) p@rot Additional rotation to be applied after orient, N, and up attributes s@name A unique name identifying which primitives belong to which piece Also used to label volumes s@instance Path of an object node to be instanced at render time - - DOP Particle Attributes f@age Time in seconds since the particle was born f@life Time in seconds the particle is allowed to live When f@age>f@life, i@dead will be set to 1 f@nage Normalized age, f@age divided by f@life Implicit attribute, you cannot write to this i@dead Whether a particle is living (0) or dead (1) A dead particle is deleted in the Reaping stage i@id A unique id for the particle that remains the same throughout a single simulation i@stopped Whether a particle is moving (0) or stopped (1) i@stuck Whether a particle is free (0) or stuck (1) i@sliding Whether a particle is free (0) or sliding along a surface (1) f@cling Force applied to sliding paritcles inwards (according to the collision's surface normal) s@pospath The path to the object that the particle is colliding with i@posprim Which collision primitive in the path geometry whose position we wish to refer to v@posuv Parametric uv on the collision primitive i@hittotal The cumulative total of all hits for the particle (only incremented once per timestep) i@has_pprevious This is set to 1 if v@pprevious contains valid values v@pprevious Stores the position of the particle on the previous frame Used for collision detection i@hitnum The number of times the particle collided in the last POP Collision Detect s@hitpath The path to the object that was hit A path to a file on disk or an op: path i@hitprim The primitive hit Could be -1 if it the collision detector couldn\u2019t figure out which prim v@hituv The parametric UV space on the primitive v@hitpos Where the hit actually occurred Useful if the colliding object was moving v@hitnml The normal of the surface at the time of the collision v@hitv The velocity of the surface at the time of the collision f@hittime When the collision occurred, that could be within a frame f@hitimpulse Records how much of an impulse was needed for the collision resolution varies with timestep f@bounce When particles bounce off another object, this controls how much energy they keep f@bounceforward Controls how much energy they keep in the tangential direction f@friction When particles bounce, they are slowed down proportional to how hard they hit s@collisionignore Objects that match this pattern will not be collided f@force Forces on the particle for this frame f@mass Inertia of the particle v@spinshape This is multiplied by f@pscale to determine the shape of the particle for rotational inertia f@drag How much the particle is effected by any wind effects f@dragexp Ranges from 1 to 2, default is set on the solver Used for both angular and linear drag v@dragshape How much the particle is dragged in each of its local axes v@dragcenter If specified, drag forces will also generate torques on the particle v@targetv The local wind speed Thought of as the goal, or target, velocity for the particle f@airresist How important it is to match the wind speed Thickness of the air f@speedmin Minumum speed, in units per second, that a particle can move f@speedmax Maximum speed, in units per second, that a particle can move p@orient Orientation of the particle Used for figuring out 'local' forces v@w Angular speed of the particle A vector giving the rotation axis v@torque The equivalent of force for spins No inertial tensor (the equivalent of mass) is supported v@targetw The goal spin direction and speed for this particle f@spinresist How important it is to match the targetw f@spinmin Minumum speed in radians per second that a particle can spin f@spinmax Maximum speed in radians per second that a particle can spin - - DOP Grains Attributes i@ispbd A value of 1 causes the particle to behave as grains f@pscale Used to determine the radius of each particle f@repulsionweight How much the particle collision forces are weighted f@repulsionstiffness How strongly particles are kept apart Higher values result in less bouncy repulsion f@attractionweight How much the particles will naturally stick together when close f@attractionstiffness How strongly nearby particles stick to each other v@targetP Particles are constrained to this location f@targetweight The weight of the v@targetP constraints f@targetstiffness The stiffness with which particles are fixed to their v@targetP attribute f@restlength Particles connected by polylines will be forced to maintain this distance (prim attribute) f@constraintweight Scale, on a per-particle basis of the constraint force f@constraintstiffness This controls the stiffness on a per-particle basis f@strain This primitive attribute records how much the constraint is stretched f@strength If f@strain exceeds this primitive attribute, the constraint will be removed - - DOP Packed RBD Attributes i@active Specifies whether the object is able to react to other objects i@animated Specifies whether the transform should be updated from its SOP geometry at each timestep i@deforming Specifies whether the collision shape should be rebuilt from its SOP geometry each timestep f@bounce The elasticity of the object i@bullet_add_impact Impacts that occur during the sim will be recorded in the Impacts or Feedback data i@bullet_ignore Specifies whether the object should be completely ignored by the Bullet solver f@bullet_angular_sleep_threshold The sleeping threshold for the object\u2019s angular velocity f@bullet_linear_sleep_threshold The sleeping threshold for the object\u2019s linear velocity i@bullet_want_deactivate Disables simulation of a non-moving object until the object moves again i@computecom Specifies whether the center of mass should be computed from the collision shape i@computemass Specifies whether the mass should be computedfrom the collision shape and density f@creationtime Stores the simulation time at which the object was created i@dead Specifies whether the object should be deleted during the next solve f@density The mass of an object is its volume times its density f@friction The coefficient of friction of the object f@inertialtensorstiffness Rotational stiffness A scale factor applied to the inertial tensor i@inheritvelocity v and w point attributes from the SOP geometry will override the initial velocity f@mass The mass of the object s@name A unique name for the object Used by Constraint Networks p@orient The orientation of the object v@P The current position of the object\u2019s center of mass v@pivot The pivot that the orientation applies to If i@computecom is non-zero, this is auto-computed v@v Linear velocity of the object v@w Angular velocity of the object, in radians per second i@bullet_adjust_geometry Shrinks the collision geometry i@bullet_autofit Use the bounds of the object for Box, Capsule, Cylinder, Sphere, or Plane f@bullet_collision_margin Padding distance between collision shapes s@bullet_georep Can be convexhull, concave, box, capsule, cylinder, compound, sphere, or plane i@bullet_groupconnected Create convex hull per set of connected primitives f@bullet_length The length of the Capsule or Cylinder collision shape in the Y direction v@bullet_primR Orientation of the Box, Capsule, Cylinder, or Plane collision shape v@bullet_primS Size of the Box collision shape v@bullet_primT Position of the Box, Sphere, Capsule, Cylinder, or Plane collision shape f@bullet_radius Radius of the Sphere, Capsule, or Cylinder collision shape f@bullet_shrink_amount Specifies the amount of resizing done by Shrink Collision Geometry s@activationignore Won't be activated by collisions with any objects that match this pattern s@collisiongroup Specifies the name of a collision group that this object belongs to s@collisionignore The object will not collide against any objects that match this pattern f@min_activation_impulse Minimum impulse that will cause the object to switch from inactive to active f@speedmin Minumum speed, in units per second, that a particle can move f@speedmax Maximum speed, in units per second, that a particle can move f@spinmin Minumum speed in radians per second that a particle can spin f@spinmax Maximum speed in radians per second that a particle can spin f@accelmax Limits the change in the object\u2019s speed that is caused by enforcing constraints f@angaccelmax Limits the change in the object\u2019s angular speed that is caused by enforcing constraints f@airresist Specifies how important it is to match the target velocity (v@targetv) f@drag How much the the v@targetv and f@airresist attributes effect the object f@dragexp Ranges from 1 to 2, default is set on the solver Used for both angular and linear drag v@force Specifies a force that will be applied to the center of mass of the object f@spinresist Specifies how important it is to match the target angular velocity (v@targetw) v@targetv Target velocity for the object Used in combination with the f@airresist attribute v@targetw Target angular velocity for the object Used in combination with the f@spinresist attribute v@torque Specifies a torque that will be applied to the object i@bullet_autofit_valid Stores whether the solver has already computed collision shape attributes i@bullet_sleeping Tracks whether the object has been put to sleep by the solver f@deactivation_time Amount of time the speed has been below the Linear Threshold or Angular Threshold i@found_overlap Used by the solver to determine whether it has performed the overlap test i@id A unique identifier for the object i@nextid Stores the i@id the solver will assign to the next new object - - DOP Constraint Network Attributes f@width Width of each edge f@density Density of each point p@orient Initial orientation of each point This value is stored as a quaternion v@v Initial velocity of each point v@w Initial angular velocity of each point measured in radians per second f@friction Friction of each point f@klinear Defines how strongly the wire resists stretching f@damplinear Defines how strongly the wire resists oscillation due to stretching forces f@kangular Defines how strongly the wire resists bending f@dampangular Defines how strongly the wire resists oscillation due to bending forces f@targetstiffness Defines how strongly the wire resists deforming from the animated position f@targetdamping Defines how strongly the wire resists oscillation due to stretch forces f@normaldrag The component of drag in the directions normal to the wire f@tangentdrag The component of drag in the direction tangent to the wire i@nocollide Collision detection for the edge is disabled (Only used if Collision Handling is SDF) v@restP Rest position of each point p@restorient Rest orientation of each point i@gluetoanimation Causes a point\u2019s position and orientation to be constrained to the input geometry i@pintoanimation Causes a point\u2019s position to be constrained to the input geometry v@animationP Target position of each point p@animationorient Target orientation of each point v@animationv Target velocity of each point v@animationw Target angular velocity of each point i@independentcollisionallowed Toggle external collisions (Only non-SDF Geometric Collision) i@independentcollisionresolved Unresolved external collisions (Only non-SDF Geometric Collision) i@codependentcollisionallowed Toggle soft body collisions (Only non-SDF Geometric Collision) i@codependentcollisionresolved Unresolved toggle soft body collisions (Only non-SDF Geometric Collision) i@selfcollisionallowed Toggle self collisions (Only non-SDF Geometric Collision) i@selfcollisionresolved Unresolved toggle self collisions (Only non-SDF Geometric Collision) - - DOP FLIP Attributes f@pscale Particle scale v@v Particle velocity f@viscosity The \"thickness\" of a fluid f@density The mass per unit volume f@temperature The temperature of the fluid f@vorticity Measures the amount of circulation in the fluid f@divergence Positive values cause particles to spread out, negative cause them to clump together v@rest Used to track the position of the fluid over time v@rest2 Used for blending dual rest attributes, avoids stretching f@droplet Identifies particles that separate from the main body of fluid f@underresolved Particles that haven't fully resolved on the grid i@ballistic Specifies particles which will be ignored by the fluid solve v@Lx Angular momentum X axis v@Ly Angular momentum Y axis v@Lz Angular momentum Z axis","title":"GroupExpand"}]}