{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CGTECH","text":"<p>:::caution</p> <p>\u3053\u306e\u30b5\u30a4\u30c8\u306f\u3001mkdocs \u3092\u4f7f\u7528\u3057\u3066\u4f5c\u6210\u3057\u3066\u3044\u305f\u3082\u306e\u304b\u3089 Docusaurus \u3092\u4f7f\u7528\u3057\u305f\u3082\u306e\u306b\u4ee5\u964d\u4e2d\u3067\u3059\u3002 \u8a18\u4e8b\u306e\u79fb\u690d\u306f\u968f\u6642\u5bfe\u5fdc\u4e2d\u3067\u3059</p>"},{"location":"IntroHoudini/","title":"Introduction","text":""},{"location":"IntroHoudini/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"IntroHoudini/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> TIP EXAMPLE <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Visit the community forum </p> PYTHON EXAMPLE<pre><code>import tensorflow as tf\n</code></pre> C++ EXAMPLE<pre><code>void setup() {\n  pinMode(0, OUTPUT);\n}\n\nvoid loop() {\n  digitalWrite(0, HIGH); \n  delay(500);\n  digitalWrite(0, LOW); \n  delay(500); \n}\n</code></pre>"},{"location":"color/","title":"Color","text":"<p>:::caution</p> <p>\u3053\u306e\u30b5\u30a4\u30c8\u306f\u3001mkdocs \u3092\u4f7f\u7528\u3057\u3066\u4f5c\u6210\u3057\u3066\u3044\u305f\u3082\u306e\u304b\u3089 Docusaurus \u3092\u4f7f\u7528\u3057\u305f\u3082\u306e\u306b\u4ee5\u964d\u4e2d\u3067\u3059\u3002 \u8a18\u4e8b\u306e\u79fb\u690d\u306f\u968f\u6642\u5bfe\u5fdc\u4e2d\u3067\u3059\u3002</p>"},{"location":"hVars/","title":"Environment variables","text":""},{"location":"hVars/#houdini-tip--taking-advantage-of-environment-variables","title":"HOUDINI TIP | TAKING ADVANTAGE OF ENVIRONMENT VARIABLES","text":"<p>Like many other applications, Houdini\u2019s configuration can be modified by using environment variables. It is a very useful way of creating different Houdini setups for different purposes \u2013 let it be different projects, different plugin versions, sandboxes for testing assets, personal configurations, etc. While this post is written for Houdini and have examples for windows and linux, it is applicable for any other application and operating system.</p>"},{"location":"hVars/#a-couple-of-words-about-environment-variables","title":"A COUPLE OF WORDS ABOUT ENVIRONMENT VARIABLES.","text":"<p>At first I will try to explain basic concepts of env vars and later on I will show examples for Houdini (linux and windows).</p> <p>Environment variables might be a bit hidden to many users, but they can be very helpful. Actually a lot of operating system behavior is controlled by them. Apart from configuring Houdini, env vars can be used to modify a search path for binaries, libraries, can set up proxy settings, get current user\u2019s name, path to home, temp directory, hostname, shell and much more.</p> <p>An useful thing is that they can be applied globally to the whole system, and locally to the current environment in a shell (a terminal session or environment set up and executed from batch/bash script). For example one might specify global settings which are available to the whole system (path to system cache dir, username, path to root directory of all projects\u2026) and local settings for each configured environments (houdini version, project name, project root, path to textures directory, license information about a plugin\u2026). All child environments (set up through batch/bash script) will inherit global settings (available to the whole operating system or an user) and can have specific local settings needed for their purpose.</p> <p>Here are couple of situations which env vars can efficiently address:</p> <p>-[x] having a multiple versions of Houdini using different renderer plugin versions and different assets libraries -[x] using different configurations for different projects / sequences / shots / tasks -[x] sharing the same settings (home dir, temp dir, cache dir, footage path, rv path) for various applications (Houdini, Nuke, Katana, Maya)</p>"},{"location":"hVars/#using-environment-variables-on-windows","title":"USING ENVIRONMENT VARIABLES ON WINDOWS","text":"<p>Here I will show an example of one of my Houdini startup script which sets an environment for a project and executes Houdini: houdini.bat. Note that Houdini understands a lot of useful env vars, documentation can be found here.</p> <pre><code>@echo off\nrem houdini launcher\n\nrem source global vars\ncall \\\\network_drive\\current_project\\00_pipeline\\globals.bat\n\nrem set Houdini paths\nrem variables PIPELINE, ROOT were set in globals.bat\nset \"HOUDINI_VERSION=Houdini 15.5.632\"\nset \"HOUDINI_PATH=&amp;;%PIPELINE%/houdini\"\nset \"HOUDINI_OTLSCAN_PATH=&amp;;%ROOT%/20_assets/otls\"\nset \"HOUDINI_SPLASH_FILE=%PIPELINE%/houdini/splash.jpg\"\nset \"HOUDINI_SPLASH_MESSAGE= | JELLY FISH | %HOUDINI_VERSION% | %USERNAME% |\"\nset \"JOB=%ROOT%\"\nset \"HOUDINI_BACKUP_FILENAME=$BASENAME_bak_$N\"\nset \"HOUDINI_BACKUP_DIR=bak\"\nset \"HOUDINI_NO_START_PAGE_SPLASH=1\"\nset \"HOUDINI_ANONYMOUS_STATISTICS=0\"\nset \"HOME=%ROOT%/05_user/%USERNAME%\"\nset \"HOUDINI_DESK_PATH=&amp;;C:/Users/%USERNAME%/Documents/houdini15.5/desktop\"\nset \"HOUDINI_TEMP_DIR=%HOME%/tmp\"\nset \"HOUDINI_BUFFEREDSAVE=1\"\nset \"HOUDINI_IMAGE_DISPLAY_GAMMA=1\"\nset \"HOUDINI_IMAGE_DISPLAY_LUT=%PIPELINE%/houdini/linear-to-srgb_14bit.lut\"\nset \"HOUDINI_IMAGE_DISPLAY_OVERRIDE=1\"\nset \"MEGASCANS=%JOB%/20_assets/megascans/Megascans/\"\nset \"MEGASCANS3D=%MEGASCANS%3d/\"\nrem convert to forward slashes\nset \"MEGASCANS=%MEGASCANS:\\=/%\"\nset \"MEGASCANS3D=%MEGASCANS3D:\\=/%\"\n\nrem create temp dir for houdini user if it does not exist, also convert to forwardslashes\nset \"TMP=%HOUDINI_TEMP_DIR%\"\nset \"TMP=%TMP:/=\\%\"\nIF not exist %TMP% (mkdir %TMP%)\n\nrem run Houdini\nset \"HOUDINI_DIR=C:\\Program Files\\Side Effects Software\\%HOUDINI_VERSION%\\bin\"\nset \"PATH=%HOUDINI_DIR%;%PATH%\"\ncd ../../\nstart houdinifx\n</code></pre> <p>Here I will explain some of batch syntax: @echo off \u2013 hides printing of commands into the console, it makes output more readable rem foo \u2013 lines starting with rem are comments call foo.bat \u2013 executes foo.bat batch script and inherits all env vars specified in it %MYVAR% \u2013 value of MYVAR variable set \u201cMYVAR=FOO\u201d \u2013 sets MYVAR variable to FOO \u2013 those variables are specific to softwares, which are expecting them set \u201cPATH=%PATH%;FOO\u201d \u2013 appends FOO to the PATH variable which is used for locating binaries to be executed (houdinifx command is not known to the whole windows system, only when houdini bin directory (containing houdinifx.exe) is appended to the PATH var) start foo \u2013 executes foo.exe (which must be found in one of the paths in the PATH env var) while passing on all existing variables to this child environment</p> <p>This script can be used for setting up local environment variables, to define a variable for the whole system environment on windows, follow this video.</p> <p>USING ENVIRONMENT VARIABLES ON LINUX (Linux version should be directly transferable to mac OS.)</p> <p>In linux env vars can be set up on multiple locations (~/.profile, ~/.bashrc, ~/.bash_profile, ~/.bash_login, /etc/environment \u2026) System env vars can be specified in /etc/environment. User env vars can be specified in ~/.profile (~ symbol points to the user home directory). ~/.profile, ~/.bashrc, ~/.bash_profile, and ~/.bash_login have similar usage, but I recommend using ~/.profile, which is the only one executed when running applications from graphical environment in a desktop session..</p> <p>In a Bash terminal the quickest way of executing an application inheriting env vars is to use this syntax:</p>"},{"location":"houdiniPython/","title":"Python","text":"<p>:::caution</p> <p>\u3053\u306e\u30b5\u30a4\u30c8\u306f\u3001mkdocs \u3092\u4f7f\u7528\u3057\u3066\u4f5c\u6210\u3057\u3066\u3044\u305f\u3082\u306e\u304b\u3089 Docusaurus \u3092\u4f7f\u7528\u3057\u305f\u3082\u306e\u306b\u4ee5\u964d\u4e2d\u3067\u3059\u3002 \u8a18\u4e8b\u306e\u79fb\u690d\u306f\u968f\u6642\u5bfe\u5fdc\u4e2d\u3067\u3059\u3002</p>"},{"location":"indexsd/","title":"Welcome to cg_tech","text":""},{"location":"indexsds/","title":"Indexsds","text":"<p>Welcome to My Site!</p> \u24d8 About Me"},{"location":"solaris/","title":"Solaris","text":"<p>Mots cl\u00e9s:     - SOLARIS     -Houdini     - USD</p>"},{"location":"solaris/#usdsolaris","title":"USD/SOLARIS","text":"<p>Et avant de commencer, un petit rappel sur l'USD. USD est une biblioth\u00e8que de gestion des sc\u00e9nographies cr\u00e9\u00e9es par Pixar. C'est un format de fichier cr\u00e9\u00e9 pour que plusieurs personnes travaillent en m\u00eame temps.</p> <p>La principale diff\u00e9rence par rapport au format de fichier existant est Rendre plusieurs fichiers USD \"proc\u00e9duraux\", appel\u00e9s \"sc\u00e9nographie proc\u00e9durale\" La possibilit\u00e9 de composer et de construire un seul sc\u00e9nographie.</p> <p></p> <p>\u00c0 partir des diapositives du s\u00e9minaire de lancement.</p> <p>SOLARIS supporte nativement cet USD. Synth\u00e9tiser proc\u00e9duralement le fichier USD pour g\u00e9rer le SOP, Vous pouvez construire un sc\u00e9nario.</p> <p></p> <p>Une autre caract\u00e9ristique est que les n\u0153uds de ce SOLARIS (r\u00e9seau Stage) sont en m\u00e9moire ou dans un fichier Chaque n\u0153ud est un fichier USD et trait\u00e9 comme une couche.</p> <p>Traitement des n\u0153uds = Traitement USD = Construction proc\u00e9durale du graphe de sc\u00e8ne</p> <p>C'est le monde de SOLARIS. Le monde de SOLARIS est aussi le monde de l'USD lui-m\u00eame.</p> <p>C'est un aper\u00e7u approximatif de SOLARIS et USD.</p>"},{"location":"solaris/#t\u00e9l\u00e9charger-le-fichier-usd","title":"T\u00e9l\u00e9charger le fichier USD","text":"<p>Nous sommes donc pr\u00eats \u00e0 ouvrir. Pour cet \u00e9chantillon, j'apporterai un fichier USD officiel de Pixar.</p> <p>http://graphics.pixar.com/usd/downloads.html</p> <p>Chaque fois que je t\u00e9l\u00e9charge le KITCHEN SET familier sur le site de t\u00e9l\u00e9chargement officiel.</p>"},{"location":"solaris/#ouvrir-solaris","title":"Ouvrir SOLARIS","text":"<p>Une fois t\u00e9l\u00e9charg\u00e9, lancez Houdini18.</p> <p></p> <p>Une fois d\u00e9marr\u00e9, changez la disposition de l'\u00e9cran en Solaris.</p>"},{"location":"solaris/#composition-de-l\u00e9cran","title":"Composition de l'\u00e9cran","text":"<p>SOLARIS a ajout\u00e9 deux nouvelles vues que Houdini n'avait pas auparavant.</p> <p></p> <p>Le premier est l'arborescence des graphes de sc\u00e8ne Ceci affiche un ``scenegraph en USD'' (appel\u00e9s \u00e9tapes) des \u00e9tapes du n\u0153ud actuellement s\u00e9lectionn\u00e9. Ce SOLARIS signifie \"USD Procedural Scene Graph\" Un n\u0153ud est trait\u00e9 comme un fichier USD = couche. Lorsque vous connectez des n\u0153uds, les n\u0153uds sont compos\u00e9s de mani\u00e8re proc\u00e9durale. et pour les n\u0153uds qui sont actuellement s\u00e9lectionn\u00e9s ou dont l'indicateur d'affichage est activ\u00e9 L'\u00e9tape est \"le r\u00e9sultat de la composition des n\u0153uds jusqu'\u00e0 ce point\".  </p> <p>Le graphique de sc\u00e8ne r\u00e9sultant est affich\u00e9 comme C'est ce \"Scene Graph Path\".  </p> <p>L'autre est \"Scene Graph Detail\".  </p> <p></p> <p>Il s'agit d'un panneau dans lequel vous pouvez centrer les attributs de la primitive actuellement s\u00e9lectionn\u00e9e et d'autres informations diverses. Quand je commence \u00e0 expliquer les d\u00e9tails, il y a suffisamment de mati\u00e8re pour \u00e9crire un article ici. Je vais l'omettre cette fois, mais quand vous regardez chaque individu, vous pouvez voir comment ils sont compos\u00e9s. Vous pouvez voir les valeurs d'attribut actuelles, etc.  </p> <p>Ces deux-l\u00e0 seront plus attach\u00e9s \u00e0 USD qu'aux fonctionnalit\u00e9s SOLARIS. \u00c9tant donn\u00e9 que le r\u00e9sultat de la construction d'un graphe sc\u00e9nique dans SOLARIS est affich\u00e9 dans ces deux Si vous v\u00e9rifiez ces deux panneaux avec la cr\u00e9ation de n\u0153uds C'est facile \u00e0 comprendre \u00e0 bien des \u00e9gards.  </p> <p>Par cons\u00e9quent, je vais montrer ces deux ensemble dans l'explication ci-dessous.</p>"},{"location":"solaris/#usd-ouvrons-le","title":"USD, ouvrons-le","text":"<p>Passons donc au sujet principal. Ouvrez le fichier USD t\u00e9l\u00e9charg\u00e9 avec Houdini/SOLARIS.  </p> <p></p> <p>Tout d'abord, utilisez le n\u0153ud \"LoadLayer\" pour ouvrir le fichier USD. La raison pour laquelle ce n'est pas LoadUSD est En effet, USD fait r\u00e9f\u00e9rence aux fichiers USD en tant que couches.</p> <p>Pour plus de d\u00e9tails \u00e0 ce sujet, https://fereria.github.io/reincarnation_tech/11_Pipeline/01_USD/04_layer_stage/ Voir article pr\u00e9c\u00e9dent.</p> <p></p> <p>Cependant, puisque l'ensemble de cuisine est Zup Comme ce sera \u00e9trange si c'est comme \u00e7a, ouvrez les fen\u00eatres 3D,  </p> <p></p> <p>Changez l'orientation en \"Z UP\".</p> <p></p> <p>J'ai pu charger avec succ\u00e8s. Je voudrais dire que ce sera OK si divers processus sont effectu\u00e9s \u00e0 ce sujet. En fait, SOLARIS a diff\u00e9rentes mani\u00e8res de lire l'USD. Jetons un coup d'\u0153il aux autres m\u00e9thodes de chargement.</p>"},{"location":"solaris/#chargercouche","title":"ChargerCouche","text":"<p>Tout d'abord, le LoadLayer utilis\u00e9 ci-dessus \"charge l'USD sp\u00e9cifi\u00e9 en tant que couche\".</p> <p></p> <p>C'est facile \u00e0 comprendre en regardant le SceneGraphPath, mais en utilisant ce n\u0153ud Pour root, \"chargez\" l'USD sp\u00e9cifi\u00e9 tel quel. Je n'ai rien fait d'autre.  </p>"},{"location":"solaris/#ouvrir-avec-le-n\u0153ud-darc-de-composition","title":"Ouvrir avec le n\u0153ud d'arc de composition","text":"<p>Alors, quel genre de m\u00e9thode existe-t-il autre que LoadLayer ? Divers n\u0153uds d'arc de composition pr\u00e9par\u00e9s comme n\u0153uds SOLARIS je vais le charger. Lorsqu'il est charg\u00e9 avec ce n\u0153ud, ainsi que la fonctionnalit\u00e9 des arcs de composition, Vous pouvez charger des fichiers USD. S'il est lu comme ceci, le ClassPrim dans le fichier usda charg\u00e9 est cach\u00e9 Il ne peut plus \u00eatre remplac\u00e9. Mais si je le charge avec LoadLayer puis que je connecte ce n\u0153ud \u00e0 un n\u0153ud de r\u00e9f\u00e9rence, Synth\u00e9tisez la couche que vous souhaitez lire par r\u00e9f\u00e9rence une fois en tant que sous-couche Apr\u00e8s cela &lt;/sdfPath\u2026&gt; Prim dans la sc\u00e8ne comme celle-ci Il sera charg\u00e9 en tant que r\u00e9f\u00e9rence.</p> <p>Dans ce cas, la couche n'est pas encapsul\u00e9e et une autre prim dans la couche sp\u00e9cifi\u00e9e est \u00e9galement charg\u00e9e. Semblable \u00e0 l'h\u00e9ritage, vous pouvez d\u00e9sormais remplacer la prim de r\u00e9f\u00e9rence. Pour plus de d\u00e9tails ici car la v\u00e9rification est r\u00e9sum\u00e9e se il vous pla\u00eet se r\u00e9f\u00e9rer.</p>"},{"location":"solaris/#ouvrir-dans-le-sous-calque","title":"Ouvrir dans le sous-calque","text":"<p>Tout d'abord, ouvrez-le dans le n\u0153ud Sous-couche.  </p> <p></p> <p>Lorsqu'il est ouvert avec une sous-couche, les bases sont les m\u00eames que lorsqu'il est charg\u00e9 avec LoadLayer.  </p> <p></p> <p>Le chemin du graphe de sc\u00e8ne sera \u00e9galement le m\u00eame que lorsque LoadLayer est utilis\u00e9. La diff\u00e9rence avec loadlayer est que vous pouvez combiner des fichiers USD avec des sous-couches avec ce seul n\u0153ud.  </p> <p></p> <p>Par exemple, chargez trois fichiers comme celui-ci :  </p> <p></p> <p>Comme une boule verte avec base.usda comme \u00e7a.  </p> <p></p> <p>Changez la couleur en rouge avec add_color.usda  </p> <p></p> <p>Essayez de le rendre carr\u00e9 avec final.usda.  </p> <p>Ensuite, il sera synth\u00e9tis\u00e9 dans l'ordre \u00e0 partir du haut, et le r\u00e9sultat de la synth\u00e8se de tout sera produit \u00e0 la suite de ce n\u0153ud.  </p> <p></p> <p>L'avantage de synth\u00e9tiser plusieurs USD dans cette sous-couche est Il est possible d'activer/d\u00e9sactiver l'effet de l'USD en utilisant Mute Layer et Enable. Par exemple, d\u00e9sactivons \"add_color.usda\" parmi ces trois \u00e9chantillons.  </p> <p></p> <p>Ensuite, ce qui se passe, c'est que seule la couche (fichier USD) appel\u00e9e \"Red\" est muette Vous vous retrouverez avec un cube vert. De cette fa\u00e7on, pour activer / d\u00e9sactiver la couche de logiciel 2D C'est l'effet du noeud Sublayer qui vous permet de basculer et de v\u00e9rifier facilement l'effet de calque USD.  </p> <p>Pendant le chargement, vous pouvez l'activer et le d\u00e9sactiver comme un calque PhotoShop C'est l'avantage de lire en utilisant ce n\u0153ud.</p>"},{"location":"solaris/#ouvrir-avec-r\u00e9f\u00e9rence","title":"Ouvrir avec r\u00e9f\u00e9rence","text":"<p>Essayez maintenant de le charger \u00e0 l'aide du n\u0153ud de r\u00e9f\u00e9rence.  </p> <p></p> <p>Le n\u0153ud de r\u00e9f\u00e9rence a \u00e9galement une fonction pour r\u00e9f\u00e9rencer le r\u00e9sultat d'entr\u00e9e de MultiInput  </p> <p> En r\u00e9glant Type de r\u00e9f\u00e9rence sur Fichiers de r\u00e9f\u00e9rence, Vous pouvez utiliser ce n\u0153ud pour \"Charger le fichier USD par r\u00e9f\u00e9rence\".  </p> <p></p> <p>En regardant le SceneGraphPath, nous pouvons voir que les noms primitifs sont verts. De plus, la grande diff\u00e9rence avec LoadLayer est que \"le nom du n\u0153ud sup\u00e9rieur est diff\u00e9rent\". Lorsqu'il est charg\u00e9 avec LoadLayer, la structure primitive \u00e9crite dans le fichier USD est Il sera charg\u00e9 tel quel. Mais pour les r\u00e9f\u00e9rences, la diff\u00e9rence est  </p> <p></p> <p>Sous le nom de la primitive sp\u00e9cifi\u00e9 dans Destination Primitive Lit la primitive USD lue par r\u00e9f\u00e9rence. Puisque $OS est le nom du n\u0153ud, la primitive enfant est charg\u00e9e sous reference1 qui est le nom du n\u0153ud.  </p> <p></p> <p>Essayez de cliquer avec le bouton droit sur le n\u0153ud de r\u00e9f\u00e9rence et d'ouvrir Actions LOP &gt; Inspecter la couche active.</p> <p></p><pre><code>#sdf 1.4.32\n\ndef HoudiniLayerInfo \"HoudiniLayerInfo\" (\n    donn\u00e9espersonnalis\u00e9es = {\n        cha\u00eene HoudiniCreatorNode = \"/stage/reference1\"\n        cha\u00eene[] HoudiniEditorNodes = [\"/stage/reference1\"]\n    }\n)\n{\n}\n\ndef \"r\u00e9f\u00e9rence1\" (\n    pr\u00e9fixer les r\u00e9f\u00e9rences = @C:/pyEnv/JupyterUSD_py27/usd/Kitchen_set/Kitchen_set.usd@\n)\n{\n}\n</code></pre> Lorsque vous ouvrez ce menu, vous pouvez voir l'\u00e9tat actuel de l'\u00e9dition du n\u0153ud (= fichier USD du n\u0153ud) Vous pouvez le v\u00e9rifier avec un fichier ASCII.   <p>J'ai expliqu\u00e9 que tous les n\u0153uds sont des couches, mais cet ActiveLayer est \"USD de ce n\u0153ud\" C'est pourquoi. Donc, en regardant cela, vous pouvez voir ce que fait ce n\u0153ud maintenant et ce qu'il fait en termes d'USD Tu peux le v\u00e9rifier.  </p> <p>Donc, d'apr\u00e8s ce que je peux voir, vous pouvez voir qu'il est lu par r\u00e9f\u00e9rence.  </p>"},{"location":"solaris/#diff\u00e9rence-de-lecture-avec-le-n\u0153ud-de-r\u00e9f\u00e9rence--20200105-ajout\u00e9","title":"Diff\u00e9rence de lecture avec le n\u0153ud de r\u00e9f\u00e9rence * 2020/01/05 ajout\u00e9","text":"<p>Il a \u00e9t\u00e9 trouv\u00e9 plus tard lors de l'examen du comportement d\u00e9taill\u00e9 du n\u0153ud Lorsqu'il est charg\u00e9 \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence et apr\u00e8s le chargement \u00e0 l'aide d'un LoadLayer J'ai trouv\u00e9 qu'il y a une grande diff\u00e9rence entre les n\u0153uds de connexion et de r\u00e9f\u00e9rence. La \"diff\u00e9rence\" est si encapsuler ou non.</p> <p>Lorsqu'il est charg\u00e9 avec un n\u0153ud de r\u00e9f\u00e9rence, lorsqu'il est visualis\u00e9 avec l'USDA pr\u00e9fixer les r\u00e9f\u00e9rences = @C:/pyEnv/JupyterUSD_py27/usd/Kitchen_set/Kitchen_set.usd@ En sp\u00e9cifiant le chemin du fichier comme celui-ci, le prim sp\u00e9cifi\u00e9 par USDA DefaultPrim ou SdfPath du chemin sp\u00e9cifi\u00e9 sera charg\u00e9 par r\u00e9f\u00e9rence.</p>"},{"location":"solaris/#ouvrir-avec-stagemanager","title":"Ouvrir avec StageManager","text":"<p>Enfin, comment ouvrir autre que l'arc de composition. C'est le noeud StageManager.  </p> <p>Il existe de nombreux exemples d'utilisation de ce StageManager dans les vid\u00e9os de didacticiel Quelle est la diff\u00e9rence entre les trois ci-dessus et ce StageManager ?  </p> <p>Comme la grande diff\u00e9rence est le gestionnaire \"Stage\", ce n\u0153ud est Vous pouvez construire une sc\u00e8ne par elle-m\u00eame.  </p> <p>Une \u00e9tape dans ce cas fait r\u00e9f\u00e9rence au r\u00e9sultat de la composition de plusieurs fichiers USD.  </p> <p></p> <p>Une fois ouvert, l'\u00e9cran StageManager ressemblera \u00e0 la fen\u00eatre de l'image ci-dessus. Cliquez sur l'ic\u00f4ne du dossier qu'il contient.  </p> <p></p> <p>Jusqu'\u00e0 pr\u00e9sent, les n\u0153uds \u00e9taient principalement ceux qui lisaient un fichier USD avec un n\u0153ud Ce StageManager construit une sc\u00e8ne (un graphe de sc\u00e8ne r\u00e9sultant de la composition USD) par lui-m\u00eame Parce que c'est un n\u0153ud qui peut le faire, chargez plusieurs USD avec des r\u00e9f\u00e9rences, etc. Vous pouvez construire une \u00e9tape tout en regroupant.</p> <p></p> <p>Ainsi, pour ce n\u0153ud StageManager seul, le ScenerGraphPath est Une \u00e9tape termin\u00e9e est construite. (Orange est charg\u00e9 par l'arc de composition)  </p>"},{"location":"solaris/#\u00e9crivez-le-fichier-ascii-tel-quel-et-ouvrez-le","title":"\u00c9crivez le fichier ASCII tel quel et ouvrez-le","text":"<p>Peu de gens le font, mais avec inlineusd, Vous pouvez \u00e9crire et lire des fichiers USD \u00e0 la main au lieu de fichiers.</p> <p></p> <p>En \u00e9crivant en ASCII pour USD Source comme ceci,</p> <p></p> <p>Vous pouvez r\u00e9ellement cr\u00e9er une sc\u00e8ne dans SceneGraphPath comme celle-ci.  </p>"},{"location":"solaris/#alors-comment-puis-je-louvrir----","title":"Alors, comment puis-je l'ouvrir ! ! ? ?","text":"<p>Comme vous pouvez le voir, il existe de nombreuses fa\u00e7ons d'ouvrir un fichier USD. Je ne sais pas quoi faire avec \u00e7a ! Pr\u00e9sentation de ma propre recommandation pour ceux qui disent.  </p>"},{"location":"solaris/#stagemnaager-si-vous-voulez-placer-beaucoup-dobjets","title":"\"StageMnaager\" si vous voulez placer beaucoup d'objets","text":"<p>Si vous voulez faire des mises en page dans la sc\u00e8ne, vous pourriez vous retrouver avec des centaines d'\u00e9l\u00e9ments. Vous devrez le charger et le placer. Dans un tel cas, il est tr\u00e8s g\u00eanant de les lire un par un ou de les regrouper.  </p> <p>Dans ce cas, si vous utilisez StageManager, vous pouvez charger tous ensemble par r\u00e9f\u00e9rence Le chargement de plusieurs actifs est facile avec le glisser-d\u00e9poser Vous pouvez facilement effectuer des op\u00e9rations telles que le regroupement et la modification de la hi\u00e9rarchie.  </p> <p>En d'autres termes, la construction de la sc\u00e8ne est termin\u00e9e avec ce seul n\u0153ud. Par cons\u00e9quent, ce n\u0153ud est recommand\u00e9 pour les mises en page.</p> <p></p> <p>Bien s\u00fbr, si vous mettez tout cela ensemble dans un seul StageManager, cela deviendra d\u00e9sordonn\u00e9 ! ! ! Dans ce cas, vous pouvez \u00e9galement connecter plusieurs StageManagers. Dans ce cas, ajoutez des assets avec + \u03b1 au graphe de sc\u00e8ne de la sc\u00e8ne re\u00e7u en entr\u00e9e devient possible.  </p>"},{"location":"solaris/#reference-si-vous-souhaitez-encapsuler-une-couche","title":"\"Reference\" si vous souhaitez encapsuler une couche","text":"<p>DefaultPrim de la couche que vous souhaitez lire par r\u00e9f\u00e9rence ou autre que la prim sp\u00e9cifi\u00e9e Si vous souhaitez \"l'encapsuler\", chargez-le \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence. Lorsqu'elles sont lues avec le n\u0153ud de r\u00e9f\u00e9rence, toutes les prims autres que la prim sp\u00e9cifi\u00e9e sont masqu\u00e9es. Il ne peut plus \u00eatre remplac\u00e9. Donc, si vous voulez lire un caract\u00e8re, etc. Le chargement \u00e0 l'aide d'un n\u0153ud de r\u00e9f\u00e9rence semble bon.</p>"},{"location":"solaris/#sinon-loadlayer","title":"sinon \"LoadLayer\"","text":"<p>En dehors de cela, LoadLayer + n\u0153ud d'arc de composition ou autre n\u0153ud pratique (Graft, etc.) est recommand\u00e9.  </p> <p>Apr\u00e8s cela, je veux construire une structure bas\u00e9e sur la base de n\u0153uds de SOLARIS pour le mod\u00e8le lu par LoadLayer (Tout en combinant des n\u0153uds l\u00e9gers qui souhaitent cr\u00e9er des variations avec des variantes et attribuer des shaders Je veux construire une sc\u00e8ne\u2026 etc.  </p> <p></p> <p>Vous pouvez lire le fichier directement avec SubLayer ou Reference Dans ce cas, il est tr\u00e8s difficile de remplacer l'ordre d'entr\u00e9e ou de le d\u00e9sactiver temporairement. Je l'ai trouv\u00e9 difficile \u00e0 utiliser car la listabilit\u00e9 au niveau du n\u0153ud n'\u00e9tait pas si bonne.  </p> <p>Si vous lisez et composez toujours avec LoadLayer, Il est facile de basculer et clarifie \"lecture d'un fichier\" Personnellement, je pense qu'il est pr\u00e9f\u00e9rable d'utiliser LoadLayer pour le chargement unique de fichiers USD.  </p>"},{"location":"solaris/#bien-s\u00fbr-vous-pouvez-utiliser-les-deux","title":"Bien s\u00fbr, vous pouvez utiliser les deux","text":"<p>Bien entendu, les n\u0153uds d'arc de composition StageMnaager et LoadLayer + peuvent \u00eatre utilis\u00e9s ensemble.  </p> <p></p> <p>Lors de la construction collective d'une sc\u00e8ne avec StageManager, faites des variations d\u00e9taill\u00e9es avec des n\u0153uds Enfin, synth\u00e9tisez En utilisant diff\u00e9rentes mani\u00e8res d'ouvrir plusieurs USD dans le bon sens Il est \u00e9galement possible de construire le plus simplement possible des n\u0153uds dans une sc\u00e8ne complexe.  </p>"},{"location":"solaris/#sommaire","title":"sommaire","text":"<p>Il s'agissait d'une introduction \u00e0 l'ouverture de fichiers USD dans SOLARIS/LOP. Il est \u00e9tonnant qu'il existe autant d'approches diff\u00e9rentes simplement en ouvrant un seul fichier USD. Vous pouvez \u00e9galement ouvrir des sc\u00e8nes + SOP avec LOP et ainsi de suite. SOLARIS est tr\u00e8s flexible lorsqu'il s'agit d'USD et permet la cr\u00e9ation de divers USD en fonction de la fa\u00e7on de penser. Je pensais que c'\u00e9tait un outil.  </p> <p>Je me demande si ce genre d'article est la premi\u00e8re \u00e9tape pour \u00e9crire beaucoup d'articles SOLARIS ? Ensuite, il s'agit du n\u0153ud d'arc de composition que j'ai \u00e9crit bri\u00e8vement cette fois J'aimerais \u00e9crire un peu plus \u00e0 ce sujet.  </p>"},{"location":"C/Cmake/","title":"CMake","text":"<p>Info</p> <p>CMake is an open-source, cross-platform family of tools designed to build, test and package software. CMake is used to control the software compilation process using simple platform and compiler independent configuration files, and generate native makefiles and workspaces that can be used in the compiler environment of your choice. The suite of CMake tools were created by Kitware in response to the need for a powerful, cross-platform build environment for open-source projects such as ITK and VTK. CMake is part of Kitware\u2019s collection of commercially supported open-source platforms for software development.</p>"},{"location":"C/Math101/","title":"Math101","text":""},{"location":"C/Math101/#math","title":"MATH","text":"<p>basics</p>"},{"location":"C/Math101/#negate-x-1","title":"Negate x*-1","text":"<p>addition   (order doesn't matter) 2+4=6</p> <p>soustraction (order matters!) 2-4=-2</p> <p>2+(-4) =-2   4*-(1) negate(mult by -1)</p> <p>ex  2* (-1) = -2 -2 * (-1) = 2 </p> <p>ex: when we want to subtract one vector from another,in order to control the order of the vector we instead add them together and negate the one we want to be subtracted.</p>"},{"location":"C/Math101/#invert-1x","title":"Invert 1/x","text":"<p>Similar relationship than addition and substraction</p> <p>Multiplication(order doesn't matter) 2*4=8</p> <p>Division(order matters ) 2/4 = 0.5</p> <ul> <li>we can represent division in term of multiplication usiong the invert function</li> </ul> <p>2*(0.125) = 0.5  (\u00bc, invert) 1/x</p>"},{"location":"C/Math101/#power-and-root","title":"Power and Root","text":"<p>Power 2 ^ 3= 8</p> <p>Root 3 squareroot of 8 = 2</p> <p>same as </p> <p>8 ^ 0.33 = 2 \u2153 invert</p> <p></p> <p>Note</p> <p>When we fix our colors (OETF, gamma) we essentialy multiply by 2.2. and when we want toreturnto linear color space, we multiply by the inverse  of 2.2 </p>"},{"location":"C/SolidAngle/","title":"Dolid Angle","text":""},{"location":"C/SolidAngle/#basics","title":"Basics:","text":"<ol> <li>Definition:</li> <li>The solid angle ( Omega ) at a point in space is defined as the ratio of the subtended surface area (( A )) to the square of the distance (( r )) from the point.</li> </ol> <p>[ Omega = frac{A}{r^2} ]</p> <ol> <li>Unit:</li> <li> <p>The unit of solid angle is the steradian (sr). A sphere with a total surface area of ( 4pi ) steradians is completely surrounding a point.</p> </li> <li> <p>Complete Sphere:</p> </li> <li>For a point at the center of a sphere, the solid angle subtended by the entire sphere is ( 4pi ) sr.</li> </ol> <p>[ Omega_{text{sphere}} = 4pi , text{sr} ]</p> <ol> <li>Half-Space:</li> <li>The solid angle subtended by a hemisphere (half of a sphere) at its boundary is ( 2pi ) sr.</li> </ol> <p>[ Omega_{text{hemisphere}} = 2pi , text{sr} ]</p>"},{"location":"C/SolidAngle/#calculation","title":"Calculation:","text":"<ol> <li>For a Cone:</li> <li>For a cone with apex angle ( theta ) at a vertex, the solid angle ( Omega ) subtended by the cone is given by:</li> </ol> <p>[ Omega_{text{cone}} = 2pi (1 - cos(theta/2)) ]</p> <ol> <li>For a Surface Patch:</li> <li>For a small surface patch with area ( dA ) at a distance ( r ) from a point, the solid angle ( dOmega ) subtended by the patch is given by:</li> </ol> <p>[ dOmega = frac{dA}{r^2} ]</p>"},{"location":"C/SolidAngle/#importance-in-physics-and-computer-graphics","title":"Importance in Physics and Computer Graphics:","text":"<ol> <li>Radiometry:</li> <li> <p>In radiometry, the solid angle is used to measure the amount of radiant flux (energy) intercepted by a surface.</p> </li> <li> <p>Lighting and Illumination:</p> </li> <li> <p>In computer graphics, solid angle is important for understanding how light is distributed in a scene. It is used in the rendering equation to model how light interacts with surfaces.</p> </li> <li> <p>Spherical Coordinates:</p> </li> <li> <p>In spherical coordinates, the solid angle is used to describe the angular extent of regions on a sphere.</p> </li> <li> <p>Probability:</p> </li> <li>In probability theory, solid angle is used when defining the probability density over a sphere.</li> </ol> <p>Understanding solid angles is crucial in various fields, especially when dealing with three-dimensional space and the distribution of energy, light, or information. It provides a way to quantify the \"size\" of regions in space as seen from a specific point.</p>"},{"location":"C/SolidAngle/#fondamentaux-","title":"Fondamentaux :","text":"<ol> <li>D\u00e9finition :</li> <li>L'angle solide ( Omega ) en un point de l'espace est d\u00e9fini comme le rapport de la surface sous-tendue (( A )) au carr\u00e9 de la distance (( r )) \u00e0 partir du point.</li> </ol> <p>[ Omega = frac{A}{r^2} ]</p> <ol> <li>Unit\u00e9 :</li> <li> <p>L'unit\u00e9 de l'angle solide est le st\u00e9radian (sr). Une sph\u00e8re avec une aire totale de ( 4pi ) st\u00e9radians entoure compl\u00e8tement un point.</p> </li> <li> <p>Sph\u00e8re compl\u00e8te :</p> </li> <li>Pour un point au centre d'une sph\u00e8re, l'angle solide sous-tendu par la sph\u00e8re enti\u00e8re est ( 4pi ) sr.</li> </ol> <p>[ Omega_{text{sph\u00e8re}} = 4pi , text{sr} ]</p> <ol> <li>Demi-espace :</li> <li>L'angle solide sous-tendu par une h\u00e9misph\u00e8re (la moiti\u00e9 d'une sph\u00e8re) \u00e0 sa fronti\u00e8re est ( 2pi ) sr.</li> </ol> <p>[ Omega_{text{h\u00e9misph\u00e8re}} = 2pi , text{sr} ]</p>"},{"location":"C/SolidAngle/#calcul-","title":"Calcul :","text":"<ol> <li>Pour un c\u00f4ne :</li> <li>Pour un c\u00f4ne avec un angle au sommet ( theta ) \u00e0 un sommet, l'angle solide ( Omega ) sous-tendu par le c\u00f4ne est donn\u00e9 par :</li> </ol> <p>[ Omega_{text{c\u00f4ne}} = 2pi (1 - cos(theta/2)) ]</p> <ol> <li>Pour un patch de surface :</li> <li>Pour un petit patch de surface avec une aire ( dA ) \u00e0 une distance ( r ) d'un point, l'angle solide ( dOmega ) sous-tendu par le patch est donn\u00e9 par :</li> </ol> <p>[ dOmega = frac{dA}{r^2} ]</p>"},{"location":"C/SolidAngle/#importance-en-physique-et-en-infographie-","title":"Importance en physique et en infographie :","text":"<ol> <li>Radiom\u00e9trie :</li> <li> <p>En radiom\u00e9trie, l'angle solide est utilis\u00e9 pour mesurer la quantit\u00e9 de flux radiant (\u00e9nergie) intercept\u00e9e par une surface.</p> </li> <li> <p>\u00c9clairage et illumination :</p> </li> <li> <p>En infographie, l'angle solide est important pour comprendre comment la lumi\u00e8re est distribu\u00e9e dans une sc\u00e8ne. Il est utilis\u00e9 dans l'\u00e9quation de rendu pour mod\u00e9liser comment la lumi\u00e8re interagit avec les surfaces.</p> </li> <li> <p>Coordonn\u00e9es sph\u00e9riques :</p> </li> <li> <p>En coordonn\u00e9es sph\u00e9riques, l'angle solide est utilis\u00e9 pour d\u00e9crire l'\u00e9tendue angulaire des r\u00e9gions sur une sph\u00e8re.</p> </li> <li> <p>Probabilit\u00e9 :</p> </li> <li>En th\u00e9orie des probabilit\u00e9s, l'angle solide est utilis\u00e9 pour d\u00e9finir la densit\u00e9 de probabilit\u00e9 sur une sph\u00e8re.</li> </ol> <p>Comprendre les angles solides est crucial dans divers domaines, en particulier lorsqu'il s'agit de l'espace tridimensionnel et de la distribution de l'\u00e9nergie, de la lumi\u00e8re ou de l'information. Cela offre une mani\u00e8re de quantifier la \"taille\" des r\u00e9gions dans l'espace telles qu'elles sont vues \u00e0 partir d'un point sp\u00e9cifique.</p>"},{"location":"C/hyperV/","title":"Hpyer-V \u3067\u4eee\u60f3\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u3092\u4f5c\u6210\u3059\u308b","text":""},{"location":"C/hyperV/#cr\u00e9er-un-bureau-virtuel-avec-hyper-v","title":"Cr\u00e9er un bureau virtuel avec Hyper-V","text":""},{"location":"C/hyperV/#activation-dhyper-v","title":"Activation d'Hyper-V","text":"<p>Tout d'abord, activez Hyper-V pour pouvoir utiliser des bureaux virtuels.</p> <p></p> <p>Dans Windows Search, s\u00e9lectionnez Activer ou d\u00e9sactiver les fonctionnalit\u00e9s Windows. 2 </p> <p>S\u00e9lectionnez \"hyper-V\" dans la liste. Apr\u00e8s avoir s\u00e9lectionn\u00e9, Windows sera (probablement) red\u00e9marr\u00e9.</p> <p></p> <p>Dans la barre de recherche saisissez \"Hyper-V Manager\" et ex\u00e9cutez-le..</p> <p>Note</p> <p>Si Hyper-V n'est pas activ\u00e9 dans les fonctionnalit\u00e9s Windows, Hyper-V Manager ne s'affichera pas.</p>"},{"location":"C/hyperV/#cr\u00e9er-un-pc-virtuel","title":"Cr\u00e9er un pc virtuel","text":"<p>S\u00e9lectionnez Actions \u2192 Cliquez sur Cr\u00e9er....</p> <p></p> <p>D\u00e9finissez le syst\u00e8me d'exploitation sur Windows 10 DevEnviroment. Une fois que vous l'avez ex\u00e9cut\u00e9, tout ce que vous avez \u00e0 faire est d'attendre  et un environnement PC virtuel sera cr\u00e9\u00e9 pour vous.</p> <p></p> <p>Cliquez sur \"Connecter\" lorsque vous avez termin\u00e9.</p>"},{"location":"C/hyperV/#setup","title":"Setup","text":"<p>Tout d'abord, d\u00e9marrez l'environnement virtuel cr\u00e9\u00e9 et avant les param\u00e8tres initiaux de Windows,  configurez la mise en r\u00e9seau de votre environnement virtuel.</p> <p>Ex\u00e9cutez Action \u2192 Gestionnaire de commutateur virtuel.</p> <p></p> <p>S\u00e9lectionnez le nouveau commutateur de r\u00e9seau virtuel, s\u00e9lectionnez \"Externe\" \u2192 Cr\u00e9er un commutateur virtuel.</p> <p></p> <p>S\u00e9lectionnez le syst\u00e8me d'exploitation de gestion \"R\u00e9seau externe\". Cochez ON et \"Appliquer\". Connectez-vous ensuite \u00e0 la machine virtuelle cr\u00e9\u00e9e. S\u00e9lectionnez Param\u00e8tres de la machine virtuelle pour ouvrir l'\u00e9cran des param\u00e8tres de l'environnement virtuel.</p> <p></p> <p>Depuis l'adaptateur r\u00e9seau, remplacez le commutateur virtuel par celui cr\u00e9\u00e9 en \u2191. Si ce r\u00e9glage n'a pas \u00e9t\u00e9 effectu\u00e9, le r\u00e9glage LAN d'Internet ne s'affichera pas.  Internet n'\u00e9tait pas disponible dans l'environnement virtuel.</p>"},{"location":"C/hyperV/#cr\u00e9er-un-point-de-contr\u00f4le","title":"Cr\u00e9er un point de contr\u00f4le","text":"<p>La bonne chose \u00e0 propos des environnements virtuels est que vous pouvez cr\u00e9er des \"points de contr\u00f4le\". Vous pouvez enregistrer les param\u00e8tres actuels de l'environnement virtuel.</p> <p>Comme cette fois, je veux v\u00e9rifier le test de l'outil dans un \u00e9tat compl\u00e8tement nu  (autre que mon propre environnement avec certains param\u00e8tres)  Donc, cr\u00e9ez un \"\u00e9tat initial\" avec seulement l'installation minimale Je veux revenir \u00e0 ce moment si n\u00e9cessaire.</p> <p></p> <p>\"Op\u00e9ration\" \u2192 \"Point de contr\u00f4le\" sur l'\u00e9cran de connexion de la machine virtuelle Choisir.</p> <p></p> <p>Donnez-lui un nom et cliquez sur \"oui\".</p> <p></p> <p>Un arbre est cr\u00e9\u00e9 pour chaque point de contr\u00f4le. Donc, cr\u00e9ez un point de contr\u00f4le au moment o\u00f9 vous voulez ramifier le travail dans une certaine mesure Ce faisant, vous pouvez retourner tout l'environnement ou changer jusqu'\u00e0 un certain point. Puisqu'il est possible de rebrancher \u00e0 partir de la synchronisation modifi\u00e9e Vous pouvez reproduire la situation dans divers mod\u00e8les et environnements.</p> <p>De plus, lorsque vous cr\u00e9ez un point de contr\u00f4le, non seulement l'environnement de r\u00e9glage du PC \u00c0 ce moment-l\u00e0, \"l'\u00e9tat de l'application en cours d'ex\u00e9cution\" est enregistr\u00e9. Par exemple, m\u00eame si vous cr\u00e9ez un \"point de contr\u00f4le\" avec Maya en cours d'ex\u00e9cution Maya sera sauvegard\u00e9e comme elle a commenc\u00e9.  La cr\u00e9ation de l'environnement est maintenant termin\u00e9e. Travailler avec Maya sur une machine virtuelle \u00e9tait comme travailler sur un PC normal. L'atmosph\u00e8re est comme l'utilisation de Maya via un bureau \u00e0 distance.</p> <p>L'environnement Python a \u00e9t\u00e9 s\u00e9par\u00e9 dans une certaine mesure avec pipenv etc. Je voulais tester l'environnement autour de Maya compl\u00e8tement s\u00e9par\u00e9ment, y compris les param\u00e8tres Windows. Il peut \u00eatre assez bon de construire avec une machine virtuelle simple et performante.</p>"},{"location":"C/hyperV/#\u53c2\u8003","title":"\u53c2\u8003","text":"<ul> <li>https://qiita.com/nomurasan/items/3c58b964943a24751802</li> <li>https://fereria.github.io/reincarnation_tech/10_Programming/00_Settings/hyper-v/</li> </ul>"},{"location":"C/numericsLimits/","title":"Numerics limit","text":""},{"location":"C/numericsLimits/#numeric-limits","title":"Numeric Limits","text":"<p>Note</p> <p>Below demonstrates the limits of all the numeric primitive types in c++ and therefore can     help in chosing which would work best for certain scernarios.</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;cstdint&gt;\n#include &lt;limits&gt;\n\n\ntemplate&lt;typename T&gt;\nstd::string typeStr(){\n    std::string pf(__PRETTY_FUNCTION__);\n    auto tEqualPos = pf.rfind(\"T = \");\n    auto closeBracPos = pf.rfind(\"]\");\n    if(tEqualPos != std::string::npos &amp;&amp;\n            closeBracPos != std::string::npos &amp;&amp;\n            closeBracPos &gt; tEqualPos){\n        return pf.substr(tEqualPos + 4, closeBracPos - 4 - tEqualPos);\n    }else{\n        return \"indeterminate\";\n    }\n}\n\nint main(int argc, char* argv[]){\n    std::cout &lt;&lt; \"Unsigned integers (only numbers &gt;= 0)\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               uint8_t low: \" &lt;&lt; static_cast&lt;uint16_t&gt;(std::numeric_limits&lt;uint8_t&gt;::lowest()) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               uint8_t min: \" &lt;&lt; static_cast&lt;uint16_t&gt;(std::numeric_limits&lt;uint8_t&gt;::min()) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               uint8_t max: \" &lt;&lt; static_cast&lt;uint16_t&gt;(std::numeric_limits&lt;uint8_t&gt;::max()) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"            sizeof uint8_t: \" &lt;&lt; sizeof (uint8_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"    actual type of uint8_t: \" &lt;&lt; typeStr&lt;uint8_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint16_t low: \" &lt;&lt; std::numeric_limits&lt;uint16_t&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint16_t min: \" &lt;&lt; std::numeric_limits&lt;uint16_t&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint16_t max: \" &lt;&lt; std::numeric_limits&lt;uint16_t&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"           sizeof uint16_t: \" &lt;&lt; sizeof (uint16_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"   actual type of uint16_t: \" &lt;&lt; typeStr&lt;uint16_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint32_t low: \" &lt;&lt; std::numeric_limits&lt;uint32_t&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint32_t min: \" &lt;&lt; std::numeric_limits&lt;uint32_t&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint32_t max: \" &lt;&lt; std::numeric_limits&lt;uint32_t&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"           sizeof uint32_t: \" &lt;&lt; sizeof (uint32_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"   actual type of uint32_t: \" &lt;&lt; typeStr&lt;uint32_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                  uint min: \" &lt;&lt; std::numeric_limits&lt;uint&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                  uint min: \" &lt;&lt; std::numeric_limits&lt;uint&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                  uint max: \" &lt;&lt; std::numeric_limits&lt;uint&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               sizeof uint: \" &lt;&lt; sizeof (uint) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"       actual type of uint: \" &lt;&lt; typeStr&lt;uint&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint64_t low: \" &lt;&lt; std::numeric_limits&lt;uint64_t&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint64_t min: \" &lt;&lt; std::numeric_limits&lt;uint64_t&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              uint64_t max: \" &lt;&lt; std::numeric_limits&lt;uint64_t&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"           sizeof uint64_t: \" &lt;&lt; sizeof (uint64_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"   actual type of uint64_t: \" &lt;&lt; typeStr&lt;uint64_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                size_t low: \" &lt;&lt; std::numeric_limits&lt;size_t&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                size_t min: \" &lt;&lt; std::numeric_limits&lt;size_t&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                size_t max: \" &lt;&lt; std::numeric_limits&lt;size_t&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"             sizeof size_t: \" &lt;&lt; sizeof (size_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"     actual type of size_t: \" &lt;&lt; typeStr&lt;size_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Signed Integers\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                int8_t low: \" &lt;&lt; static_cast&lt;int16_t&gt;(std::numeric_limits&lt;int8_t&gt;::lowest()) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                int8_t min: \" &lt;&lt; static_cast&lt;int16_t&gt;(std::numeric_limits&lt;int8_t&gt;::min()) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                int8_t max: \" &lt;&lt; static_cast&lt;int16_t&gt;(std::numeric_limits&lt;int8_t&gt;::max()) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"             sizeof int8_t: \" &lt;&lt; sizeof (int8_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"     actual type of int8_t: \" &lt;&lt; typeStr&lt;int8_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int16_t low: \" &lt;&lt; std::numeric_limits&lt;int16_t&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int16_t min: \" &lt;&lt; std::numeric_limits&lt;int16_t&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int16_t max: \" &lt;&lt; std::numeric_limits&lt;int16_t&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"            sizeof int16_t: \" &lt;&lt; sizeof (int16_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"    actual type of int16_t: \" &lt;&lt; typeStr&lt;int16_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int32_t min: \" &lt;&lt; std::numeric_limits&lt;int32_t&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int32_t min: \" &lt;&lt; std::numeric_limits&lt;int32_t&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int32_t max: \" &lt;&lt; std::numeric_limits&lt;int32_t&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"            sizeof int32_t: \" &lt;&lt; sizeof (int32_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"    actual type of int32_t: \" &lt;&lt; typeStr&lt;int32_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                   int min: \" &lt;&lt; std::numeric_limits&lt;int&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                   int min: \" &lt;&lt; std::numeric_limits&lt;int&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                   int max: \" &lt;&lt; std::numeric_limits&lt;int&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                sizeof int: \" &lt;&lt; sizeof (int) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"        actual type of int: \" &lt;&lt; typeStr&lt;int&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int64_t low: \" &lt;&lt; std::numeric_limits&lt;int64_t&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int64_t min: \" &lt;&lt; std::numeric_limits&lt;int64_t&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"               int64_t max: \" &lt;&lt; std::numeric_limits&lt;int64_t&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"            sizeof int64_t: \" &lt;&lt; sizeof (int64_t) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"    actual type of int64_t: \" &lt;&lt; typeStr&lt;int64_t&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Floating Point Numbers\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                 float low: \" &lt;&lt; std::numeric_limits&lt;float&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                 float min: \" &lt;&lt; std::numeric_limits&lt;float&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                 float max: \" &lt;&lt; std::numeric_limits&lt;float&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"              sizeof float: \" &lt;&lt; sizeof (float) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"      actual type of float: \" &lt;&lt; typeStr&lt;float&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                double low: \" &lt;&lt; std::numeric_limits&lt;double&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                double min: \" &lt;&lt; std::numeric_limits&lt;double&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"                double max: \" &lt;&lt; std::numeric_limits&lt;double&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"             sizeof double: \" &lt;&lt; sizeof (double) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"     actual type of double: \" &lt;&lt; typeStr&lt;double&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"           long double low: \" &lt;&lt; std::numeric_limits&lt;long double&gt;::lowest() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"           long double min: \" &lt;&lt; std::numeric_limits&lt;long double&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"           long double max: \" &lt;&lt; std::numeric_limits&lt;long double&gt;::max() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"        sizeof long double: \" &lt;&lt; sizeof (long double) &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"actual type of long double: \" &lt;&lt; typeStr&lt;long double&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n    return 0;\n\n}\n</code></pre> <pre><code>Compile Command:\ng++-5 -std=c++14 main.cpp -o exampleCpp\nExecution Call:\n./exampleCpp\nOutput:\nUnsigned integers (only numbers &gt;= 0)\n               uint8_t low: 0\n               uint8_t min: 0\n               uint8_t max: 255\n            sizeof uint8_t: 1\n    actual type of uint8_t: unsigned char; std::string = std::basic_string&lt;char&gt;\n\n              uint16_t low: 0\n              uint16_t min: 0\n              uint16_t max: 65535\n           sizeof uint16_t: 2\n   actual type of uint16_t: short unsigned int; std::string = std::basic_string&lt;char&gt;\n\n              uint32_t low: 0\n              uint32_t min: 0\n              uint32_t max: 4294967295\n           sizeof uint32_t: 4\n   actual type of uint32_t: unsigned int; std::string = std::basic_string&lt;char&gt;\n\n                  uint min: 0\n                  uint min: 0\n                  uint max: 4294967295\n               sizeof uint: 4\n       actual type of uint: unsigned int; std::string = std::basic_string&lt;char&gt;\n\n              uint64_t low: 0\n              uint64_t min: 0\n              uint64_t max: 18446744073709551615\n           sizeof uint64_t: 8\n   actual type of uint64_t: long unsigned int; std::string = std::basic_string&lt;char&gt;\n\n                size_t low: 0\n                size_t min: 0\n                size_t max: 18446744073709551615\n             sizeof size_t: 8\n     actual type of size_t: long unsigned int; std::string = std::basic_string&lt;char&gt;\n\nSigned Integers\n                int8_t low: -128\n                int8_t min: -128\n                int8_t max: 127\n             sizeof int8_t: 1\n     actual type of int8_t: signed char; std::string = std::basic_string&lt;char&gt;\n\n               int16_t low: -32768\n               int16_t min: -32768\n               int16_t max: 32767\n            sizeof int16_t: 2\n    actual type of int16_t: short int; std::string = std::basic_string&lt;char&gt;\n\n               int32_t min: -2147483648\n               int32_t min: -2147483648\n               int32_t max: 2147483647\n            sizeof int32_t: 4\n    actual type of int32_t: int; std::string = std::basic_string&lt;char&gt;\n\n                   int min: -2147483648\n                   int min: -2147483648\n                   int max: 2147483647\n                sizeof int: 4\n        actual type of int: int; std::string = std::basic_string&lt;char&gt;\n\n               int64_t low: -9223372036854775808\n               int64_t min: -9223372036854775808\n               int64_t max: 9223372036854775807\n            sizeof int64_t: 8\n    actual type of int64_t: long int; std::string = std::basic_string&lt;char&gt;\n\nFloating Point Numbers\n                 float low: -3.40282e+38\n                 float min: 1.17549e-38\n                 float max: 3.40282e+38\n              sizeof float: 4\n      actual type of float: float; std::string = std::basic_string&lt;char&gt;\n\n                double low: -1.79769e+308\n                double min: 2.22507e-308\n                double max: 1.79769e+308\n             sizeof double: 8\n     actual type of double: double; std::string = std::basic_string&lt;char&gt;\n\n           long double low: -1.18973e+4932\n           long double min: 3.3621e-4932\n           long double max: 1.18973e+4932\n        sizeof long double: 16\nactual type of long double: long double; std::string = std::basic_string&lt;char&gt;\n</code></pre>"},{"location":"C/vcpkg/","title":"Vcpkg","text":""},{"location":"C/vcpkg/#get-started-with-vcpkg","title":"Get started with vcpkg","text":"<p>visit vcpkg</p> <p>Info</p> <p>vcpkg is a free C/C++ package manager for acquiring and managing libraries.  Choose from over 1500 open source libraries to download and build in a single  step or add your own private libraries to simplify your build process.  Maintained by the Microsoft C++ team and open source contributors.</p>"},{"location":"C/vcpkg/#install-vcpkg","title":"Install vcpkg","text":"<p>Installing vcpkg is a two-step process: first, clone the repo, then run the bootstrapping script to produce the vcpkg binary.  The repo can be cloned anywhere, and will include the vcpkg binary after bootstrapping as well as any libraries that are installed from the command line. </p> <p>It is recommended to clone vcpkg as a submodule for CMake projects, but to install it globally for MSBuild projects. If installing globally, we recommend a short install path like: <code>sh C:\\src\\vcpkg</code> or <code>sh C:\\dev\\vcpkg</code>, since otherwise you may run into path issues for some port build systems.</p> <p>Step 1: Clone the vcpkg repo</p> <pre><code>git clone https://github.com/Microsoft/vcpkg.git\n</code></pre> <p>Make sure you are in the directory you want the tool installed to before doing this.</p> <p>Step 2: Run the bootstrap script to build vcpkg</p> <pre><code>.\\vcpkg\\bootstrap-vcpkg.bat\n</code></pre> <p>Install libraries for your project</p> <pre><code>vcpkg install [packages to install]\n</code></pre> <p>Using vcpkg with MSBuild / Visual Studio (may require elevation)</p> <pre><code>vcpkg integrate install\n</code></pre> <p>After this, you can create a new project or open an existing one in the IDE. All installed libraries should already be discoverable by IntelliSense and usable in code without additional configuration.</p> <p>Using vcpkg with CMake</p> <p>In order to use vcpkg with CMake outside of an IDE, you can use the toolchain file:</p> <pre><code>cmake -B [build directory] -S . -DCMAKE_TOOLCHAIN_FILE=[path to vcpkg]/scripts/buildsystems/vcpkg.cmake\n</code></pre> <p>Then build with:</p> <pre><code>cmake --build [build directory]\n</code></pre> <p>With CMake, you will need to use find_package() to reference the libraries in your Cmakelists.txt files.</p>"},{"location":"Maya/IntroMaya/","title":"Introduction","text":"<p>:::caution</p> <p>\u3053\u306e\u30b5\u30a4\u30c8\u306f\u3001mkdocs \u3092\u4f7f\u7528\u3057\u3066\u4f5c\u6210\u3057\u3066\u3044\u305f\u3082\u306e\u304b\u3089 Docusaurus \u3092\u4f7f\u7528\u3057\u305f\u3082\u306e\u306b\u4ee5\u964d\u4e2d\u3067\u3059\u3002 \u8a18\u4e8b\u306e\u79fb\u690d\u306f\u968f\u6642\u5bfe\u5fdc\u4e2d\u3067\u3059\u3002</p>"},{"location":"Maya/Python/scripts/","title":"Scripts","text":""},{"location":"Maya/Python/scripts/#python-scripts","title":"Python Scripts","text":""},{"location":"Maya/Python/scripts/#create-animated-camera-from-multiple-cameras-in-maya-python","title":"Create animated camera from multiple cameras in Maya (Python)","text":"<p>Info</p> <p>This script creates a single animated camera from selected cameras. In order to set the keyframes to correct frames, the cameras need to have imagePlane node that has an image file containing the frame number. The image file needs to be in format \u201cimage.####.ext\u201d. You will also need to change the file_ext variable string to the file extension that the image sequence is using. This is so that the script can substring out the frame number correctly.</p> <p>Logic around the script:</p> <ul> <li> <p> List selected nodes</p> </li> <li> <p> create a new camera and store its shape and transform node names</p> </li> <li> <p> for each camera in selection</p> <ul> <li>read transform and camera attributes to variables</li> <li>set these attributes to the newly created camera</li> <li>read frame number from the image file in the image plane of the camera</li> <li>create a key at this frame to all the attributes</li> </ul> </li> </ul> <p></p><pre><code>import maya.cmds as cmds\n\nfile_ext = \".jpg\"\n\nselected = cmds.ls( sl=True )\n\nnew_cam = cmds.camera( n='anim_cam')\ncameraShape = new_cam[1]\ncameraXform = new_cam[0]\nprint(cameraShape)\nprint(cameraXform)\n\n\nfor node in selected:\n    pos = cmds.camera(node, q=True, p=True)\n    rot = cmds.camera(node, q=True, rot=True)\n    focal_len = cmds.camera(node, q=True, fl=True)\n    vfilm_apert = cmds.camera(node, q=True, vfa=True)\n    hfilm_apert = cmds.camera(node, q=True, hfa=True)\n   \n    relatives = cmds.listRelatives( node )\n    imagefile = cmds.getAttr ( relatives[0] + '.imageName' )\n    frame = imagefile[(-4-len(file_ext)):(len(file_ext)*-1)]\n    print(frame)\n   \n   \n    cmds.camera(cameraShape, e=True, p=pos )\n    cmds.camera(cameraShape, e=True, rot=rot )\n    cmds.camera(cameraShape, e=True, fl=focal_len )\n    cmds.camera(cameraShape, e=True, vfa=vfilm_apert )\n    cmds.camera(cameraShape, e=True, hfa=hfilm_apert )\n   \n    cmds.setKeyframe( cameraXform, t=frame )\n    cmds.setKeyframe( cameraShape, t=frame )\n</code></pre> www.thebrainextension.com"},{"location":"Maya/Python/scripts/#set-all-image-plane-images-to-a-new-path-in-maya","title":"Set all image plane images to a new path in Maya","text":"<p>Info</p> <p>You want to grade or resize image plane images and write them to a new location with Nuke. To set all the file paths on the scene to the new path run this script. The images need to be in an image sequence. Replace new_path variable string with the file path pointing to the first image of the new image sequence. Replace file_ext variable string with the file extension of the new image sequence.</p> <p>Logic around the script:</p> <ul> <li> <p> Get a list of all the image plane shape nodes in the scene with ls command with flag type=\u201cimagePlane\u201d</p> </li> <li> <p> for each node in the list</p> <ul> <li>get image name attribute to a variable</li> <li>substring out the frame</li> <li>construct a new file path from the prefix, frame, and file_ext variables</li> <li>set image plane image name attribute to the final_path</li> </ul> </li> </ul> <pre><code>import maya.cmds as cmds\n\nnew_path = r\"new_file_path\\test_imageplane_newpath.1001.jpg\"\nfile_ext = \".jpg\"\n\nprefix = new_path[0:(-4-len(file_ext))]\n\nnodes = cmds.ls( type='imagePlane' )\n\nfor node in nodes:\n    image_name = cmds.getAttr ( node + '.imageName' )\n    frame = image_name[(-4-len(file_ext)):(len(file_ext)*-1)]\n    final_path = prefix + frame + file_ext\n    cmds.setAttr( node + '.imageName', final_path, type='string')\n</code></pre> <p>www.thebrainextension.com</p>"},{"location":"Nuke/ACES_Nuke_Fundamentals/","title":"Color Management","text":""},{"location":"Nuke/ACES_Nuke_Fundamentals/#the-benefits-of-color-management","title":"The Benefits of Color Management","text":"<p>This video defines Color Management and explains why it is important to the Netflix production process. It illustrates the differences between a non-color managed workflow and a color managed one and how ACES (the Academy Color Encoding System) helps to standardize Color Management.</p>"},{"location":"Nuke/ACES_Nuke_Fundamentals/#color-management-fundamentals--aces-workflows-in-nuke","title":"Color Management Fundamentals &amp; ACES Workflows in Nuke","text":"<p>In this series, acclaimed filmmaker and VFX Supervisor, Victor Perez demystifies the science of color management and shows you how to work with ACES in Nuke.</p> <p>In this first video, Victor takes you from understanding how our brains interpret light frequencies as colors to the history of color reproduction in imaging technology and beyond.</p> <p>By the end, you\u2019ll have everything you need to know about color space and insight into why it\u2019s so important to achieve a consistent standard of color reproduction, quality and artistic intent across an entire production.</p>"},{"location":"colors/ACES_DCC/","title":"ACES Install","text":""},{"location":"colors/ACES_DCC/#ocio","title":"OCIO","text":""},{"location":"colors/ACES_DCC/#aces-maya","title":"ACES MAYA","text":"<p>The default settings for color management is ACES and the default rendering sapce is ACEScg.</p> <p>You setup color color management in the maya preferences, windows&gt; settings/preferences&gt; preferences.</p> <p></p> <p>Color management in maya is based on OpenColorIO (OICO). OCIO use a configuration file, usually called \"config.ocio\" in yaml format. this file defines the color spaces and transforms that are available. The same config file can be used by many creaction, compositing and editing software packages.</p> <p>Maya contain two config, the default config for new scenes is based on ACES, the academy color encoding system. The legacy config is used for compatibility in scenes from maya 2020 an earlier.</p> <p>If your scene uses a specific cconfig file, you cans browse and select the instead.</p>"},{"location":"colors/ACES_DCC/#aces-houdini","title":"ACES HOUDINI","text":"<p>Computer programs like Houdini prefer to work in a linear color space internally, which is easy to manipulate digitally. It\u2019s often preferable for outputs, such as final images, printed film, and displays, to be in perceptual color space (gamma corrected/color corrected). Color management in Houdini involves translating between linear and perceptual color spaces at the boundary between Houdini and the outside world at appropriate times.</p> <p>OpenColorIO (OCIO) is an industry-standard open-source library for managing and translating color spaces.</p> <p>OCIO is more powerful and flexible than Houdini\u2019s default gamma and lookup table (LUT) support, and is recommended for professional use.</p>"},{"location":"colors/ACES_DCC/#setup","title":"Setup","text":"<ul> <li> <p> Set the $OCIO environment variable to the file path of an OpenColorIO configuration file (such as config.ocio).      The existence of this environment variable controls whether Houdini automatically uses OCIO in various places.</p> </li> <li> <p> You can set the default colorspace using the OCIO_ACTIVE_DISPLAYS and OCIO_ACTIVE_VIEWS environment variables.</p> </li> <li> <p> For sRGB files and sources, the source colorspace used for sRGB is defined by the Houdini environment variable          HOUDINI_OCIO_SRGB_FILE_COLORSPACE. If this variable is not defined, the OpenColorIO config file is searched for a colorspace      that matches srgb, either fully or partially.</p> </li> <li> <p> You can turn various forms of OCIO support in the Edit \u25b8 Color Settings window.</p> </li> </ul> <p>Note</p> <p>The OCIO, OCIO_ACTIVE_VIEWS, and OCIO_ACTIVE_DISPLAYS environment variables are not Houdini environment variables. They cannot be     set through the Aliases and Variables dialog, nor can they be set in the houdini.env file. They must be set in the shell or     desktop environment before launching Houdini.</p>"},{"location":"colors/ACES_DCC/#image-inputs","title":"Image inputs","text":"<ul> <li> <p> Texture images are assumed to be linear in Houdini and Mantra. If needed, you can manually convert textures using the OCIO Color Transform VOP.</p> </li> <li> <p> If OCIO is configured and you set the File COP to linearize colors, it will use the OCIO file naming convention to deduce the color space of the input image and linearize it correctly. (This will also automatically switch the output to 16 bit to prevent banding.)</p> </li> </ul>"},{"location":"colors/ACES_DCC/#render-output","title":"Render output","text":"<ul> <li> <p> Renders from Mantra are in linear space.</p> </li> <li> <p> Flipbooks from the viewport are either sRGB or linear, depending on the \u201cRender Beauty Pass Only\u201d option (linear when On, sRGB when off).</p> </li> <li> <p> The image in the Render View can be color corrected using OCIO.</p> </li> <li> <p> The Correction toolbar is stowed by default at the bottom of the view. Click the stowbar just below the view area to show it. When OpenColorIO is active, Houdini replaces default gamma and LUT controls on the Correction toolbar with OCIO controls.</p> </li> <li> <p> The output of the Render Region tool can be color corrected using OCIO. (You can turn this on or off in the Edit \u25b8 Color Settings window.)</p> </li> </ul>"},{"location":"colors/ACES_DCC/#color-correcting-the-display","title":"Color correcting the display","text":"<ul> <li> <p> When OpenColorIO is active, the Scene View, Render View, and MPlay can use OCIO to color-correct display.</p> </li> <li> <p> When OCIO is active, Houdini replaces default gamma and LUT controls on the Correction toolbar with OCIO controls.</p> </li> </ul> <p></p> <ul> <li> <p>In the Scene View, open the Viewport menu (the menu to the left of the Camera menu, in the top left corner of the viewport) and turn on Correction Toolbar to show the color correction toolbar at the bottom of the viewer.</p> </li> <li> <p>In the Render View, the Correction toolbar is stowed by default at the bottom of the view. Click the stowbar just below the view area to show it.</p> </li> </ul> <ul> <li> <p> When OpenColorIO is active ($OCIO points to a configuration file), gamma and LUT controls in the interface are replaced by Display and View menus. These define the output colorspace for the display. The colorspace within Houdini is linear, and the scene_linear role specifies the OpenColorIO linear colorspace.</p> </li> <li> <p> For example, when OCIO is active, the scene viewer\u2019s Color Correction toolbar (Viewport menu \u25b8 Correction Toolbar) has OCIO controls.</p> </li> <li> <p> MPlay can load both linear and non-linear files, such as OpenEXR (linear) and JPEG (sRGB).</p> </li> </ul> <p>.........</p>"},{"location":"colors/ACES_DCC/#aces-3dsmax","title":"ACES 3DSMAX","text":"<p>Unfortunatly 3dsmax doesn't support OCIO</p>"},{"location":"colors/Aces/","title":"ACES intro","text":""},{"location":"colors/Aces/#aces-introduction","title":"ACES introduction","text":"<p>For ACES Technical documentation visit acescentral.</p> <p></p>"},{"location":"colors/Aces/#what-is-aces-and-why-is-it-recommended-octicons-heart-fill-16-heart-throb","title":"What is ACES and Why is it Recommended? :octicons-heart-fill-16:{: .heart-throb}","text":"<p>The Academy Color Encoding System (ACES) - is a free, expandable, device-independent color management and image sharing system developed under the auspices of the Academy of Motion Picture Arts and Sciences. </p> <p>It is also a set of technical specifications for working with color, coding, and transformation.</p> <p>This system allows you to store all the data of digital images in the same mathematical space, which allows a more consistent workflow during the transfer of working material between different departments. </p> <p>It is also important that ACES allows you to create archival materials with a high dynamic range and wide color gamut, even taking into account possible future devices.</p> <p>Info</p> <p>OpenColorIO Configuration for ACES </p> <p>Github repo OpenColorIO-Config-ACES.</p>"},{"location":"colors/Aces_cheatSheet/","title":"ACES Notes","text":""},{"location":"colors/Aces_cheatSheet/#notes","title":"NOTES","text":""},{"location":"colors/Aces_cheatSheet/#acescg-conversion-cheat-sheet","title":"ACEScg Conversion Cheat Sheet","text":"<p>Note</p> <p>Textures for 3D materials:</p> <ul> <li> <p> You need to preserve the look of the colors?: Utility - sRGB - Texture</p> <ul> <li>Diffuse</li> <li>SSS</li> <li>Albedo</li> <li>Spec</li> <li>Metallicity</li> </ul> </li> <li> <p> You need to preserve the numeric values of you pixels?: Utility - Raw</p> <ul> <li>Bump</li> <li>Normal</li> <li>Displacement</li> </ul> </li> <li> <p> You need your image to look as though you opened it up in Photoshop?: Output - sRGB</p> <ul> <li>Backplates or Compositing</li> </ul> </li> </ul>"},{"location":"colors/Aces_cheatSheet/#abbreviations","title":"Abbreviations","text":"<p>Note</p> <ul> <li> <p>ACES - Academy Color Encoding System</p> </li> <li> <p>AMPAS - Academy of Motion Picture Arts and Science</p> </li> <li> <p>OCIO - Open Color IO</p> </li> <li> <p>OIIO - Open Image IO</p> </li> <li> <p>LUT - Look Up Table</p> </li> <li> <p>ICC - International Color Consortium</p> </li> <li> <p>ICM - Image Color Management</p> </li> <li> <p>IDT - Input Device Transform</p> </li> <li> <p>ODT - Output Device Transform</p> </li> <li> <p>LMT - Look Modification Transform</p> </li> <li> <p>RRT - Reference Rendering Transform</p> </li> <li> <p>UI - Unsigned Integer</p> </li> <li> <p>OCES - Output Color Encoding Specification</p> </li> </ul>"},{"location":"colors/Aces_cheatSheet/#ciexy-1931-color-diagram","title":"CIExy 1931 Color Diagram","text":"<p>This is a very common diagram depicting the extent of colors humans can perceive. It is important to note that the diagram does not depict value/intensity only hue and saturation. To explain gamut, primaries, and white points I will be using a CIExy diagram to visualize color-spaces so we can easily see the size of a colorspace and how much of the visual light spectrum can be represented by these colorspaces.</p>"},{"location":"colors/Aces_cheatSheet/#gamut","title":"gamut","text":"<p>A colorspace's gamut is the set of all the colors that can be represented within that colorspace. In the diagram below the gamut is the set of all the colors within the colorspace triangle. Many popular gamuts have been graphed to help illustrate the wide range to pick from. Notice how few colors can actually be represented by sRGB.</p>"},{"location":"colors/Aces_cheatSheet/#primaries","title":"primaries","text":"<p>For RGB images primaries are your reddest reds, greenest greens, and bluest blues of a color gamut. In the diagrams below you can see these are represented by the corners of the gamut triangles. Also note the position of the green primary of the ACEScg colorspace. It falls outside of the visible spectrum of hues (called an imaginary color). It was placed there so as many hues between rend and green could fall within the ACEScg gamut. One consequence of the primary being where it is is you will never use a fully green value by itself.</p>"},{"location":"colors/Aces_cheatSheet/#white-point","title":"white point","text":"<p>The white point is the point within a colorspace we consider to be white. ACEScg and sRGB have different white points. Below the Kelvin temperature scale (and P3) is graphed over all human visible hues (CIE 1931 Chromaticity Diagram). Often you will see a white point of a colorspace refered to by its CIE Standard Luminant designation. These usually start with the letter \"D\" and two numbers. For example ACEScg and sRGB have D60 and D65 white points. These closely translate to a value on the Kelvin scale: D65 is 6500 Kelvin and D60 is 6000 Kelvin.</p>"},{"location":"colors/Aces_cheatSheet/#gamma","title":"gamma","text":"<p>When dealing with gamma you are going to come across the terms linear and gamma curves quite often. ACES is linear but marjority of the time the images we take with our camera, make in photoshop, and download off the internet are not. So, we need to understand what gamma is and how to make a non-linear image linear. Back when computers were slow, drive space was expensive, and memory was small we needed to store our images in as efficient a manner as possible. First of all we could only store our images using 8 bits per color channel which meant each channel only had 256 individual values it could store. It turns out our eyes are more sensitive to small increases of value in darker colors than in lighter colors. We could store our images using a gamma curve to dedicate more of the 256 values in each channell to the darker colors rather than the lighter ones. This meant as a pixel value increased that value would increase by greater and greater amounts. So, to put it another way, going from 0 to 1 would result in a smaller increase in value than an increase from 254 to 255. In a linear image the value increases are uniform. 3D applications prefer a linear image as it makes the math for calculating color and light a lot easier and it makes using high dynamic range images possible. Understanding if your non-ACES images is linear or has a gamma curve is really important to converting it properly.</p> <p>Notice for the non-linear curves the values flip after 1. For an images with a dark gamma values begin to brighten after 1 and the oposite for a bright gamma. This is why we need to work with linear images when dealing with values above 1 (HDR iamges).</p> <p>When doing research on ACES on your own you will often come across the terms scene-referred and display-referred and it is important to understand what they mean and how they relate to your images.</p>"},{"location":"colors/Aces_cheatSheet/#scene-referred","title":"scene-referred","text":"<p>Scene referred images are linear and are meant to represent real-world light values or light as it actually is. However, they look terrible when displayed raw on a monitor because they don't take into account the characteristics of the display (dynamic range, gamma, etc.). ACES and ACEScg are both Scene-Referred. Scene-referred images have a linear gamma curve.</p>"},{"location":"colors/Aces_cheatSheet/#display-referred","title":"display-referred","text":"<p>Display referred images are encoded in a way to make them look good when displayed or has the data encoded in a way affords efficient storage. sRGB, P3, Rec. 709, and Adobe RGB (1998) are all Display-Referred images. Display-Referred images are encoded to be looked at on a specific device (sRGB monitor, Rec. 709 TV, P3 movie screen) or come from a specific camera colorspace (RED DRAGONcolor, ARRI LogC, Sony S-Log, etc.). Display-referred images have a non-linear gamma curve.</p>"},{"location":"colors/Aces_workflow/","title":"ACES Pipeline","text":""},{"location":"colors/Aces_workflow/#aces-color-pipeline","title":"ACES Color Pipeline","text":"<pre><code>flowchart LR\n  A(Scene Tristimuli) --&gt; B[OECF]\n  B --&gt; C[IDT] \n  C --&gt; D[LMT]\n  D --&gt; E[RRT]\n  E --&gt; F[ODT]\n  F --&gt; G[EOCF]\n  G --&gt; H(Display Tristimuli)\n  C --&gt;|ACES| E\n style A stroke-width:2px,stroke-dasharray: 5 5\n style H stroke-width:2px,stroke-dasharray: 5 5\n</code></pre> <ul> <li> <p> Tristimulis refers to linear light, code value (from 0 to ) are linearly related to the power in the light  that is entering in the camera lens. those numbers are 12bits and typically we don't have the downstream infrastruture  that will handle 12 bits well established.</p> </li> <li> <p> OECF Tippically the first step in the color pipeline is an OECF (opto electronic conversion function), in the old days we will would've call this GAMMA.Built in the camera, </p> </li> <li> <p> IDT (input device transform) ingesting data map from one linear light color space to another. 3*3 matrix transform </p> </li> </ul> <p>Note</p> <p>IDT:</p> <ul> <li>The Input Device Transform is the transform used to convert the pixel colors of images/videos from specific devices into an ACES      colorspace.</li> </ul> <p>RRT:</p> <ul> <li>The Reference Rendering Transform prepares scene referred linear data into high dynamic range display referred data. This data is     then meant to be handed over to an ODT to convert the data to be viewed in a specific display type.</li> </ul> <p>ODT:</p> <ul> <li>The Output Display Transform is responsible for converting the data created by the RRT to data that can be viewed on specific     devices or color-spaces: sRGB, P3, Rec. 709.</li> </ul> <p>LMT RRT ODT ROCF Display Tristimuli</p>"},{"location":"colors/CIE/","title":"CIE XYZ","text":""},{"location":"colors/CIE/#colorspace","title":"ColorSpace","text":"<p>Un espace de couleur est un mod\u00e8le math\u00e9matique utilis\u00e9 pour repr\u00e9senter les couleurs de mani\u00e8re standardis\u00e9e. Il utilise g\u00e9n\u00e9ralement plusieurs dimensions pour d\u00e9crire les couleurs, telles que la luminance, la teinte et la saturation.</p> <p>Les espaces de couleur sont utilis\u00e9s dans de nombreuses applications, comme l'impression, la photographie, les arts graphiques et les sciences de la couleur. Ils permettent de mesurer et de comparer les couleurs de mani\u00e8re objective, ce qui est utile pour garantir la fid\u00e9lit\u00e9 des couleurs dans les travaux professionnels.</p> <p>Il existe diff\u00e9rents espaces de couleur, chacun ayant des caract\u00e9ristiques et des utilisations diff\u00e9rentes. L'espace de couleur CIE XYZ est un espace de couleur absolu qui ne tient pas compte des caract\u00e9ristiques sp\u00e9cifiques de l'\u00e9cran ou de l'\u00e9clairage utilis\u00e9s pour afficher les couleurs. L'espace de couleur RGB est un espace de couleur adapt\u00e9 \u00e0 l'affichage sur \u00e9cran, tandis que l'espace de couleur CMYK est utilis\u00e9 pour les impressions.</p>"},{"location":"colors/StandardObserver/","title":"Standard Observer","text":""},{"location":"colors/StandardObserver/#observateur-standard-cie-1931-","title":"Observateur Standard CIE 1931 :","text":"<ol> <li>Introduction :</li> <li>Propos\u00e9 par la Commission Internationale de l'\u00c9clairage (CIE) en 1931.</li> <li> <p>Bas\u00e9 sur des exp\u00e9riences men\u00e9es par Wright et Guild.</p> </li> <li> <p>Composants :</p> </li> <li>Comprend trois fonctions de correspondance des couleurs : <code>X(\u03bb)</code>, <code>Y(\u03bb)</code> et <code>Z(\u03bb)</code>.</li> <li> <p>Ces fonctions d\u00e9crivent la sensibilit\u00e9 de l'\u0153il humain moyen \u00e0 la lumi\u00e8re de diff\u00e9rentes longueurs d'onde.</p> </li> <li> <p>Objectif :</p> </li> <li>Fournit une repr\u00e9sentation math\u00e9matique des propri\u00e9t\u00e9s de correspondance des couleurs de l'\u0153il humain.</li> <li> <p>Forme la base de l'espace colorim\u00e9trique CIE XYZ.</p> </li> <li> <p>Repr\u00e9sentation :</p> </li> <li>Les fonctions de correspondance des couleurs <code>X(\u03bb)</code>, <code>Y(\u03bb)</code> et <code>Z(\u03bb)</code> sont normalis\u00e9es de mani\u00e8re \u00e0 ce que la fonction <code>Y</code> repr\u00e9sente la luminance.</li> </ol>"},{"location":"colors/StandardObserver/#observateur-standard-cie-1964-","title":"Observateur Standard CIE 1964 :","text":"<ol> <li>Introduction :</li> <li>Propos\u00e9 comme mise \u00e0 jour de l'Observateur Standard CIE 1931 en 1964.</li> <li> <p>Bas\u00e9 sur des donn\u00e9es exp\u00e9rimentales suppl\u00e9mentaires.</p> </li> <li> <p>Composants :</p> </li> <li>Similaire \u00e0 l'Observateur Standard CIE 1931, il comprend trois fonctions de correspondance des couleurs : <code>X(\u03bb)</code>, <code>Y(\u03bb)</code> et <code>Z(\u03bb)</code>.</li> <li> <p>Il fournit une repr\u00e9sentation plus pr\u00e9cise de la vision des couleurs \u00e0 diff\u00e9rentes longueurs d'onde.</p> </li> <li> <p>Am\u00e9liorations :</p> </li> <li>Tient compte du fait que la vision humaine n'est pas \u00e9galement sensible \u00e0 toutes les longueurs d'onde.</li> <li>Int\u00e8gre des ajustements aux fonctions de correspondance des couleurs pour mieux correspondre aux donn\u00e9es exp\u00e9rimentales.</li> </ol>"},{"location":"colors/StandardObserver/#r\u00f4le-en-science-des-couleurs-","title":"R\u00f4le en Science des Couleurs :","text":"<ol> <li>Espaces Colorim\u00e9triques :</li> <li>L'espace colorim\u00e9trique CIE XYZ est d\u00e9riv\u00e9 des fonctions de correspondance des couleurs de l'observateur standard.</li> <li> <p>D'autres espaces colorim\u00e9triques, tels que RGB et LAB, sont souvent d\u00e9riv\u00e9s ou d\u00e9finis en fonction de l'espace colorim\u00e9trique XYZ.</p> </li> <li> <p>Correspondance des Couleurs :</p> </li> <li> <p>Les observateurs standard sont utilis\u00e9s dans des exp\u00e9riences de correspondance des couleurs pour d\u00e9terminer la couleur per\u00e7ue des sources lumineuses dans des conditions standardis\u00e9es.</p> </li> <li> <p>Colorim\u00e9trie :</p> </li> <li> <p>Forme la base de la colorim\u00e9trie, qui est la science de la mesure et de la quantification des couleurs.</p> </li> <li> <p>Rendu des Couleurs :</p> </li> <li>Les observateurs standard jouent un r\u00f4le crucial dans le rendu des couleurs en imagerie num\u00e9rique, en infographie et dans d'autres domaines o\u00f9 une reproduction pr\u00e9cise des couleurs est importante.</li> </ol> <p>En r\u00e9sum\u00e9, l'observateur standard fournit un mod\u00e8le standardis\u00e9 de la fa\u00e7on dont l'\u00eatre humain moyen per\u00e7oit les couleurs. C'est un concept cl\u00e9 en science des couleurs et est utilis\u00e9 dans diverses applications pour garantir la coh\u00e9rence et la pr\u00e9cision dans la repr\u00e9sentation et la reproduction des couleurs.</p>"},{"location":"colors/introToLight/","title":"Light","text":""},{"location":"colors/introToLight/#introduction-to-light-color-and-color-space","title":"Introduction to Light, Color and Color Space","text":""},{"location":"colors/introToLight/#introduction","title":"Introduction","text":"<p>Simple en apparence et assez courante, la notion de couleur est en r\u00e9alit\u00e9 une question complexe. Ce n'est pas seulement quelque chose qui peut \u00eatre d\u00e9crit scientifiquement, auquel cas nous ne pourrions avoir qu'une point de vue objectif et rationnel sur la question.</p> <p>Les couleurs sont \u00e9galement le r\u00e9sultat d'un processus qui implique la vision, l'un des syst\u00e8mes sensoriels \u00e0 travers lesquels nous percevons et interagissons avec le monde qui nous entoure. </p> <p>En tant que tel, c'est aussi une question tr\u00e8s subjective avec une composante psychologique (la signification des couleurs) et physiologique (comment notre cerveau traite-t-il les couleurs) (avez-vous d\u00e9j\u00e0 v\u00e9cu une dispute avec une autre personne \u00e0 propos de la couleur d'un objet ?).</p> <p>Vous avez aussi probablement tous \u00e9t\u00e9 tromp\u00e9s par des illusions d'optique bien connues qui sont un autre exemple de l'influence de l'esprit sur la fa\u00e7on dont nous percevons les formes et les couleurs.</p> <p>Nous n'entrerons pas dans les d\u00e9tails et nous nous en tiendrons \u00e0 la fa\u00e7on dont nous pouvons repr\u00e9senter, stocker et afficher les couleurs dans le monde des ordinateurs. Cependant, c'est juste pour dire que le sujet est beaucoup plus complexe qu'il n'y para\u00eet \u00e0 premi\u00e8re vue.</p> <p>L'\u00e9tude de la couleur est g\u00e9n\u00e9ralement appel\u00e9e science des couleurs, qui comprend tous les \u00e9l\u00e9ments que nous avons mentionn\u00e9s ci-dessus : </p> <ul> <li>Comment le cerveau traite-t-il les stimuli visuels en ce que nous percevons comme des couleurs.</li> <li>L'utilisation des couleurs d'un point de vue artistique et l'\u00e9tude de ondes \u00e9lectromagn\u00e9tiques responsables de la lumi\u00e8re telle qu'elle existe dans le monde physique.</li> <li>Traiter les couleurs dans le domaine des \u00e9crans num\u00e9riques, qui est aussi une science \u00e0 part enti\u00e8re (ce que l'on appelle parfois la gestion des couleurs).</li> </ul>"},{"location":"colors/introToLight/#light","title":"Light","text":"<p>Tout commence par la lumi\u00e8re. Avant de pouvoir nous pencher sur les couleurs, nous devons d'abord comprendre la lumi\u00e8re et comment la lumi\u00e8re interagit avec la mati\u00e8re.</p> <p>La lumi\u00e8re voyage dans l'espace sous forme d'ondes \u00e9lectromagn\u00e9tiques, mais peut \u00e9galement \u00eatre d\u00e9crite comme un flux de particules qu'Einstein appelait photons (c'est la raison pour laquelle nous disons que la lumi\u00e8re a une double nature onde-particule). Dans cette le\u00e7on cependant, nous ne consid\u00e9rerons la lumi\u00e8re que sous sa forme ondulatoire.</p> <p>Une onde p\u00e9riodique, comme nous le savons, est d\u00e9finie par sa fr\u00e9quence (le nombre de fois qu'un cycle est r\u00e9p\u00e9t\u00e9 par unit\u00e9 de temps) ou sa longueur d'onde (qui est l'inverse de la fr\u00e9quence) qui est la distance sur laquelle la forme de l'onde se r\u00e9p\u00e8te.</p> <p>La couleur de la lumi\u00e8re peut \u00eatre consid\u00e9r\u00e9e comme l'\u00e9quivalent du concept de hauteur du son. Les deux sont bas\u00e9s sur la longueur d'onde ou la fr\u00e9quence du signal voyageant dans l'espace (consultez la le\u00e7on sur l'interaction lumi\u00e8re-mati\u00e8re pour en savoir plus sur la longueur d'onde et la fr\u00e9quence de la lumi\u00e8re).</p> <p></p> Figure 1:a) the length a complete cycle of the wave is called the wavelength of the periodic function.         The wavelength of visible light varies from 380 nm to 740 nm.      b) white light passing through a prism is decomposed into a rainbow of colors.  <p>A wavelength is denoted with the greek letter \u03bb (lambda). Visible light is made of waves which frequency varies from 380 to about 740 nanometres (a nanometre is 1\u00d710\u22129 meter). Any waves which wavelength is below 380 nm or above 740 nm can not be perceived by the human eye. The following image shows the full spectrum of colors the visible light spectrum is made of (each color you see has a wavelength within the range of about 380 nm to 780 nm)</p> <p></p> <p>Most people are also familiar with the Newton experiment which consists of using a prism to decompose white light into a rainbow of colors (figure 1b).</p> <p>This experiment shows that white light is made of all the visible colors from the visible light spectrum, mixed in some proportions.  The prism experiment can also be carried out the other way around. If we take all the light colors from the visible light spectrum and add them up in the same proportions, then we can recreate white light (figure 2). </p> <p>White light as such doesn't exist. White light is the result of a light source, the sun or the screen of your computer, producing a mixture of light colors from the visible spectrum. </p> <p>If you examine your computer screen or television with a magnifying glass, you will see tiny dots, probably red, green and blue and by mixing these colors in different amounts, a large range of colors can be produced.</p> <p></p> Figure 2:a wooden mannequin lit by a green, red and blue light. The three colors are mixed on the background which appears     white.  <p>Info</p> <p>Pointillism is a painting technique by which sensations of colors can be obtained by putting small dots of pure colors side         by side in organised patterns rather than using the more traditional way of mixing colors on the palette.  From the distance, the colored dots blend into one single color. Screens work in very similar fashion. Each pixel from the      scene is actually made of three small components which emit red, green and blue light. </p> <p>By changing the amount of red, green and blue light emitted, we can create all the colors we need. From the distance these      three separable elements are indistinguishable and the contribution of each light blend within each other to form one single        light color. The next chapter will provide more information on the way screens actually work. </p>"},{"location":"compositing/tracking/","title":"Tracking","text":"<p>lift -&gt; shadow, gamma -&gt; midtones, Gain -&gt; highlight </p> <p>lift == multiply</p> <p>3 type of tracking</p> <p>2D track -&gt; Point track</p> <p>2.5D tracks -&gt; Planar Tracks</p> <p>3D track -&gt; Camera track</p> <p>2d XY motion Color correction </p> <p>any parallax where i try to track myu element ? ANY ROTATION ,      </p> <p>2.5 Track planar surfaces, anythin that can be put on a flat grid </p> <p>3D track</p> <p>Shot that introduce parallax and dimentionality that need to be  reflected in the element you adding</p> <p>Parallax:  The relative speeds two separate objects move based on their distance from the camera.  Creates the feeling of depth.</p> <p>Dimensionality: The relative speeds differents parts of a single object move  on their distance from the Camera. Creates the feeling thaht an object is 3 dimensiontal and not a flat plane.</p>"},{"location":"pipeline/OCIO/","title":"OCIO","text":""},{"location":"pipeline/OCIO/#ocio","title":"OCIO","text":""},{"location":"pipeline/OCIO/#opencolorio","title":"OpenColorIO","text":"<p>OpenColorIO (OCIO) was initially developed (since 2003), open-sourced by Sony Pictures Imageworks and \u275d is an Academy Scientific and Technical Award winning color management solution for creating and displaying consistent images across multiple content creation applications during visual effects and animation production \u275e - It became the primary color management framework solution many software vendor started to support.</p> <p>The Academy Software Foundation (ASWF) was founded in 2018 to help foster and shepherd the development of open source software projects in the visual effects industry [ASWF 2018]. OpenColorIO was the second project accepted into the foundation [Olin 2019] (source). OCIO is open-source, free and is licensed under the BSD-3-Clause license. The Technical Steering Committee (\u201cTSC\u201d) is responsible for all technical oversight of the open source project. Last but not least, there are significant contributions that have also been made by Industrial Light &amp; Magic, DNEG, and many individuals (contributors list). Version 2.0 is the second major version of OCIO, led by full-time Autodesk software engineers. </p> <p>Note</p> <p>Open Color IO (OCIO) is a color management framework: \u201cOCIO enables color transforms and image display to be handled in a consistent manner across multiple graphics applications.     Unlike other color management solutions, OCIO is geared towards motion-picture post production, with an emphasis on visual     effects and animation color pipelines. OpenColorIO has been used since 2003 to address the challenges of working with multiple     commercial image-processing applications that have different approaches to color management\u201d - Sony Imageworks</p>"},{"location":"pipeline/OCIO/#ocio-configuration-files","title":"OCIO Configuration Files","text":"<p>The configuration file \u201ccontrols\u201d OCIO (its file package), is usually named config.ocio and is a YAML document that can be opened in most text or code (e.g VSC) editors.</p>"},{"location":"pipeline/OCIO/#ocio-configuration-files_1","title":"OCIO Configuration Files","text":""},{"location":"pipeline/OCIO/#loading-ocio-package","title":"Loading OCIO Package","text":"<p>Environment Variable The operating-system environment-variables (shortened as env-var) allows all software (supporting OCIO) to automatically load the same OCIO configuration file (shortened to config-file) by setting a file path.</p> <p>https://www.elsksa.me/scientia/cgi-offline-rendering/ocio</p>"},{"location":"pipeline/OCIO_DT_misconceptions/","title":"OCIO Misconception","text":""},{"location":"pipeline/OCIO_DT_misconceptions/#introduction","title":"Introduction","text":"<p>I often think about this great question asked by Doug Walker at one of our OpenColorIO (OCIO) meetings : what are you trying to solve ? So in our case, what is this post about ?</p> <p>In the past year (2021), I have given several talks online about OCIO and the Academy Color Encoding System(ACES). I thought it would be useful to put those slides online for two reasons :</p> <ul> <li> <p> If there is any inaccurracy, anyone can correct me and I\u2019ll be more than happy to update this page.</p> </li> <li> <p> With these slides online, I hope I can reach more people and raise awareness about certain topics.</p></li></ul> <p>I do not consider myself an expert. I do not believe in this word anymore. We are only human beings, doing our best to understand things. We make mistakes, hopefully learn from them and keep going. So no, I\u2019m not an expert on anything !</p> <p>The only thing I can offer is an artist\u2019s point of view on Color Management. Quite often, I have been lost on where to begin when it comes to OCIO and ACES.  So by writing this post, I hope I can share with you  some useful information.</p> <p>There is a reason my book starts with two chapters about Color Management. It will give you the fundation to create beautiful images. So I thought it would be interesting to compare various OCIO configs, with a series of visual examples in order to study their strength and flaws.</p> <p>Really the idea is to look back but NOT judge these historical OCIO configs. Not at all ! Huge respect to all the persons who more than ten years ago developed and shared these configs with the community.</p>"},{"location":"pipeline/OCIO_DT_misconceptions/#a-great-community","title":"A great community","text":"<p>Some people may find this post too obvious, boring or outdated. Probably ! But I was really interested to go back in time and write this historical analysis on OCIO and Display Transforms. This is post is primarly about my own misconceptions, but hopefully it will help others too !</p> <p>Please note that I will only talk about OCIOv1 configs here.</p> <p>First of all, I would like to thank the Academy of Motion Picture Arts and Sciences (AMPAS) because without them, I would have probably never discovered this crazy wonderful world of Color Management. ACES has been my entry point to Color Management and I will never be thankful enough for that.</p> <p>Thanks to them, I have discovered this amazing community of colour nerds and colour scientists, and I would like to personnally thank the following persons for their generosity, sharing and patience with my questions :</p>"},{"location":"pipeline/Pipeline_usd/","title":"Pipeline usd","text":"<p>:::caution</p> <p>\u3053\u306e\u30b5\u30a4\u30c8\u306f\u3001mkdocs \u3092\u4f7f\u7528\u3057\u3066\u4f5c\u6210\u3057\u3066\u3044\u305f\u3082\u306e\u304b\u3089 Docusaurus \u3092\u4f7f\u7528\u3057\u305f\u3082\u306e\u306b\u4ee5\u964d\u4e2d\u3067\u3059\u3002 \u8a18\u4e8b\u306e\u79fb\u690d\u306f\u968f\u6642\u5bfe\u5fdc\u4e2d\u3067\u3059\u3002</p>"},{"location":"pipeline/USD/00_install_USD/","title":"USD Install","text":""},{"location":"pipeline/USD/00_install_USD/#essayez-dutiliser-usd","title":"Essayez d'utiliser USD","text":"<p>SIGGRAPH2019 a constat\u00e9 que l'USD est tr\u00e8s chaud, donc Je vais t\u00e9l\u00e9charger un article de synth\u00e8se tout en v\u00e9rifiant diverses choses.</p> <p>Tout d'abord, nous allons t\u00e9l\u00e9charger et configurer USD.</p>"},{"location":"pipeline/USD/00_install_USD/#construire","title":"Construire","text":"<p>Construisez d'abord l'USD. Auparavant, il fallait beaucoup de travail pour cr\u00e9er et utiliser USD, mais maintenant, cr\u00e9ez des scripts Il est pr\u00e9par\u00e9 et peut \u00eatre construit relativement facilement.</p>"},{"location":"pipeline/USD/00_install_USD/#installez-ce-dont-vous-avez-besoin","title":"Installez ce dont vous avez besoin","text":"<ul> <li>https://git-scm.com/</li> <li>https://visualstudio.microsoft.com/ja/downloads/</li> <li>https://www.python.org/downloads/release/python-3712/</li> </ul> <p>Tout d'abord, t\u00e9l\u00e9chargez et installez VisualStudio n\u00e9cessaire \u00e0 la construction, Git pour obtenir le code source et Python.</p> <p>Une fois Python install\u00e9 </p> <p>Ajoutez Python37 directement en dessous et Scripts \u00e0 la variable d'environnement PATH. De plus, installez PySide2 et PyOpenGL, qui sont requis lors de l'utilisation d'USDView.</p> <pre><code>pip installer PyOpenGL PyOpenGL_accelerate PySide2\n</code></pre> <p>Les pr\u00e9paratifs sont maintenant termin\u00e9s.</p>"},{"location":"pipeline/USD/00_install_USD/#cloner-le-d\u00e9p\u00f4t","title":"Cloner le d\u00e9p\u00f4t","text":"<p>Une fois install\u00e9, clonez le r\u00e9f\u00e9rentiel \u00e0 partir du Github d'USD.</p> <p>\u00c0 l'invite de commande, acc\u00e9dez au r\u00e9pertoire dans lequel vous avez t\u00e9l\u00e9charg\u00e9</p> <pre><code>git clone https://github.com/PixarAnimationStudios/USD.git\n</code></pre> <p>Cloner</p> <p>Une fois clon\u00e9, ouvrez l'invite de commande du d\u00e9veloppeur de VisualStudio.</p> <p></p> <p>Une fois ouvert, acc\u00e9dez au dossier dans lequel vous avez clon\u00e9 le r\u00e9f\u00e9rentiel et ex\u00e9cutez la construction.</p> <pre><code>python build_scripts\\build_usd.py &lt;destination de sortie des artefacts de construction&gt;\n</code></pre>"},{"location":"pipeline/USD/00_install_USD/#\u00e0-travers-le-chemin","title":"\u00e0 travers le chemin","text":"<p>Une fois le t\u00e9l\u00e9chargement termin\u00e9, placez-le dans le PATH requis. tu as besoin de deux</p> nom de la variable CHEMIN PYTHONPATH /lib/python PATH /bin  /lib <p>Info</p> <p>Si votre PATH n'est pas sous lib, Notez qu'une erreur se produira lors de l'importation d'un fichier pyd.</p> <p>Passez par ces deux et vous \u00eates pr\u00eat \u00e0 partir</p>"},{"location":"pipeline/USD/00_install_USD/#essayez-douvrir-des-exemples-de-donn\u00e9es","title":"essayez d'ouvrir des exemples de donn\u00e9es","text":"<p>Lorsque vous \u00eates pr\u00eat, t\u00e9l\u00e9chargez l'exemple d'USD et ouvrez-le dans votre visionneuse.</p> <p>http://graphics.pixar.com/usd/downloads.html</p> <p>Des exemples de donn\u00e9es sont disponibles sur le site officiel de PIXAR, alors t\u00e9l\u00e9chargez ce KitchenSet.</p> <p>Apr\u00e8s le t\u00e9l\u00e9chargement, d\u00e9compressez et ouvrez l'invite de commande.</p> <pre><code>usdview Kitchen_set.usd\n</code></pre> <p>Ouvrez Kitchen_set.usd avec usdview.</p> <p></p> <p>Utilisez USDView pour voir les graphiques de sc\u00e8ne, les couches, les propri\u00e9t\u00e9s, etc. des fichiers USD bo\u00eete. Il est \u00e9galement livr\u00e9 avec une console Python, donc Il est facile de comprendre (semble) d'utiliser cet usdview pour tester diverses choses.</p>"},{"location":"pipeline/USD/00_install_USD/#cr\u00e9er-un-fichier-usd-\u00e0-partir-de-python","title":"Cr\u00e9er un fichier USD \u00e0 partir de Python","text":"<p>Lorsque vous \u00eates pr\u00eat, essayez d'ex\u00e9cuter le didacticiel officiel. https://graphics.pixar.com/usd/docs/Hello-World---Creating-Your-First-USD-Stage.html</p> <pre><code>depuis l'importation pxr Usd, UsdGeom\nstage = Usd.Stage.CreateNew('HelloWorld.usda')\nxformPrim = UsdGeom.Xform.Define(\u00e9tape, '/hello')\nspherePrim = UsdGeom.Sphere.Define(stage, '/hello/world')\n\u00e9tape.GetRootLayer().Save()\n</code></pre> <p>Une fois ex\u00e9cut\u00e9, un fichier HelloWorld.usda sera g\u00e9n\u00e9r\u00e9 dans le dossier sp\u00e9cifi\u00e9.</p> <pre><code>#usda 1.0\n\ndef Xform \"bonjour\"\n{\n    def Sph\u00e8re \"monde\"\n    {\n    }\n}\n</code></pre> <p>\u00c0 l'int\u00e9rieur se trouve un simple fichier USD (vide).</p> <p></p> <p>Quand je l'ai ouvert dans usdeview, il a montr\u00e9 une simple sph\u00e8re.</p> <p>Pour le moment, nous avons maintenant un environnement o\u00f9 nous pouvons toucher l'USD. Comme petite pr\u00e9cision, car je n'ai pas mis PATH dans le dossier lib J'ai une erreur DLL introuvable Erreur d'autorisation lors de la sp\u00e9cification d'un dossier sur le NAS de Synology comme destination de sauvegarde o\u00f9 il ne pouvait pas \u00eatre \u00e9crit.</p> <p>Maintenant que nous sommes pr\u00eats, tout en explorant la structure de base de l'USD Je voudrais r\u00e9sumer comment l'utiliser.</p>"},{"location":"pipeline/USD/01_start_USD/","title":"Cr\u00e9er un environnement pour diverses op\u00e9rations sur USD \u00e0 partir de Python","text":""},{"location":"pipeline/USD/01_start_USD/#cr\u00e9er-un-environnement-pour-diverses-op\u00e9rations-sur-usd-\u00e0-partir-de-python","title":"Cr\u00e9er un environnement pour diverses op\u00e9rations sur USD \u00e0 partir de Python","text":"<p>Je pensais que j'expliquerais d'abord les bases autour de l'USD et l'explication autour de la structure Si j'\u00e9cris avec une compr\u00e9hension vaguement \u00e0 moiti\u00e9 cuite<sub>~</sub>J'ai l'impression que je vais \u00eatre l\u00e9g\u00e8rement g\u00ean\u00e9<sub>~</sub> On dirait que \u00e7a va semer la confusion, alors je vais m\u00e9diter un peu plus dessus\u2026</p> <p>J'ai pu ouvrir USD avec usdview la derni\u00e8re fois, donc Au lieu d'\u00e9crire directement le fichier USD tel quel, faites-le fonctionner du c\u00f4t\u00e9 Python Je voudrais cr\u00e9er un environnement pour v\u00e9rifier ce qui se passe.</p>"},{"location":"pipeline/USD/01_start_USD/#pr\u00e9paration","title":"pr\u00e9paration","text":"<p>Tout d'abord, pr\u00e9parez-vous. Usdview peut v\u00e9rifier le mod\u00e8le, mais ce n'est qu'une visionneuse Vous ne pouvez pas faire des choses comme le contr\u00f4le en entrant des valeurs num\u00e9riques dans AttributeEditor.</p> <p></p> <p>Cependant, si vous voulez jouer avec des nombres, PythonInterpreter est attach\u00e9, donc Vous pouvez le contr\u00f4ler \u00e0 partir de l\u00e0.</p> <p>Mais\u2026 c'est impossible de faire de mon mieux avec cet interpr\u00e8te J'ai d\u00e9cid\u00e9 de cr\u00e9er un environnement en utilisant VSCode et Jupyter.</p> <p>Tout d'abord, utilisez la s\u00e9rie Python 3. Mais ici nous n'avons pas usdview T\u00e9l\u00e9chargez s\u00e9par\u00e9ment USD (build nvidia) pour Python2 Ouvrez usdview \u00e0 partir de l\u00e0.</p> <pre><code>cd /d I:\\jupyter_notebook_root\ncahier jupyter\n</code></pre> <p>Pour le moment, cr\u00e9ez un lot qui peut d\u00e9marrer Jupyter \u00e0 un emplacement fixe comme celui-ci D\u00e9marrer le bloc-notes en arri\u00e8re-plan. Mais lorsque j'utilise ce notebook depuis mon navigateur,</p> <p></p> <p>Il peut \u00eatre utilis\u00e9 pour le moment, mais dans ce cas, la saisie semi-automatique ne fonctionne pas Comme le raccourci est difficile \u00e0 utiliser, je vais essayer de le frapper du c\u00f4t\u00e9 korewoVSCode.</p> <p></p> <p>Ouvrez les param\u00e8tres URI du serveur Jupyter de Python, Entrez l'URL du Notebook ex\u00e9cut\u00e9 en arri\u00e8re-plan.</p> <p>Cependant, il \u00e9tait difficile d'ins\u00e9rer le jeton</p> <pre><code>bloc-notes jupyter --generate-config\n</code></pre> <p>Tout d'abord, cr\u00e9ez une configuration,</p> <p>C:/Users//.jupyter</p> <p>Ci-dessous, dans jupyter_notebook_config.py</p> <pre><code>c.NotebookApp.token = ''\n</code></pre> <p>Supprimez le jeton et</p> <pre><code>c.NotebookApp.password = \"sha1:\uff5e\uff5e\uff5e\uff5e\"\n</code></pre> <p>Tapez votre mot de passe.</p> <p>Le mot de passe est</p> <pre><code>python -c \"importer IPython; imprimer (IPython.lib.passwd())\"\n</code></pre> <p>Vous pouvez le g\u00e9n\u00e9rer avec cette commande.</p> <p>et.</p> <p>Une fois que tout est pr\u00eat, il est temps de tester les choses c\u00f4t\u00e9 VSCode.</p>"},{"location":"pipeline/USD/01_start_USD/#faites-quelque-chose-avec-vscode","title":"Faites quelque chose avec VSCode","text":"<p>Dans la version actuelle de VSCode, lorsque j'ouvre .ipynb (au format JupyterNotebook) Vous pouvez modifier des cahiers sur VSCode. (Des raccourcis, etc. de VSCode peuvent \u00e9galement \u00eatre utilis\u00e9s)</p>"},{"location":"pipeline/USD/01_start_USD/#imprimer","title":"imprimer","text":"<p>Tout d'abord, imprimons le contenu du fichier USD avant de l'ouvrir avec usdview.</p> <pre><code>print(stage.GetRootLayer().ExportToString())\n</code></pre> <p>Puisque vous voudrez le v\u00e9rifier souvent, divisez-le uniquement en cellules J'essaierai de l'imprimer si n\u00e9cessaire.</p> <p></p> <p>Comme \u00e7a, le fichier USD actuel (couches pour \u00eatre pr\u00e9cis) peut \u00eatre imprim\u00e9.</p> <p>Par pr\u00e9caution, lors de l'impression, comme \"imprimer (<sub>~</sub>)\" Que vous devez utiliser la commande d'impression correctement. Il peut \u00eatre affich\u00e9 sans lui, mais dans ce cas, les sauts de ligne ne sont pas possibles.</p>"},{"location":"pipeline/USD/01_start_USD/#enregistrer","title":"enregistrer","text":"<p>Exportez la sc\u00e8ne lors de l'enregistrement. Je le ferai fr\u00e9quemment, il est donc pratique de le garder dans une cellule.</p> <pre><code>\u00e9tape.GetRootLayer().Export(USD_PATH_ROOT + \"/refTest.usda\")\n</code></pre> <p>Il sortira comme ceci.</p> <p>USD peut \u00e9galement \u00eatre NewOpened et Saved lors de l'ouverture S'il y a d\u00e9j\u00e0 un fichier, ce sera une erreur, donc c'\u00e9tait un peu g\u00eanant</p> <pre><code># Cr\u00e9er un fichier une fois en m\u00e9moire\nstage = Usd.Stage.CreateInMemory()\n# Exporter\n\u00e9tape.GetRootLayer().Export(\"CHEMIN\")\n</code></pre> <p>Il est pr\u00e9f\u00e9rable de cr\u00e9er une sc\u00e8ne en m\u00e9moire comme celle-ci et de l'exporter \u00e0 la fin Je pense qu'il est facile de tester avec une nouvelle sc\u00e8ne \u00e0 chaque fois. (peut-\u00eatre)</p>"},{"location":"pipeline/USD/01_start_USD/#ouvrir-avec-usdview","title":"ouvrir avec usdview","text":"<p>Apr\u00e8s l'exportation, ouvrez le fichier avec usdview.</p> <pre><code>usdview I :\\usd_test\\refTest.usda\n</code></pre> <p>Est-ce un pi\u00e8ge lors de l'utilisation d'usdview sous Windows ? Le fichier usd pass\u00e9 en argument doit \u00eatre un chemin complet et le fichier usd doit \u00eatre pass\u00e9 en argument. De plus, comme le d\u00e9marrage de cet outil est inhabituellement lent Ouvrez-le avec un fichier appropri\u00e9 uniquement pour la premi\u00e8re fois Il est recommand\u00e9 d'ouvrir ou de recharger le fichier \u00e0 partir du menu de l'outil.</p> <p></p> <p>Je viens d'ouvrir le fichier.</p> <p>Si vous \u00eates pr\u00eat jusqu'\u00e0 pr\u00e9sent, alors en \u00e9crivant le code du c\u00f4t\u00e9 VSCode Apr\u00e8s avoir enregistr\u00e9, rechargez la sc\u00e8ne avec Ctrl + R dans usdview et les propri\u00e9t\u00e9s, prims et apparence Assurez-vous qu'il ressemble \u00e0 ce que vous voulez.</p> <p>https://snippets.cacher.io/snippet/e4a461c3093c7ce7929f</p> <p>Apr\u00e8s cela, le r\u00e9sultat du test est t\u00e9l\u00e9charg\u00e9 sur Cacher sous forme de m\u00e9mo.</p>"},{"location":"pipeline/USD/01_start_USD/#petite-histoire","title":"petite histoire","text":"<p>Affichage du r\u00e9sultat d'ex\u00e9cution PythonInteractive de VSCode \u00c7a devient de plus en plus. mais peut \u00eatre r\u00e9initialis\u00e9 en appuyant sur le bouton X dans le coin sup\u00e9rieur droit d'Interactif.</p> <p>Vous pouvez \u00e9galement le sortir sous forme de fichier Jupyter ipynb en appuyant sur l'ic\u00f4ne de disquette.</p> <p>https://snippets.cacher.io/snippet/90166b7fd86eb73d7d0e</p> <p>Je peux le sortir, mais je ne pense pas que je l'utiliserai beaucoup.</p> <p>Pour le moment, si vous avez fait jusqu'ici, vous pouvez jouer avec Python Nous avons cr\u00e9\u00e9 un environnement sans stress.</p> <p>Probablement, pour l'utiliser comme format de donn\u00e9es avec Exporter, etc. Je pense que tu n'as pas besoin d'aller aussi loin. Apr\u00e8s tout, pour g\u00e9rer la synth\u00e8se USD, le fonctionnement avec Python ou C ++ est essentiel. Parce qu'il est important de g\u00e9rer du c\u00f4t\u00e9 Python en termes de compr\u00e9hension de la structure des donn\u00e9es</p> <p>Je pense qu'il est important de rendre l'environnement de test sans stress.</p>"},{"location":"python/Pipenv/","title":"Pipenv","text":"<p>==============================================</p> <p>Pipenv:</p> <p>Pipenv is a Python virtualenv management tool that supports a multitude of systems and nicely bridges the gaps between pip, python (using system python, pyenv or asdf) and virtualenv. Linux, macOS, and Windows are all first-class citizens in pipenv.</p> <p>Pipenv automatically creates and manages a virtualenv for your projects, as well as adds/removes packages from your <code>Pipfile</code> as you install/uninstall packages. It also generates a project <code>Pipfile.lock</code>, which is used to produce deterministic builds.</p> <p>The problems that Pipenv seeks to solve are multi-faceted:</p> <ul> <li>You no longer need to use <code>pip</code> and <code>virtualenv</code> separately: they work together.</li> <li>Managing a <code>requirements.txt</code> file with package hashes can be problematic.  Pipenv uses <code>Pipfile</code> and <code>Pipfile.lock</code> to separate abstract dependency declarations from the last tested combination.</li> <li>Hashes are documented in the lock file which are verified during install. Security considerations are put first.</li> <li>Strongly encourage the use of the latest versions of dependencies to minimize security risks arising from outdated components.</li> <li>Gives you insight into your dependency graph (e.g. <code>$ pipenv graph</code>).</li> <li>Streamline development workflow by supporting local customizations with <code>.env</code> files.</li> </ul>"},{"location":"python/Pipenv/#installation","title":"Installation","text":"<p>Pipenv can be installed with Python 3.7 and above.</p> <p>For most users, we recommend installing Pipenv using <code>pip</code>:</p> <pre><code>pip install --user pipenv\n</code></pre> <p>Or, if you're using Fedora:</p> <pre><code>sudo dnf install pipenv\n</code></pre> <p>Or, if you're using FreeBSD:</p> <pre><code>pkg install py39-pipenv\n</code></pre> <p>Or, if you're using Gentoo:</p> <pre><code>sudo emerge pipenv\n</code></pre> <p>Or, if you're using Void Linux:</p> <pre><code>sudo xbps-install -S python3-pipenv\n</code></pre> <p>Alternatively, some users prefer to use Pipx:</p> <pre><code>pipx install pipenv\n</code></pre> <p>Or, some users prefer to use Python pip module</p> <pre><code>python -m pip install pipenv\n</code></pre> <p>Refer to the documentation for latest instructions.</p>"},{"location":"python/Poetry/","title":"Poetry","text":""},{"location":"python/Poetry/#poetry-python-packaging-and-dependency-management-made-easy","title":"Poetry: Python packaging and dependency management made easy","text":"<p>Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere.</p> <p></p> <p>Poetry replaces <code>setup.py</code>, <code>requirements.txt</code>, <code>setup.cfg</code>, <code>MANIFEST.in</code> and <code>Pipfile</code> with a simple <code>pyproject.toml</code> based project format.</p> <pre><code>[tool.poetry]\nname = \"my-package\"\nversion = \"0.1.0\"\ndescription = \"The description of the package\"\n\nlicense = \"MIT\"\n\nauthors = [\n    \"S\u00e9bastien Eustace &lt;sebastien@eustace.io&gt;\"\n]\n\nrepository = \"https://github.com/python-poetry/poetry\"\nhomepage = \"https://python-poetry.org\"\n\n# README file(s) are used as the package description\nreadme = [\"README.md\", \"LICENSE\"]\n\n# Keywords (translated to tags on the package index)\nkeywords = [\"packaging\", \"poetry\"]\n\n[tool.poetry.dependencies]\n# Compatible Python versions\npython = \"&gt;=3.8\"\n# Standard dependency with semver constraints\naiohttp = \"^3.8.1\"\n# Dependency with extras\nrequests = { version = \"^2.28\", extras = [\"security\"] }\n# Version-specific dependencies with prereleases allowed\ntomli = { version = \"^2.0.1\", python = \"&lt;3.11\", allow-prereleases = true }\n# Git dependencies\ncleo = { git = \"https://github.com/python-poetry/cleo.git\", branch = \"main\" }\n# Optional dependencies (installed by extras)\npendulum = { version = \"^2.1.2\", optional = true }\n\n# Dependency groups are supported for organizing your dependencies\n[tool.poetry.group.dev.dependencies]\npytest = \"^7.1.2\"\npytest-cov = \"^3.0\"\n\n# ...and can be installed only when explicitly requested\n[tool.poetry.group.docs]\noptional = true\n[tool.poetry.group.docs.dependencies]\nSphinx = \"^5.1.1\"\n\n# Python-style entrypoints and scripts are easily expressed\n[tool.poetry.scripts]\nmy-script = \"my_package:main\"\n</code></pre>"},{"location":"python/Poetry/#installation","title":"Installation","text":"<p>Poetry supports multiple installation methods, including a simple script found at [install.python-poetry.org]. For full installation instructions, including advanced usage of the script, alternate install methods, and CI best practices, see the full [installation documentation].</p>"},{"location":"python/PySide2/","title":"PySide","text":""},{"location":"python/PySide2/#using-qrc-files-pyside2-rcc","title":"Using .qrc Files (pyside2-rcc)","text":"<p>Note</p> <p>The Qt Resource System is a mechanism for storing binary files in an application. The most common uses are for custom images, icons, fonts, among others.</p> <ul> <li>The .qrc file Before running any command, add information about the resources to a .qrc file.  In the following example, notice how the resources are listed in icons.qrc</li> </ul> <pre><code>&lt;/ui&gt;\n&lt;!DOCTYPE RCC&gt;&lt;RCC version=\"1.0\"&gt;\n&lt;qresource&gt;\n    &lt;file&gt;icons/play.png&lt;/file&gt;\n    &lt;file&gt;icons/pause.png&lt;/file&gt;\n    &lt;file&gt;icons/stop.png&lt;/file&gt;\n    &lt;file&gt;icons/previous.png&lt;/file&gt;\n    &lt;file&gt;icons/forward.png&lt;/file&gt;\n&lt;/qresource&gt;\n&lt;/RCC&gt;\n</code></pre> <ul> <li>Generating a Python file</li> </ul> <p>Now that the icons.qrc file is ready, use the pyside2-rcc tool to generate a Python class containing the binary information about the resources</p> <pre><code>pyside2-rcc icons.rc -o rc_icons.py\n</code></pre> <pre><code>from PySide2.QtGui import QIcon, QKeySequence, QPixmap\nplayIcon = QIcon(QPixmap(\":/icons/play.png\"))\npreviousIcon = QIcon(QPixmap(\":/icons/previous.png\"))\npauseIcon = QIcon(QPixmap(\":/icons/pause.png\"))\nnextIcon = QIcon(QPixmap(\":/icons/forward.png\"))\nstopIcon = QIcon(QPixmap(\":/icons/stop.png\"))\n</code></pre>"},{"location":"python/PySide2/#using-ui-file","title":"Using .ui file","text":"<p>We've created a very simple UI. The next step is to get this into Python and use it to construct a working application.</p> <ul> <li>Loading the .ui file directly</li> </ul> <p>To load .ui files in PySide we first create a QUiLoader instance and then call the loader.load() method to load the UI file.</p> <pre><code>import sys\nfrom PySide2 import QtCore, QtGui, QtWidgets\nfrom PySide2.QtUiTools import QUiLoader\n\nloader = QUiLoader()\napp = QtWidgets.QApplication(sys.argv)\nwindow = loader.load(\"mainwindow.ui\", None)\nwindow.show()\napp.exec_()\n</code></pre> <p>Warning</p> <p>but slot-signal bindings, which are set in the designer in *.ui file, are not working anyway.</p> <p>So, for full-function use of designer GUI and slot-signal bindings, the only way I found is to compile *.ui file to python module with pyside UI compiler:</p> <ul> <li>Converting your .ui file to Python Instead of importing your .uic files into your application directly, you can instead convert them into Python code and then import them like any other module. To generate a Python output file run pyside2-uic from the command line, passing the .ui file and the target file for output, with a -o parameter. The following will generate a Python file named MainWindow.py which contains our created UI.</li> </ul> <pre><code>pyside2-uic mainwindow.ui -o MainWindow.py\n</code></pre> <pre><code>import sys\nfrom PySide2 import QtWidgets\n\nfrom MainWindow import Ui_MainWindow\n\n\nclass MainWindow(QtWidgets.QMainWindow, Ui_MainWindow):\n    def __init__(self):\n        super(MainWindow, self).__init__()\n        self.setupUi(self)\n\n\napp = QtWidgets.QApplication(sys.argv)\n\nwindow = MainWindow()\nwindow.show()\napp.exec_()\n</code></pre>"},{"location":"python/packages/","title":"Packages","text":""},{"location":"python/packages/#using-requirementtxt","title":"Using Requirement.txt","text":"<pre><code>pip install -r .\\requirements.txt --force-reinstall\npip wheel -w dist\n</code></pre>"},{"location":"python/pyenv/","title":"Pyenv","text":""},{"location":"python/pyenv/#pyenv-for-windows","title":"pyenv for Windows","text":"<ul> <li>pyenv for Windows</li> </ul>"},{"location":"python/pyenv/#pyenv","title":"pyenv","text":"<p>[pyenv][1] is a simple python version management tool. It lets you easily switch between multiple versions of Python. It's simple, unobtrusive, and follows the UNIX tradition of single-purpose tools that do one thing well.</p>"},{"location":"python/pyenv/#quick-start","title":"Quick start","text":"<ol> <li>Install pyenv-win in PowerShell.</li> </ol> <pre><code>Invoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &amp;\"./install-pyenv-win.ps1\"\n</code></pre> <ol> <li>Reopen PowerShell</li> <li>Run <code>pyenv --version</code> to check if the installation was successful.</li> <li>Run <code>pyenv install -l</code> to check a list of Python versions supported by pyenv-win</li> <li>Run <code>pyenv install &lt;version&gt;</code> to install the supported version</li> <li>Run <code>pyenv global &lt;version&gt;</code> to set a Python version as the global version</li> <li>Check which Python version you are using and its path</li> </ol> <pre><code>&gt; pyenv version\n&lt;version&gt; (set by \\path\\to\\.pyenv\\pyenv-win\\.python-version)\n</code></pre> <ol> <li>Check that Python is working</li> </ol> <pre><code>&gt; python -c \"import sys; print(sys.executable)\"\n\\path\\to\\.pyenv\\pyenv-win\\versions\\&lt;version&gt;\\python.exe\n</code></pre>"},{"location":"python/pyenv/#pyenv-win-commands","title":"pyenv-win commands","text":"<pre><code>   commands     List all available pyenv commands\n   local        Set or show the local application-specific Python version\n   global       Set or show the global Python version\n   shell        Set or show the shell-specific Python version\n   install      Install 1 or more versions of Python \n   uninstall    Uninstall 1 or more versions of Python\n   update       Update the cached version DB\n   rehash       Rehash pyenv shims (run this after switching Python versions)\n   vname        Show the current Python version\n   version      Show the current Python version and its origin\n   version-name Show the current Python version\n   versions     List all Python versions available to pyenv\n   exec         Runs an executable by first preparing PATH so that the selected Python\n   which        Display the full path to an executable\n   whence       List all Python versions that contain the given executable\n</code></pre>"},{"location":"python/pyenv/#installation","title":"Installation","text":"<p>Currently we support following ways, choose any of your comfort:</p> <ul> <li>PowerShell - easiest way</li> <li>Git Commands - default way + adding manual settings</li> <li>Pyenv-win zip - manual installation</li> <li>Python pip - for existing users</li> <li>Chocolatey</li> <li>How to use 32-train</li> <li>check announcements</li> </ul>"},{"location":"python/python_intro/","title":"Intro","text":""},{"location":"python/python_intro/#icon","title":"icon","text":"<p>:smile: :fa-coffee:</p> <p>:fish: :frog:</p>"},{"location":"python/python_intro/#fontawesome","title":"fontawesome","text":"<p>:fa-building: :fa-android: :fa-copy: :fa-folder: :fa-angle-double-right:</p> <p>:fa-angle-right:</p>"},{"location":"python/python_tips/","title":"Tips","text":"<p>:maya: :python: By Dhruv Govil</p> <p>When I write a Python class that exposes the functionality of a Maya node , I override <code>py__repr__</code>  so that it returns the node's path so I can just pass the object to any Maya commands and it just works.</p> <p>:GitHub: By Thomas Masencal</p> <p>If you append .patch to a Github commit URL, you get the patch file: https://github.com/KelSolaar/colour/commit/e38b3e706e4e3581dd4e9c7806fe84422abadac2.patch</p> <p>:maya: By Yantor3d</p> <p>TIL Maya can't load audio if the file basename starts with a number. https://twitter.com/yantor3d/status/1433464047278575622</p> <p>:python: By Lee Dunham</p> <p>Remember that if you really want to use the nasty from foo import * it is worth considering using the all variable in foo to control what is going to be imported.</p> <p>https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python/64130#64130</p> <p>:houdini: By Paul Ambrosiussen</p> <p>Did you know you can comment one or multiple lines of code in #Houdini using CTRL+/ ? (in the VEXpression editor)</p> <p>https://twitter.com/ambrosiussen_p/status/1463177572766863374?s=20</p> <p>:Maya: By Stuart</p> <p>Maya's internal angular units are radians, even if the settings are set to degrees? And that's why the unitConversion gets added when both input and output are meant to be degrees ?</p> <p>Confirmed: addAttr -ln \"rotTest2\"  -at doubleAngle  -dv 0 |nurbsCircle1; and then connecting to a rotate attribute doesn't create an unitConversion node.</p> <p>https://tech-artists.slack.com/archives/C0AN0KPMZ/p1643739680183619</p> <p>:Maya: By Mark Jackson</p> <p>You can have a string of any length but if stored in an UI element and the element is selected, the tring become clamped to 16bit = 32,767 characters.</p> <p>http://markj3d.blogspot.com/2012/11/maya-string-attr-32k-limit.html</p> <p>:Houdini: By Richard C Thomas</p> <p>Hou-ple, Pulling Lops into Sops? Want to grab data using these VEX methods\u2026 https://sidefx.com/docs/houdini/solaris/vex.html Don't forget to use op: !  I hope you never know the frustration of the last 2 hours.</p> <pre><code>string stage = \"op:/stage/OUT\";\nmatrix mat = usd_primvar(stage, root, \"xformOP:transform\", 0);\n</code></pre> <p>https://twitter.com/DoesCG/status/1502363843208622097</p> <p>:python: By Thomas Mansencal</p> <p>Remember that you can use a semi-colon as a \"line break\" in python. It is super useful to pass commands to the interpreter, e.g.  <code>pypython -c \u201cimport sys;import pprint;pprint(dir(sys))\u201d</code></p> <p>:GitHub: By Oleksii Holub</p> <p>GitHub now supports special \"warning\" and \"note\" blockquotes for callouts in your markdown content. </p><pre><code>&gt; **Warning**\n&gt; Some text\n</code></pre> <p>https://twitter.com/Tyrrrz/status/1554784140326748161</p>"},{"location":"python/venv/","title":"Venv","text":""},{"location":"python/venv/#environnement-virtuel","title":"ENVIRONNEMENT VIRTUEL","text":"<p>Un environnement virtuel est un environnement d'ex\u00e9cution isol\u00e9.</p> <p>Les environnements virtuels sont utilis\u00e9s afin d'isoler les paquets utilis\u00e9s pour un projet.</p> <p>On peut ainsi avoir sur le m\u00eame ordinateur deux projets Python (ou plus) qui utilisent chacun une version diff\u00e9rente d'un m\u00eame paquet.</p> <p>Pour cr\u00e9er un environnement virtuel, on peut utiliser le module venv qui est inclus dans la biblioth\u00e8que standard de Python :</p> <pre><code>python -m venv nom_de_lenvironnement\n</code></pre> <p>L'environnement virtuel contient plusieurs dossiers et fichiers :</p> <pre><code>\u251c\u2500\u2500 bin\n\u2502   \u251c\u2500\u2500 Activate.ps1\n\u2502   \u251c\u2500\u2500 activate\n\u2502   \u251c\u2500\u2500 activate.csh\n\u2502   \u251c\u2500\u2500 activate.fish\n\u2502   \u251c\u2500\u2500 easy_install\n\u2502   \u251c\u2500\u2500 easy_install-3.8\n\u2502   \u251c\u2500\u2500 pip\n\u2502   \u251c\u2500\u2500 pip3\n\u2502   \u251c\u2500\u2500 pip3.8\n\u2502   \u251c\u2500\u2500 python -&gt; python3\n\u2502   \u2514\u2500\u2500 python3 -&gt; /Library/Developer/CommandLineTools/usr/bin/python3\n\u251c\u2500\u2500 include\n\u251c\u2500\u2500 lib\n\u2502   \u2514\u2500\u2500 python3.8\n\u2502      \u2514\u2500\u2500 site-packages\n\u2514\u2500\u2500 pyvenv.cfg\n</code></pre> <p>Le dossier bin contient l'interpr\u00e9teur Python et tous les ex\u00e9cutables dont vous pourriez avoir besoin (comme easy_install ou pip). C'est \u00e9galement dans ce dossier que vous trouverez les fichiers qui vous permettent d'activer votre environnement virtuel (activate).</p> <p>Pour activer votre environnement virtuel, il suffit donc de \u00ab sourcer \u00bb le fichier activate dans votre terminal :</p> <pre><code>$ source bin/activate\n</code></pre> <p>windows </p><pre><code>venvName/Scripts/activate.bat\n</code></pre> Le dossier include ne contient par d\u00e9faut aucun fichier. Ce dossier sert dans le cas de la cr\u00e9ation de biblioth\u00e8ques utilisant le langage C. <p>Le dossier lib contient tous les paquets que vous installerez avec pip dans cet environnement virtuel (\u00e0 l'int\u00e9rieur du sous-dossier site-packages).</p> <p>Le fichier pyvenv.cfg contient des variables qui d\u00e9finissent certains param\u00e8tres de votre environnement virtuel, comme le chemin vers l'interpr\u00e9teur Python syst\u00e8me ou la version de Python de votre environnement virtuel.</p>"},{"location":"rendering/GuiltyGear/","title":"NPR","text":""},{"location":"rendering/GuiltyGear/#guilty-gear-xrd--sign--the-secret-of-using-real-time-3d-graphics-technology-to-realize-animation-screen-rendering-part-1---2","title":"GUILTY GEAR Xrd -SIGN- The Secret of Using Real-time 3D Graphics Technology to Realize Animation Screen Rendering, Part 1 - 2","text":""},{"location":"rendering/GuiltyGear/#lighting-and-shading-2","title":"lighting and shading (2)","text":"<p>In cell animation-like 2D graphics, the presence of \"specular highlights\" that appear due to specular reflections (highlights that depend on the line of sight) is often not very noticeable. However, in GUILTY GEAR Xrd -SIGN-, this specular highlight is also added to the lighting as a result of adjustment based on the \"grammar for adding highlights in hand-drawn illustrations\".</p> <p></p> <p>So what is that grammar?</p> <p>It's difficult to express it in one word, but to give some examples, like skin and clothes, even if they are close to each other, if the materials are different, the highlights will not stick together. It's not exactly correct for , and it's like highlighting along uneven boundaries or material boundaries.</p> <p></p> <p>Specular highlights are controlled by the \"ease of highlighting\" parameter stored in the B (=blue) channel of the texture for lighting control. Since this parameter adjusts the intensity of the specular reflection calculation results, the maximum value results in burnt-in highlights, and conversely, the smaller the value, the more the highlights fade.</p> <p>By the way, the R channel of the lighting control texture is the \"specular highlight intensity\" parameter, and a larger value is set for metal and smooth material parts</p> <p></p> <p>However, even with all these efforts, the development team was still not convinced. What is lacking in the aim of achieving a \"perfect cel-animated taste\"?</p> <p></p>"},{"location":"rendering/GuiltyGear/#mr-motomura","title":"Mr. Motomura:","text":"<p>It's \"color\". As a result of the toon shader, the light and dark are created, but I got the impression that the light and dark were somewhat monotonous. With a simple toon shader process that just multiplies the shaded areas with a single shade, I felt that the texture was lacking in persuasiveness and richness.</p> <p>On the other hand, at the production site of TV animation, there is an artist who specializes in color design. So, the colors are set individually. I thought that the difference lies here.</p> <p>Of course, it is not possible to make detailed individual settings like cell animation. Therefore, as a result of researching systematic implementation, the person in charge of color design for TV animation instinctively examines the \"ambient light color of the scene\" and \"the light transmittance of the material to be expressed\". , It seems that he arrived at the inference that he decided the color to be set. Mr. Motomura said that when he tried to implement it based on this reasoning, he was able to obtain a result that was quite close to the ideal, and decided to include it in the final specification.</p> <p>The actual mechanism is not that complicated. First, prepare a texture with a distribution of \"colors that are likely to appear when shaded\" corresponding to the basic texture (Base Texture) applied to the 3D model. This texture is called \"SSS texture\" (SSS: SubSurface Scattering) for convenience within the development team.</p> <p>If the pixel is shaded as a result of lighting, the color obtained by multiplying the value of this SSS texture and the \"ambient light color\" is determined as the shade color. On the other hand, if the lighting result is bright, the SSS texture value is ignored, so only the light source color is affected.</p> <p>It seems that the development team was satisfied with this carving process, which brought it closer to a cel-animation-like coloring.</p> <p></p> <p>Specifically, what kind of effect appears, for example, the shadow that appears on the character's skin color becomes slightly reddish. In addition, the color saturation of the clothes remains in the shade of the clothes. In other words, the SSS texture is composed of such \"reddish\" and \"colors that retain the saturation of clothes\".</p> <p>The SSS texture doesn't simulate subsurface scattering, so the name may not be exactly correct (laughs). As a supplementary explanation, I think that the SSS texture simply represents \"how much light passes through the material\". The color of shadows on thin paper is light, isn't it? It is the texture of such an image.</p>"},{"location":"rendering/GuiltyGear/#contour-secret-1","title":"Contour secret (1)","text":"<p>Contour lines (line drawings) play a significant role in the elements that make up the anime-style visuals of GUILTY GEAR Xrd -SIGN-. In terms of the inking part in manga, GUILTY GEAR Xrd -SIGN- implements a combination of two approaches for this expression.</p> <p>The \"Back-Facing\" method is used for the contour lines of the 3D model, which is the most basic line drawing.</p> <p>Normally, when trying to draw a 3D model with a GPU, the polygons on the back side of the viewpoint are discarded as \"invisible\" and out of the drawing target. This mechanism is called \"Backface Culling\", which is based on the idea that \"the polygons on the back side of the character model facing the front cannot be seen from the point of view anyway, so they are not drawn\". .</p> <p>On the other hand, in drawing with the back surface method, the rendering that reverses this mechanism of back surface culling is combined. In other words, \"reverse\" means to reverse the processing system that normally \"does not draw the back\", that is, \"draws the back and does not draw the front\".</p> <p></p> <p>To explain the flow of processing, as the first step, the 3D model is slightly expanded and reversed \"back culling\" is performed. As a result, a black silhouette of the 3D model is drawn, so save this for the time being.</p> <p>As a second step, the 3D model is rendered at its original size using a normal processing system. Then, in the final stage, the black silhouette and the normal rendering result are combined. Most of the black silhouette is overwritten by normal rendering results, but the black silhouette is a slightly expanded 3D model, so only the outline remains as a result.</p> <p>In the GUILTY GEAR Xrd -SIGN- rendering pipeline, Z-buffer advance is performed, so a nearly perfect outline can be obtained at the first stage. So I feel that the final synthesis phase is unnecessary, but the concept is like this.</p> <p></p> <p>In fact, this method is a classic that has been used before the rise of programmable shader technology, but GUILTY GEAR Xrd -SIGN- makes full use of the vertex shader to implement an original extension of this classic method. are doing.</p> <p></p> <p>Arc System Works' own extensions were made to reproduce the hand-drawn drawing touch, as well as control parts that prevent lines from becoming too thin or too thick, regardless of camera zoom or character perspective. It is a control part that adds strength and weakness to the thickness of the line in the curved and straight parts. The line drawing of GUILTY GEAR Xrd -SIGN-, as if drawn by actually running the pen, is born from the control of such vertex shaders.</p>"},{"location":"rendering/GuiltyGear/#mr-motomura_1","title":"Mr. Motomura:","text":"<p>I chose the back-face method because I felt that it was advantageous for the artist to freely control the strength of the line drawing. In the GUILTY GEAR Xrd -SIGN- 3D model, the vertex color has a \"thickness control value when drawing lines\", which allows you to add strength to the line drawing. With this, it's possible to create expressions like the ones you see in anime hand-drawn drawings, where the cheeks are thick and taper toward the chin.</p> <p></p> <p>According to Mr. Motomura, the usage breakdown of vertex colors in GUILTY GEAR Xrd -SIGN- is as follows.</p> <ul> <li> R: Offset for the shadow determination threshold. 1 is the standard, and areas that tend to be shadowed are darker. 0 always casts a shadow</li> <li> G: Coefficient for how much the contour is expanded according to the distance from the camera</li> <li> B: Z offset value of contour line</li> <li> A: Contour thickness factor. 0.5 is standard, 1 is the maximum thickness, 0 is no outline</li> </ul> <p>G and A are parameters related to outline thickness control. R is the hand-crafted ambient occlusion-like self-occlusion factor, which was also mentioned in Lighting and Shading (1). B is a coefficient that determines how much the depth direction (Z direction) is offset (=offset) with respect to the viewpoint when dilating using the back method. As a result, the outline can be erased. According to Mr. Motomura, it was a parameter that was included to prevent unwelcome contour lines such as wrinkles appearing in the hair and under the nose of the face.</p> <p></p>"},{"location":"rendering/GuiltyGear/#mr-ishiwata","title":"Mr. Ishiwata:","text":"<p>The reason I didn't use a post-effect approach was because I thought it would be difficult to control the strength of the line thickness like this time. With this back-face line drawing mechanism, it is possible to adjust how the lines will appear on the actual machine from the stage of 3D model creation, which is performed by the artist, so we can create the model and how the outline will appear at the same time. Shin.</p> <p>Mr. Ishiwatari's \"post-effect approach\" is to draw lines in real-time photo retouching with a pixel shader on the rendering result. Specifically, by detecting the depth price difference in the rendering result, or by detecting the step of the inner product value of the pixel unit line of sight (= line of sight vector) and the direction of the surface (= normal vector), the contour Determine line pixels. This method is often adopted when it is judged that the geometry load is too high with the back surface method.</p> <p></p>"},{"location":"rendering/GuiltyGear/#contour-secret-2","title":"Contour secret (2)","text":"<p>~New development!?</p> <p>In GUILTY GEAR Xrd -SIGN-, it is said that another method of line drawing is added to the grooves, seams, and muscle ridges of clothes and accessories to express outlines.</p> <p>Mr. Motomura:</p> <p>There are places where outlines cannot be drawn with the back surface method, such as grooves in the 3D structure. There is no choice but to insert texture mapping in such places, but if texture mapping is done normally, jaggies will appear when the camera is zoomed in, and the difference from the beautiful outline by the back method will be noticeable. That's right. Therefore, I went back to the basics of \"under what circumstances does jaggy appear in texture mapping?\"</p> <p>The result was a unique line-drawing technique known \u3000within the development team as the \" Motomura-style line .\"</p> <p></p> <p>In the first place, jaggies in texture mapping become apparent when a certain texel (= pixel that constitutes texture) is drawn as a single texel on a polygon surface. On the other hand, if there are adjacent texels, unlike the case of a single texel, the outline of the square texel shape practically disappears, so the jaggy feeling is less likely to be exposed.</p> <p></p> <p>However, even if they are adjacent to each other, if they are diagonally above or below, the texels are virtually identical to a single state, resulting in a jaggy feeling. In other words, a set of pixels in which texels are arranged in horizontal or vertical lines can add a bokeh effect, but avoid the jaggy effect .</p> <p></p> <p>It's a matter of course, but when Mr. Motomura realized this, he created a `` texture that consists of only vertical and horizontal lines as solid lines that are given as contour lines'' . After that, when applying it to the 3D model, where you want diagonal lines and curves, apply distortion and bending so that it is mapped UV map (= where each polygon on the 3D model corresponds to the texture map data that expresses the</p> <p>Now, when you try texture mapping with this method, it's amazing. Beautiful and smooth line drawing was obtained even with textures of not so high resolution.</p> <p>Below is the texture for the Motomura line that was actually used. Note that the texture for the Motomura line is stored in a single color on the \u03b1 channel of the texture map for lighting control because only line drawing information is sufficient.</p> <p></p> <p>In the example of the Motomura line, you can see that it has a mysterious texture that looks like a city planning map, with only orthogonal line segments. It can be a contour line. The muscle protuberances have an elliptical hemispherical shape, and in the Motomura line, they are ``mapped into an elliptical border line of a square''. \u3000Since the square is elliptical, the area inside the square is stretched and distorted considerably. So, if there are characters or patterns inside this square, it will naturally be distorted, but since this texture is only for adding contour lines, such characters and patterns are not included. Since only lines are visible visually, the distortion caused by this method does not appear as a visual problem.</p> <p>Of course, texture mapping is done based on UVs that have been distorted and bent, and if the character or viewpoint moves, the texture will be affected by scaling and rotation. As a result, the line segments that are drawn are also affected by it and become curved or diagonal lines, but since bilinear filtering is applied in texture mapping, such curves and diagonal lines has a moderate degree of bokeh. And this is just the right anti-aliasing effect.</p> <p></p> <p>By the way, the strength and weakness of thickness can also be seen in the line segments drawn by the Motomura line. However, the texture itself for the Motomura line does not have such subtle line segment strength, and the line thickness is almost uniform. How is the strength of the drawn line added even though the thickness of the texture line is uniform?</p> <p>!!!Mr. Motomura:     The strength and weakness of the line drawn with the Motomura style line is added in the design of the UV map. If you want to make a thick line, you can devise a UV map so that the line representation texels on the texture are widely allocated to the polygon surface.</p> <p>I have a feeling that this Motomura style line can be widely applied to game graphics other than anime touches, depending on how it is done.</p> <p></p>"},{"location":"rendering/LIB/","title":"Library","text":""},{"location":"rendering/LIB/#mylibrary","title":"MyLibrary","text":"<ul> <li> TODO: organize this in a better way (https://github.com/AlexanderVeselov/MyLibrary/blob/master/README.md)</li> </ul>"},{"location":"rendering/LIB/#important-resources","title":"Important resources","text":"<ul> <li>SIGGRAPH Course: Physically Based Shading in Theory and Practice</li> <li>SIGGRAPH Course: Advances in Real-Time Rendering in 3D Graphics and Games</li> <li>SIGGRAPH Course: Rendering Engine Architecture</li> <li>GDC Vault</li> <li>Graphics Programming Weekly - Article Database</li> <li>KIT Computer Graphics Group research page</li> <li>EPFL RGL (Realistic Graphics Lab)</li> <li>Ray Tracing Gems</li> <li>Ray Tracing Gems II</li> <li>Light Transport Papers</li> </ul>"},{"location":"rendering/LIB/#authors-pages","title":"Authors' pages","text":"<ul> <li>Matt Pharr</li> <li>Morgan McGuire</li> <li>Iliyan Georgiev</li> <li>Eric Heitz</li> <li>Stephen Hill</li> <li>Anton Kaplanyan</li> <li>Arseny Kapoulkine</li> <li>Edward Liu</li> <li>Thomas M\u00fcller</li> <li>Yusuke Tokuyoshi</li> <li>Ari Silvennoinen</li> <li>Peter-Pike Sloan</li> <li>Wojciech Jarosz</li> <li>Brian Karis</li> <li>Chris Wyman</li> <li>Cem Yuksel</li> <li>S\u00e9bastien Hillaire</li> <li>Matt Pettineo</li> <li>Ravi Ramamoorthi</li> <li>Christoph Peters</li> </ul> Paper Name + Link Authors Year/Event Illumination for computer generated pictures + Full Dissertation Bui Tuong Phong 1975 An Improved Illumination Model for Shaded Display Turner Whitted CACM 1980 A Reflectance Model for Computer Graphics Robert L. Cook, Kenneth E. Torrance SIGGRAPH 1981 Modeling the Interaction of Light Between Diffuse Surfaces  Cindy M. Goral, Kenneth E. Torrance, Donald P. Greenberg, Bennett Battaile SIGGRAPH 1984 Ray Tracing Volume  Densities James Kajiya, Brian Von Herren SIGGRAPH 1984 The Reyes Rendering Architecture Robert L. Cook, Loren Carpenter, Edwin Catmull SIGGRAPH 1987 Bi-Directional Path Tracing Eric P. Lafortune, Yves D. Willems Compugraphics 1993 Radiosity and Realistic Image Synthesis Michael F. Cohen Book, 1993 The Irradiance Volume + Full thesis Gene Greger, Peter Shirley, Philip M. Hubbard, Donald P. Greenberg 1996 Instant Radiosity + Slides Alexander Keller SIGGRAPH 1997 Robust monte carlo methods for light transport simulation Eric Veach 1997 Metropolis Light Transport Eric Veach, Leonidas J. Guibas SIGGRAPH 1997 Fast Minimum Storage Ray/Triangle Intersection Tomas M\u00f6ller, Ben Trumbore JGT 1997 An Anisotropic Phong Light Reflection Model Michael Ashikhmin, Peter Shirley Journal of Graphics Tools 2000 Weighted Importance Sampling Techniques for Monte Carlo Radiosity Philippe Bekaert, Mateu Sbert, Yves D. Willems Eurographics 2000 An Efficient Representation for Irradiance Environment Maps Ravi Ramamoorthi,  Pat Hanrahan SIGGRAPH 2001 Precomputed Radiance Transfer for Real-Time Rendering in Dynamic, Low-Frequency Lighting Environments + PRTCourse + Normal Mapping for PRT + Deformations Peter-Pike Sloan, Jan Kautz, John Snyder SIGGRAPH 2002 Global Illumination Compedinum Philip Dutr\u00e9 Half-Life 2 / Valve Source Shading Gary McTaggart GDC 2004 Deferred Shading Shawn Hargreaves GDC 2004 Irradiance Volumes for Games N. Tatarchuk GDC Europe 2005 Precomputed Radiance Transfer: Theory and Practice J. Kautz, J. Lehtinen, Peter-Pike Sloan SIGGRAPH 2005 Energy Redistribution Path Tracing + Video talk David Cline, Justin Talbot, Parris Egbert SIGGRAPH 2005 Reflective Shadow Maps Carsten Dachsbacher, Marc Stamminger i3D 2005 Importance Resampling for Global Illumination Justin Talbot EGS 2005 Non-interleaved Deferred Shading of Interleaved Sample Patterns Benjamin Segovia, Jean-Claude Iehl, Bernard P\u00e9roche Eurographics 2006 Jump Flooding in GPU with Applications to Voronoi Diagram and Distance Transform Guodong Rong, Tiow-Seng Tan i3D 2006 Variance Shadow Maps William Donnelly, Andrew Lauritzen i3D 2006 HDR in Valve\u2019s Source Engine  Gary McTaggart SIGGRAPH 2006 Shading in Valve\u2019s Source Engine + Slides Jason Mitchell, Gary McTaggart, Chris Green SIGGRAPH 2006 Accelerated regular grid traversals using extended anisotropic chessboard distance fields on a parallel stream processor Alphan Es, Veysi Isler 2007 Advanced Real-Time Rendering in 3D Graphics and Games (all papers) Johan Andersson, Shannon Drone, Nico Galoppo, Chris Green, Chris Oat, Jason L. Mitchell, Martin Mittring, Natalya Tatarchuk SIGGRAPH 2007 Efficient Self-Shadowed Radiosity Normal Mapping + SIGGRAPH paper Chris Green SIGGRAPH 2007 Joint Bilateral Upsampling Johannes Kopf, Michael F. Cohen, Dani Lischinski, Matt Uyttendaele SIGGRAPH 2007 Finding Next Gen \u2013 CryEngine 2 Martin Mittring SIGGRAPH 2007 Microfacet Models for Refraction through Rough Surfaces Bruce Walter, Stephen R. Marschner, Hongsong  Li, Kenneth E. Torrance ESGR 2007 Convolution shadow maps Thomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, Jan Kautz EGSR 2007 Raytracing Prefiltered Occlusion for Aggregate Geometry D. Lacewell, B. Burley, S. Boulos, P. Shirley 3<sup>rd</sup> Symposium on Interactive Ray Tracing (2008) Stupid Spherical Harmonics (SH) Tricks Peter-Pike Sloan GDC 2008 Post Processing in The Orange Box Alex Vlachos GDC 2008 Image-Space Horizon-Based Ambient Occlusion + Presentation Louis Bavoil, Miguel Sainz, Rouslan Dimitrov SIGGRAPH 2008 Advanced Virtual Texture Topics Martin Mittring SIGGRAPH 2008 Exponential shadow maps + Thesis Thomas Annen, Tom Mertens, Hans-Peter Seidel, Eddy Flerackers, Jan Kautz GI 2008 Imperfect Shadow Maps for Efficient Computation of Indirect Illumination T. Ritschel, T. Grosch, M. H. Kim, H.-P. Seidel, C. Dachsbacher, J. Kautz 2008 Approximating Dynamic Global Illumination in Image Space (SSDO) T. Ritschel, T. Grosch H.P., Seidel i3D 2009 Importance sampling spherical harmonics Wojciech Jarosz, Nathan A. Carr, Henrik Wann Jensen Eurographics 2009 Image Space Gathering Austin Robison, Peter Shirley HPG 2009 Light Propagation Volumes in CryEngine 3 + Slides Anton Kaplanyan SIGGRAPH 2009 Stochastic Progressive Photon Mapping Toshiya Hachisuka, Henrik Wann Jensen SIGGRAPH Asia 2009 Shading a Bigger, Better Sequel: Techniques in Left 4 Dead 2 Bronwen Grimes GDC 2010 Rendering Wounds in Left 4 Dead 2 Alex Vlachos GDC 2010 Efficient Sparse Voxel Octrees Samuli Laine, Tero Karras i3D 2010 Spatio-Temporal Upsampling on the GPU Robert Herzog, Elmar Eisemann, Karol Myszkowski, H.-P. Seidel SIGGRAPH 2010 Water Flow in Portal 2 Alex Vlachos SIGGRAPH 2010 Real-time Diffuse Global Illumination in CryENGINE 3 Anton Kaplanyan SIGGRAPH 2010 CryENGINE 3: reaching the speed of light Anton Kaplanyan SIGGRAPH 2010 A Real-Time Radiosity Architecture for Video Game Sam Martin, Per Einarsson SIGGRAPH 2010 Introduction to GPU Radix Sort Takahiro Harada, Lee Howes 2011 VoxelPipe: A Programmable Pipeline for 3D Voxelization Jacopo Pantaleoni HPG 2011 Bidirectional light transport with vertex merging Iliyan Georgiev, Jaroslav K\u0159iv\u00e1nek, Philipp Slusallek SIGGRAPH Asia 2011 The Technology Behind the DirectX 11 Unreal Engine \"Samaritan\" Demo + Video Martin Mittring, Bryan Dudash GDC 2011 Introduction to GPU Radix Sort Takahiro Harada, Lee Howes 2011 Deferred Radiance Transfer Volumes: Global Illumination in Far Cry 3 Nikolay Stefanov GDC 2012 Stable SSAO in Battlefield 3 with Selective Temporal Filtering (Video) + Slides Louis Bavoil GDC 2012 Forward+: Bringing Deferred Lighting to the Next Level Takahiro Harada, Jay McKee, Jason C.Yang EGSR 2012 Light Probe Interpolation Using Tetrahedral Tessellations Robert Cupisz SIGGRAPH 2012 Graphics Gems for Games - Findings from Avalanche Studios (Just Cause 2)  Emil Persson SIGGRAPH 2012 Axis-Aligned Filtering for Interactive Sampled Soft Shadows Mehta et al. SIGGRAPH 2012 Real-Time Bidirectional Path Tracing via Rasterization Yusuke Tokuyoshi, Shinji Ogaki SIGGRAPH 2012 Physically Based Shading at Disney + Slides Brent Burley SIGGRAPH 2012 Manifold Exploration: A Markov Chain Monte Carlo Technique for Rendering Scenes with Difficult Specular Transport Wenzel Jakob, Steve Marschner SIGGRAPH 2012 Local Image-based Lighting With Parallax-corrected Cubemap S\u00e9bastien Lagarde, Antoine Zanuttini SIGGRAPH 2012 Light Transport Simulation with Vertex Connection and Merging Iliyan Georgiev, Jaroslav K\u0159iv\u00e1nek, Tom\u00e1\u0161 Davidovi\u010d, Philipp Slusallek SIGGRAPH Asia 2012 The Compact YCoCg Frame Buffer Pavlos Mavridis, Georgios Papaioannou JCGT 2012 Lighting Technology of \"The Last of Us\" Micha\u0142 Iwanicki SIGGRAPH 2013 Axis-Aligned Filtering for Interactive Physically-Based Diffuse Indirect Lighting Mehta et al. SIGGRAPH 2013 Real Shading in Unreal Engine 4 Brian Karis SIGGRAPH 2013 Crafting a Next-Gen Material Pipeline for The Order: 1886 + Slides David Neubelt, Matt Pettineo SIGGRAPH 2013 Getting More Physical in Call of Duty: Black Ops II + Slides Dimitar Lazarov SIGGRAPH 2013 CRYENGINE 3 Graphics Gems Tiago Sousa SIGGRAPH 2013 Playing with Real-Time Shadows (CRYENGINE 3) Nikolas Kasyan SIGGRAPH 2013 The Visibility Buffer: A Cache-Friendly Approach to Deferred Shading Christopher A. Burns, Warren A. Hunt JCGT 2013 Specular BRDF Reference Brian Karis Site, 2013 Tone Mapping Brian Karis Site, 2013 Lighting Killzone: Shadow Fall Michal Drobot Digital Dragons 2013 Line-Sweep Ambient Obscurance Ville Timonen ESGR 2013 In-Game and Cinematic Lighting of The Last of Us Vivian Ding GDC 2014 Efficient GPU Screen-Space Ray Tracing Morgan McGuire, Michael Mara JCGT 2014 Survey of Efficient Representations for Independent Unit Vectors Zina H. Cigolle, Sam Donow, Daniel Evangelakos, Michael Mara, Morgan McGuire JCGT 2014 Importance Sampling Microfacet-Based BSDFs using the Distribution of Visible Normals + Slides Eric Heitz, Eugene d\u2019Eon EGSR 2014 Taking Killzone Shadow Fall Image Quality Into The Next Generation Michal Valient GDC 2014 Next Generation Post Processing in Call of Duty: Advanced Warfare Jorge Jimenez SIGGRAPH 2014 High-Quality Temporal Supersampling Brian Karis SIGGRAPH 2014 Moving Frostbite to Physically Based Rendering Sebastien Lagarde, Charles de Rousiers SIGGRAPH 2014 Multi-Scale Global Illumination in Quantum Break Remedy Entertainment SIGGRAPH 2015 Stochastic Screen-Space Reflections (Stochastic SSR) Tomasz Stachowiak SIGGRAPH 2015 An Adaptive Acceleration Structure for Screen-space Ray Tracing Sven Widmer HPG 2015 Landscape creation and rendering in REDengine 3 Marcin Gollent SIGGRAPH 2015 The Real-time Volumetric Cloudscapes of Horizon: Zero Dawn Andrew Schneider SIGGRAPH 2015 Dynamic Occlusion with Signed Distance Fields Daniel Wright SIGGRAPH 2015 Advanced Lighting R&amp;D at Ready At Dawn Studios David Neubelt, Matt Pettineo SIGGRAPH 2015 Moment Shadow Mapping + Thesis Christoph Peters, Reinhard Klein i3D 2015 Horizon Occlusion for Normal Mapped Reflections Jeff Russell 2015 Specular Lobe-Aware Filtering and Upsampling for Interactive Indirect Illumination Yusuke Tokuyoshi 2015 Stochastic Light Culling + Slides Yusuke Tokuyoshi, Takahiro Harada JCGT 2016 Global Illumination in Tom Clancy\u2019s The Division Nikolay Stefanov GDC 2016 Rendering Rainbow Six Siege Jalal Eddine El Mansouri GDC 2016 Volumetric Global Illumination At Treyarch JT Hooker SIGGRAPH 2016 Blue-noise Dithered Sampling + Slides I. Georgiev, M. Fajardo SIGGRAPH 2016 Real-Time Polygonal-Light Shading with Linearly Transformed Cosines Eric Heitz et al. SIGGRAPH 2016 The devil is in the details:  idTech 666 Tiago Sousa, Jean Geffroy SIGGRAPH 2016 Real-Time Global Illumination Using Precomputed Illuminance Composition with Chrominance Compression + Thesis David Kuri, Johannes Jendersie, Thorsten Grosch JCGT 2016 Product Importance Sampling for Light Transport Path Guiding Sebastian Herholz, Oskar Elek, Ji\u0159\u00ed Vorba, Hendrik Lensch, Jaroslav K\u0159iv\u00e1nek EGSR 2016 Image-space control variates for rendering Fabrice Rousselle, Wojciech Jarosz, Jan Nov\u00e1k SIGGRAPH Asia 2016 Real-Time Global Illumination using Precomputed Light Field Probes + Slides + Thesis Morgan McGuire i3D 2017 Rendering of Call of Duty Infinite Warfare Michal Drobot Digital Dragons 2017 Precomputed Lighting in Call of Duty Infinite Warfare Micha\u0142 Iwanicki, Peter-Pike Sloan SIGGRAPH 2017 Real-time Global Illumination by Precomputed Local Reconstruction from Sparse Radiance Probes Ari Silvennoinen, Jaakko Lehtinen SIGGRAPH Asia 2017 Deep Scattering: Rendering Atmospheric Clouds with Radiance-Predicting Neural Networks Simon Kallweit, Thomas M\u00fcller, Brian McWilliams, Markus Gross, Jan Nov\u00e1k SIGGRAPH Asia 2017 Spatiotemporal Variance-Guided Filtering: Real-Time Reconstruction for Path-Traced Global Illumination Schied et al. HPG 2017 An efficient denoising algorithm for global illumination Michael Mara, Morgan McGuire, Benedikt Bitterli, Wojciech Jarosz HPG 2017 Combining Analytic Direct Illumination and Stochastic Shadows + Slides E. Heitz, S. Hill, M. McGuire i3D 2018 Progressive Multi-Jittered Sample Sequences Per Christensen, Andrew Kensler, Charlie Kilpatrick Eurographics 2018 Introduction to DirectX RayTracing Chris Wyman, Shawn Hargreaves, Peter Shirley, Colin Barr\u00e9-Brisebois SIGGRAPH 2018 A Life of a Bokeh Guillaume Abadie SIGGRAPH 2018 Integrating clipped spherical harmonics expansions Laurent Belcour, Guofu Xie, Christophe Hery, Mark Meyer, Wojciech Jarosz, Derek Nowrouzezahrai SIGGRAPH 2018 Sampling the GGX Distribution of Visible Normals Eric Heitz JCGT 2018 Importance Sampling of Many Lights with Adaptive Tree Splitting Alejandro Conty Estevez, Christopher Kulla HPG 2018 Deferred Adaptive Compute Shading Ian Mallett, Cem Yuksel HPG 2018 Gradient Estimation for Real-Time Adaptive Temporal Filtering Christoph Schied, Christoph Peters, Carsten Dachsbacher PACMCGIT 2018 Horizon-Based Indirect Lighting + Code Beno\u00eet \u201cPatapom\u201d Mayaux Site, 2018 Screen Space Cone Tracing for Glossy Reflections Lukas Hermanns Thesis, 2018 Progressive Spatiotemporal Variance-Guided Filtering Jan Dundr CESCG 2018 Exploring Ray Traced Future in Metro Exodus Shyshkovtsov, Karmalsky, Archard, Zhdan GDC 2019 It Just Works: Ray-Traced Reflections in \u2018Battlefield V\u2019 + Slides Johannes Deligiannis, Jan Schmid GDC 2019 Fast Non-uniform Radiance Probe Placement and Tracing Wang et. al. i3D 2019 Real-Time Rendering with Lighting Grid Hierarchy Daqi Lin, Cem Yuksel i3D 2019 Improved Geometric Specular Antialiasing Yusuke Tokuyoshi, Anton Kaplanyan i3D 2019 Checkerboard rendering in Dark Souls: Remastered Pursche, Vennstrom DD 2019 Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields Zander Majercik, Jean-Philippe Guertin, Derek Nowrouzezahrai, Morgan McGuire JCGT 2019 On Histogram-preserving Blending for Randomized Texture Tiling Brent Burley JCGT 2019 A Multiple-Scattering Microfacet Model for Real-Time Image Based Lighting Carmelo J. Fdez-Ag\u00fcera JCGT 2019 Real-Time Path Tracing and Denoising in Quake II Christoph Schied, Alexey Panteleev GTC 2019 Stochastic Lightcuts Cem Yuksel HPG 2019 A Low-Discrepancy Sampler that Distributes Monte Carlo Errors as a Blue Noise in Screen Space Eric Heitz et al. SIGGRAPH 2019 Neural Importance Sampling Thomas M\u00fcller, Brian McWilliams, Fabrice Rousselle, Markus Gross, Jan Nov\u00e1k SIGGRAPH 2019 Path Guiding in Production Ji\u0159\u00ed Vorba, Johannes Hanika, Sebastian Herholz, Thomas M\u00fcller, Jaroslav K\u0159iv\u00e1nek, Alexander Keller SIGGRAPH 2019 Hierarchical Russian Roulette for Vertex Connections Yusuke Tokuyoshi, Takahiro Harada SIGGRAPH 2019 Blockwise Multi-Order Feature Regression for Real-Time Path Tracing Reconstruction + Video Matias Koskela, Kalle Immonen, Markku M\u00e4kitalo, Alessandro Foi, Timo Viitanen, Pekka J\u00e4\u00e4skel\u00e4inen, Heikki Kultala, Jarmo Takala SIGGRAPH 2019 Reparameterizing discontinuous integrands for differentiable rendering Guillaume Loubet, Nicolas Holzschuch, Wenzel Jakob SIGGRAPH Asia 2019 Mitsuba 2: A Retargetable Forward and Inverse Renderer Merlin Nimier-David, Delio Vicini, Tizian Zeltner, Wenzel Jakob SIGGRAPH Asia 2019 Ray Guiding for Production Lightmap Baking Ari Silvennoinen, Peter-Pike Sloan SIGGRAPH Asia 2019 Neural Temporal Adaptive Sampling and Denoising Hasselgren, Munkberg, Salvi, Patney, Lefohn Eurographics 2020 On Ray Reordering Techniques for Faster GPU Ray Tracing Meister, Bok\u0161ansk\u00fd, Guthe, Bittner i3D 2020 Stochastic Substitute Trees for Real-Time Global Illumination + Code Wolfgang Tatzgern, Benedikt Mayr, Bernhard Kerbl, Markus Steinberger i3D 2020 Real-Time Stochastic Lightcuts Daqi Lin, Cem Yuksel i3D 2020 A Survey of Temporal Antialiasing Techniques Lei Yang, Shiqiu Liu, Marco Salvi ESGR 2020 Can\u2019t Invert the CDF? The Triangle-Cut Parameterization of the Region under the Curve Eric Heitz EGSR 2020 Temporal Sample Reuse for Next Event Estimation and Path Guiding for Real-Time Path Tracing Addis Dittebrandt, Johannes Hanika, Carsten Dachsbacher ESGR 2020 The design and evolution of the UberBake light baking system Seyb, Sloan, Silvennoinen, Iwanicki, Jarosz SIGGRAPH 2020 Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting Benedikt Bitterli, Chris Wyman, Matt Pharr, Peter Shirley, Aaron Lefohn, Wojciech Jarosz SIGGRAPH 2020 Advances in Monte Carlo Rendering: The Legacy of Jaroslav Krivanek Keller et al. SIGGRAPH 2020 Practical Product Sampling by Fitting and Composing Warps Hart et al. SIGGRAPH 2020 Introduction to the Vulkan Computer Graphics API Mike Bailey SIGGRAPH 2020 Specular Manifold Sampling for Rendering High-Frequency Caustics and Glints Tizian Zeltner, Iliyan Georgiev, Wenzel Jakob SIGGRAPH 2020 Some Thoughts on the Fresnel Term Naty Hoffman SIGGRAPH 2020 Continuous Multiple Importance Sampling Rex West, Iliyan Georgiev, Adrien Gruson, Toshiya Hachisuka SIGGRAPH 2020 Online path sampling control with progressive spatio-temporal filtering Jacopo Pantaleoni ArXiv 2020 Progressive Least-Squares Encoding for Linear Bases Thomas Roughton JCGT 2020 Surface Gradient\u2013Based Bump Mapping Framework Morten S. Mikkelsen JCGT 2020 Raytraced Shadows in Call of Duty: Modern Warfare Michal Olejnik, Pawel Kozlowski DD 2020 Creating EA's Next Generation Hair Technology Jon Valdes, Robin Taillandier DD 2020 Perceptual Error Optimization for Monte Carlo Rendering Vassillen Chizhov, Iliyan Georgiev, Karol Myszkowski, Gurprit Singh ArXiv 2020 Ray-Traced Glossy Reflections Using Screen-Space Roughness Peter Kristof GTC 2020 Fast Denoising with Self Stabilizing Recurrent Blurs (Slides) Dmitry Zhdan GTC 2020 Neural Control Variates Thomas M\u00fcller, Fabrice Rousselle, Alexander Keller, Jan Nov\u00e1k SIGGRAPH Asia 2020 Glossy Probe Reprojection for Interactive Global Illumination Simon Rodriguez, Thomas Leimk\u00fchler, Siddhant Prakash, Chris Wyman, Peter Shirley, George Drettakis SIGGRAPH Asia 2020 NeRF: Neural Radiance Fields Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng ECCV 2020 High-Performance Image Filters via Sparse Approximations Kersten Schuster, Philip Trettner, Leif Kobbelt PACMGIT 2020 Crash Course in BRDF Implementation Jakub Boksansky 2021 A Survey on Bounding Volume Hierarchies for Ray Tracing Daniel Meister, Shinji Ogaki, Carsten Benthin, Michael J. Doyle, Michael Guthe, Ji\u0159\u00ed Bittner Eurographics 2021 Temporally Reliable Motion Vectors for Real-time Ray Tracing + Slides + Video Zheng Zeng, Shiqiu (Edward) Liu, Jinglei Yang, Lu Wang, Ling-Qi Yan Eurographics 2021 Correlation-aware multiple importance sampling for bidirectional rendering algorithms Pascal Grittmann, Iliyan Georgiev, Philipp Slusallek Eurographics 2021 Improved Shader and Texture Level of Detail Using Ray Cones Tomas Akenine-M\u00f6ller, Cyril Crassin, Jakub Boksansky, Laurent Belcour, Alexey Panteleev, Oli Wright JCGT 2021 Scaling Probe-Based Real-Time Dynamic Global Illumination for Production Zander Majercik, Adam Marrs, Josef Spjut, Morgan McGuire JCGT 2021 Stable Geometric Specular Antialiasing with Projected-Space NDF Filtering Yusuke Tokuyoshi, Anton Kaplanyan JCGT 2021 Fast temporal reprojection without motion vectors Johannes Hanika, Lorenzo Tessari, Carsten Dachsbacher JCGT 2021 Neural Radiosity Saeed Hadadan, Shuhong Chen, Matthias Zwicker ArXiv 2021 Dynamic Diffuse Global Illumination Resampling Zander Majercik, Thomas M\u00fcller, Alexander Keller, Derek Nowrouzezahrai, Morgan McGuire ArXiv 2021 Plenoxels: Radiance Fields without Neural Networks Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa ArXiv 2021 Rearchitecting Spatiotemporal Resampling for Production Chris Wyman, Alexey Panteleev HPG 2021 ReSTIR GI: Path Resampling for Real-Time Path Tracing Yaobin Ouyang, Shiqiu Liu, Markus Kettunen, Matt Pharr, Jacopo Pantaleoni HPG 2021 Monte Carlo Estimators for Differential Light Transport Tizian Zeltner, S\u00e9bastien Speierer, Iliyan Georgiev, Wenzel Jakob SIGGRAPH 2021 BRDF Importance Sampling for Polygonal Lights Christoph Peters SIGGRAPH 2021 Unbiased VNDF Sampling for Backfacing Shading Normals Yusuke Tokuyoshi SIGGRAPH 2021 Reliable Feature-Line Driven Quad-Remeshing Nico Pietroni, Stefano Nuvoli, Thomas Alderighi, Paolo Cignoni, Marco Tarini SIGGRAPH 2021 Real-time Neural Radiance Caching for Path Tracing Thomas M\u00fcller, Fabrice Rousselle, Jan Nov\u00e1k, Alexander Keller SIGGRAPH 2021 Improved Spatial Upscaling through FidelityFX Super Resolution for Real-Time Game Engines Timothy Lottes, Kleber Garcia SIGGRAPH 2021 Experimenting With Concurrent Binary Trees for Large-scale Terrain Rendering Thomas Deliot, Jonathan Dupuy, Kees Rijnen, Xiaoling Yao SIGGRAPH 2021 Large-Scale Global Illumination at Activision Ari Silvennoinen SIGGRAPH 2021 Real-Time Samurai Cinema: Lighting, Atmosphere, and Tonemapping in Ghost of Tsushima Jasmin Patry SIGGRAPH 2021 A Deep Dive into Nanite Virtualized Geometry Brian Karis, Rune Stubbe, Graham Wihlidal SIGGRAPH 2021 Radiance Caching for Real-Time Global Illumination Daniel Wright SIGGRAPH 2021 Global Illumination Based on Surfels Andreas Brinck, Xiangshun Bei, Henrik Halen, Kyle Hayward SIGGRAPH 2021 Unity Rendering Architecture Sebastian Aaltonen, Timothy Cooper, Natalya Tatarchuk SIGGRAPH 2021 Geometry Rendering Pipeline Architecture at Activision Michal Drobot SIGGRAPH 2021 Hierarchical Neural Reconstruction for Path Guiding Using Hybrid Path and Photon Samples Shilin Zhu, Zexiang Xu, Tiancheng Sun, Alexandr Kuznetsov, Mark Meyer, Henrik Wann Jensen, Hao Su, Ravi Ramamoorthi SIGGRAPH 2021 Character Locomotion in Half-Life: Alyx + Video Joe van den Heuvel SIGGRAPH 2021 Moving Basis Decomposition for Precomputed Light Transport Ari Silvennoinen, Peter-Pike Sloan EGSR 2021 Fast Volume Rendering with Spatiotemporal Reservoir Resampling  Daqi Lin, Chris Wyman, Cem Yuksel SIGGRAPH Asia 2021 Tiled Reservoir Sampling for Many-Light Rendering Yusuke Tokuyoshi 2021 Instant Neural Graphics Primitives with a Multiresolution Hash Encoding Thomas M\u00fcller, Alex Evans, Christoph Schied, Alexander Keller ArXiv 2022"},{"location":"rendering/LPE/","title":"LPE","text":""},{"location":"rendering/LPE/#introduction-to-light-path-expressions","title":"Introduction to Light Path Expressions","text":"<p>Light Path Expressions (LPEs) are useful for outputting light into specific AOVs. LPEs describe the transport of light through the scene, starting from a source of light, bouncing between objects, and finally ending up at the camera. LPEs can be used to extract specific light contributions from Arnold into separate built-in or custom AOVs which can be output and recombined in various ways in a compositing package.</p> <p>A light path expression (LPE) is a type of regular expression that describes a specific light path (or set of paths) based on the scattering events. The expression can be used to \u201cselect\u201d the contributions of those kinds of paths to the output. You can use this to isolate very specific parts of the output when compositing, without having to write a custom shader or render that part as a separate pass.</p> <p>Light Path Expressions or LPEs are a very powerful and advanced tool for extracting specific lighting events from the scene to a separate channel. This allows for a very fine control of the image in compositing.</p> <p>For example, LPEs allow breaking the GI down to different types of light sources like lights, the environment and self-illuminating objects, or breaking down the GI to its separate bounces, or capturing only self-reflections, or the SSS that's only seen behind glass and similar for compositing control of only this aspect of the image.</p> <p>Light Path Expressions or LPEs are regular expressions describing light transport paths. They define paths initiated from camera that are bouncing all around the scene until they reach light sources. LPEs are extremely powerful as it is a mean to extract specific paths to output their results in custom AOVs without of relying on specific AOVs predefined by materials. This is extremely useful to extract all per light AOVs to relight in compositing for example or to output specific AOVs that are not available by materials.</p> <p>A light path expression is a regular expression representing the different vertices of a path between the camera and a light. Guerilla comes with many presets, but you can create your own expression.</p> <p>For example, CDL matches the direct lighting on a diffuse surface: a path of three events, each represented by a capital letter: Camera, Diffuse and a Light.</p> <p>This section is a formal definition of the light path expression language. Ones may be interested by the the default expressions available in Guerilla</p> <p>from nvidia iray docs</p> <p>1Light path expressions 1.1Introduction Light path expressions (LPEs) describe the propagation of light through a scene, for example starting from a source of light, bouncing around between the objects of the scene and ultimately ending up at the eye. The paths that light takes through a scene are called light transport paths. LPEs may be used for example, to extract only specific light contributions from a renderer into separate image buffers.</p> <p>Light path expressions were first proposed as a description of light transport by Paul Heckbert [Heckbert90]. Heckbert suggested that regular expressions \u2014 typically used to describe patterns of characters in text \u2014 could also describe light transport events and paths. The alphabet of LPEs consists of event descriptions, that is, of interactions between light particles and the scene.</p> <p>1.2Events Each event of a path, that is, each interaction of light with the scene objects and materials, is described by its type (for example, emission or reflection), the mode of scattering (for example, diffuse or specular), and an optional handle.</p> <p>A full event is described as &lt; t m h &gt;, where</p> <p>t is the event type, either R (reflection), T (transmission), or V (volume interaction), m is the scattering mode, either D (diffuse), G (glossy), or S (specular), and h is a handle in single quotes, for example. 'foo'. This position may be omitted from the specification. In that case, any handle is accepted. See below for details. Spaces are ignored unless they occur inside a handle string. The dot character (.) may be used as a wildcard in any position. It accepts any valid input. For example, a diffuse reflection event may be specified as , or, omitting the handle, . A specular transmission event identified with the handle \"window\" may be specified as .</p> <p>1.3Handles Handles are strings of ASCII characters, enclosed in single quotes ('). The following characters must be escaped by prefixing them with a backslash inside handles: , ', and \". The assignment of a handle as a name for a scene element is typically made possible through the graphical interface of an application.</p> <p>1.4Sets and exclusion As an alternative to the type, mode, and handle specifiers described above, each position of the event triple may contain a character set enclosed in square brackets. Any element of the set will be accepted. For example, &lt;[RT]..&gt; matches all reflection events and all transmission events.</p> <p>The complementary set is specified by first including the caret (^) character. For example, &lt;.[^S]&gt; matches any non-specular event and &lt;..[^'ground']&gt; matches any event that is not identified with the handle \"ground\".</p> <p>Event sets also work on full events. For instance, [] matches glossy reflection and specular transmission events. Note that this is different from &lt;[RT][GS]&gt;, which accepts glossy transmission and specular reflection in addition to the events accepted by the previous expression.</p> <p>1.5Abbreviations In order to make specification of LPEs simpler, event descriptions may be abbreviated. An event in which only one of type, mode, or handle is explicitly specified may be replaced by that item. For example,  may be abbreviated as R. Likewise, &lt;..'foo'&gt; may be abbreviated as 'foo'. Note the difference between , which accepts a single specular transmission event, and TS, which accepts an arbitrary transmission event followed by an arbitrary specular event. Finally, . matches any event except the special events described below.</p> <p>Abbreviation rules also apply to event sets, that is, [&lt;.S.&gt;] reduces to [TS]. Again note that this is different from TS (without brackets).</p> <p>1.6Constructing expressions LPEs may be constructed by combining event specifications through concatenation, alternation, and quantification. The following operators are supported and have the same semantics as they would for standard regular expressions. For expressions A and B and integers n and m with m &gt;= n:</p> <p>AB  Accepts first A, then B A|B Accepts A or B A?  Optionally accepts A, that is, A may or may not be present A*  Accepts any number of occurrences of A in sequence, including zero times A+  Accepts any non-empty sequence of A's. It is equivalent to AA* A{n}    Accepts exactly n consecutive occurrences of A. For example, A{3} is equivalent to AAA. A{n,m}  Accepts from n to m, inclusively, occurrences of A A{n,}   Equivalent to A{n}A* The precedence from high to low is quantifiers (?, *, +, {}), concatenation, alternatives. Items can be grouped using normal parentheses ( and ).</p> <p>1.7Special events Each LPE is delimited by special events for the eye (or camera) and light vertices. These events serve as markers and must be the first and last symbols in a LPE.</p> <p>LPEs may be specified starting either at the light or at the eye. All expressions must be constructed in such a way that every possible match has exactly one eye and one light vertex. This is due to the nature of LPEs: They describe light transport paths between one light and the eye. Note that this does not mean that light and eye markers must each show up exactly once. For example, \"E (D La | G Le)\" is correct, because either alternative has exactly one light and one eye marker: \"E D La\" and \"E G Le\". On the other hand \"E D La?\", \"E (D | La)\", and \"E (D | La) Le\" are ill-formed, because they would match paths with zero or two light markers.</p> <p>In the abbreviated form, the eye marker is simply E and the light marker is L. These items are special in that they represent two distinct characteristics: the shape of the light (or camera lens) and the mode of the emission distribution function. The full notation therefore differs from that of the standard events.</p> <p>In the full form, a light source as the first vertex of a transport path is described as &lt; L h m h &gt;, where L is the light type, and m and h are as before. The first pair of type and handle describes the light source itself. The type L can be one of Lp (point shape), La (area light), Le (environment or background), Lm (matte lookup), or L (any type). In the case of alpha expressions, Lms (matte shadows) is supported in addition to the aforementioned types. The second pair describes the light's directional characteristics, that is, its EDF. (This form loosely corresponds to the full-path notation introduced by Eric Veach in [Veach97], Section 8.3.2.)</p> <p>As before, the handles are optional. Furthermore, the EDF specification may be omitted. Thus,  is equivalent to  and La.</p> <p>Especially when dealing with irradiance (rather than radiance) render targets, it is convenient to use a special form of LPE, called irradiance expression. Such expressions contain an irradiance marker, rather than an eye marker. Using this marker, it is possible to describe light transport up to the point of the irradiance measurement, rather than up to the camera.</p> <p>The full form of the irradiance marker is , the abbreviated form is simply I. As before, h represents an optional handle. If set, irradiance will only be computed on those surfaces that have a matching handle.</p> <p>1.8Advanced operations Several operations exist in order to make specifying the right expression easier. These operations do not add expressive power, but simplify certain tasks.</p> <p>When an application provides a means for multiple output images (or canvases) to be rendered at the same time, expressions may be re-used in subsequent canvas names. This is achieved by assigning a name to the expressions that should be re-used, for example:</p> <p>caustics: L.*SDE LE | $caustics In this example, the second canvas will receive both caustics and directly visible light sources. As illustrated above, variables are introduced by specifying the desired name, followed by a colon and the desired expression. Variable names may contain any positive number of alphanumeric characters and the underscore character, with the limitation that they may not start with a terminal symbol. Since all terminals of the LPE grammar start with capital letters, it is good practice to start variable names with lowercase letters. Note that sub-expressions cannot be captured by variable names.</p> <p>Variables are referenced by applying the dollar (or value-of) operator to the variable name.</p> <p>Expressions may be turned into their complement by prefixing them with the caret symbol. An expression of type ^A will yield all light transport paths that are not matched by A. Note that the complement operator cannot be applied to sub-expressions: \"^(L.E)\" is valid, but \"L^(.)E\" is not.</p> <p>It is possible to compute the intersection of two or more expressions by combining them with the ampersand symbol. Expressions of type A &amp; B will match only those paths which are matched by both A and B.</p> <p>1.9Matte object interaction The color of matte objects is determined by two types of interaction. The first is the lookup into the environment or backplate. This contribution is potentially shadowed by other objects in the scene and may be selected by expressions ending in Lm. Selection of such contributions can be further refined by specifying the handle of the matte object, for example, .</p> <p>The second type of contributions is made up of effects that synthetic objects and lights have on matte objects. This includes effects like synthetic lights illuminating matte objects and synthetic objects reflected by matte objects. For these contributions, matte objects behave exactly like synthetic objects.</p> <p>With regards to LPEs, matte lights illuminating synthetic objects behave exactly as if they were synthetic lights.</p> <p>1.10LPEs for alpha control Some additional considerations are necessary when using LPEs to control alpha output.</p> <p>By definition, alpha is transparent (alpha 0) only for paths that match the provided expression. Note that this means that light transport paths which do not reach a light source or the environment because they are terminated prematurely (for whatever reason) are opaque. This is necessary to avoid undesired semi-transparent areas in the standard case.</p> <p>This has implications for the creation of object masks. Since the mask is supposed to be completely transparent also when undesired objects are hit by camera rays, these paths have to be captured by the expression even if they are terminated. This requires a special type of LPE, that is, one that captures terminated paths. Such LPEs are only allowed for alpha channels. For example, the expression \"E ([^'crate'] .*)? L?\" will render a mask for the object 'crate'.</p> <p>Shadows received by matte objects may also affect the opacity of the alpha channel. This opacity can be removed by capturing paths which end in Lms.\u203b Adding opacity in areas of matte shadow to the previously shown mask expression may be achieved by slightly changing the LPE to \"E ([^'crate'] .*) L? | E [^Lms]\".</p> <p>Note that presence or absence of Lms controls whether shadows which are received by a certain matte object make the alpha channel opaque. This affects all matte shadow, regardless of how it was cast, and by which objects.</p> <p>The API provides functions mi::neuraylib::IRendering_configuration::make_alpha_expression() and mi::neuraylib::IRendering_configuration::make_alpha_mask_expression() to generate various common alpha expressions, including masks.</p> <p>1.11Example LPEs The universal light path expression, \"L .* E\", accepts all light transport paths. By default, this expression yields the same result as not using LPEs at all. Remember that this is equivalent to \"L.E\" (whitespace is ignored) and \"E.L\" (the expression can be reversed).</p> <p>Direct illumination can be requested by specifying \"L .? E\", or \"L . E\" if directly visible light sources are not desired. Indirect illumination is then specified by \"L .{2,} E\".</p> <p>Compositing workflows often use the concept of diffuse and reflection passes. They can be specified with the LPEs \"E  L\" and \"E  L\", respectively. Note that these passes as specified above do not contain indirect illumination. \"E  .* L\" extends this to global illumination where the visible surfaces are diffuse. If only diffuse interactions are desired, \"E * L\" can be used.</p> <p>Caustics are usually described as \"E D S .* L\". The expression \"E D (S|G) .* L\" or \"E D [GS] .* L\" also considers glossy objects as caustics casters. If, for example, only specular reflection caustics cast by an object identified with a handle \"crate\" are desired, the expression is changed to \"E D  .* L\". This can further be restricted to caustics cast onto \"ground\" from a spot light by changing the expression to \"E 'ground'  .* \".</p> <p>Assuming an expression variable called caustics was defined in a previous expression, \"L.{2,5}E &amp; ^$caustics\" will match any path that has the specified length and is not a caustic.</p> <p>1.12Summary of typical LPEs Typical production workflow structures in digital compositing often employ a set of standard elements that can be represented by light path expressions. The following expressions define the color (RGB) component of rendering:</p> <p>LPE Description E D .* L    Diffuse pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a diffuse event. E G .* L    Glossy pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a glossy event. E S .* L    Specular pass commonly used in conventional compositing workflows. The last event on the light path before the eye was a specular event, that is, a mirror reflection or specular refraction. E D S .* L  Diffuse part of caustics cast by a mirror reflection or specular refraction on another surface. E .*    All direct and indirect light contribution coming from the key light group, which are all lights in the scene with the handle attribute set to key. E'crate'.*L All direct and indirect light falling onto any object in the scene with the handle attribute set to crate. The alpha channel can also be specified by light path expressions:</p> <p>LPE Description E [LmLe]    Alpha is based solely on primary visibility. This is the approach used traditionally by many renderers. ET[LmLe]   Transmitting objects make the alpha channel transparent. This is the default behavior. E  [LmLe]  Only specular transmission makes the alpha channel transparent. This avoids unexpected results in scenes with materials that have a diffuse transmission component. The graphical interface of an application may provide a way of naming and storing LPEs for reuse. Common LPEs like the above may also be part of a standard set of named LPEs in an application interface.</p> <p>1.13Light path expression grammar L   light E   eye R   reflection type T   transmission type V   volume interaction type D   diffuse mode G   glossy mode S   specular mode 'h' handle h &lt; type mode handle &gt;    event Lp  point light type La  area light type Le  environment or background light type Lm  matte lookup type Lms shadows cast onto matte objects (alpha expressions only) &lt; light-type light-handle mode handle &gt; light source full form &lt; I h &gt; irradiance marker type    abbreviation for &lt; type ..&gt; mode    abbreviation for &lt;. mode .&gt; handle  abbreviation for &lt;.. handle &gt; I   abbreviation for  .   match anything (in context) [ A \u2026 ]   match any element in set [ ^A ]  match all but A AB  A followed by B A|B A or B A?  zero or one A A*  zero or more As A+  one or more As A{n}    a sequence of n As A{n,m}  n to m occurences of A A{n,}   equivalent to A{n}A* ( \u2026 ) grouping ^ expression    complement of expression expression-1 &amp; expression-2 match both expressions name: expression    assign expression to name $name   use value of name</p>"},{"location":"rendering/Lexique/","title":"Lexique","text":""},{"location":"rendering/Lexique/#rendering","title":"Rendering","text":""},{"location":"rendering/Lexique/#irradiance","title":"Irradiance","text":"<p>L'irradiance est une mesure de la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9. Elle est g\u00e9n\u00e9ralement exprim\u00e9e en watts par m\u00e8tre carr\u00e9 (W/m\u00b2) et peut \u00eatre mesur\u00e9e \u00e0 l'aide d'un radiom\u00e8tre ou d'un spectrophotom\u00e8tre. L'irradiance est diff\u00e9rente de l'intensit\u00e9 lumineuse, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par une source lumineuse en un temps donn\u00e9 et est exprim\u00e9e en candelas (cd). L'irradiance est \u00e9galement diff\u00e9rente de l'illuminance, qui mesure la quantit\u00e9 de lumi\u00e8re qui atteint une surface en un temps donn\u00e9 et est exprim\u00e9e en lux (lx).</p>"},{"location":"rendering/Lexique/#radiance","title":"Radiance","text":"<p>La radiance est une mesure de la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par un corps en un temps donn\u00e9, per\u00e7ue dans une direction donn\u00e9e. Elle est g\u00e9n\u00e9ralement exprim\u00e9e en watts par m\u00e8tre carr\u00e9 par steradian (W/m\u00b2/sr) et peut \u00eatre mesur\u00e9e \u00e0 l'aide d'un radiom\u00e8tre ou d'un spectrophotom\u00e8tre. La radiance est diff\u00e9rente de l'intensit\u00e9 lumineuse, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par une source lumineuse en un temps donn\u00e9 et est exprim\u00e9e en candelas (cd). La radiance est \u00e9galement diff\u00e9rente de l'irradiance, qui mesure la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9 et est exprim\u00e9e en watts par m\u00e8tre carr\u00e9 (W/m\u00b2). La radiance est un concept important en optique, en astrophysique et en m\u00e9t\u00e9orologie, car elle permet de quantifier la quantit\u00e9 d'\u00e9nergie lumineuse \u00e9mise par des corps c\u00e9lestes, des nuages, des atmosph\u00e8res, etc.</p>"},{"location":"rendering/Lexique/#flux","title":"Flux","text":"<p>Le flux est une mesure de la quantit\u00e9 d'\u00e9nergie, de mati\u00e8re, de particules ou d'autres grandeurs qui traversent une surface en un temps donn\u00e9. Le flux est g\u00e9n\u00e9ralement exprim\u00e9 en unit\u00e9s de mesure appropri\u00e9es (par exemple, en watts pour l'\u00e9nergie, en kilogrammes pour la mati\u00e8re, en particules par seconde pour les particules, etc.) et peut \u00eatre mesur\u00e9 \u00e0 l'aide d'un instrument appropri\u00e9 (comme un wattm\u00e8tre pour l'\u00e9nergie, une balance pour la mati\u00e8re, un compteur de particules pour les particules, etc.). Le flux peut \u00eatre un concept important en physique, en chimie, en biologie, en ing\u00e9nierie, en m\u00e9t\u00e9orologie, etc., car il permet de quantifier la quantit\u00e9 de diff\u00e9rentes grandeurs qui traversent une surface en un temps donn\u00e9. Par exemple, le flux solaire mesure la quantit\u00e9 d'\u00e9nergie lumineuse qui traverse une surface en un temps donn\u00e9, le flux de chaleur mesure la quantit\u00e9 de chaleur qui traverse une surface en un temps donn\u00e9, le flux de particules radioactive mesure la quantit\u00e9 de particules radioactive qui traversent une surface en un temps donn\u00e9, etc.</p>"},{"location":"rendering/OSL_samples/","title":"OSLSamples","text":""},{"location":"rendering/OSL_samples/#simple-skin-shader","title":"Simple Skin shader","text":"<pre><code>shader my_shader(\n    vector I,\n    vector N,\n    vector T,\n    float sigma_a,\n    float sigma_s,\n    float g,\n    float roughness,\n    color albedo = color(1, 1, 1),\n    float ior = 1.4,\n    float multiscattering = 0.5,\n    float melanin = 1.0,\n    float subsurface_scale = 1.0,\n    float specular_scale = 1.0,\n    float diffuse_scale = 1.0,\n    float melanin_scale = 1.0,\n    float scattering_model = 0,\n    color sigma_prime_s_coeff = color(0.74, 0.88, 1.01),\n    color sigma_prime_a_coeff = color(0.0014, 0.0025, 0.0142),\n    output color result = color(0, 0, 0)\n)\n{\n    // Calculate the halfway vector\n    vector H = normalize(I + N);\n\n    // Calculate the diffuse term\n    color diffuse = albedo / M_PI;\n\n    // Calculate the specular term\n    color specular = pow(max(dot(H, N), 0.0), 5);\n\n    // Calculate the Beckmann term\n    color beckmann = exp(-pow(dot(N, H), 2) / (pow(tan(acos(dot(N, H))), 2) * pow(roughness, 2))) / (M_PI * pow(roughness, 2) * pow(dot(N, H), 4));\n\n    // Calculate the absorption term\n    color absorption = exp(-2 * (sigma_a + sigma_s) * dot(N, H));\n\n    // Calculate the reduced scattering term\n    color reduced_scattering = (1 - g) / (1 + (1 - g) * pow(dot(N, H), 2));\n\n    // Calculate the phase function term\n    color phase_function = 0.25 / M_PI * (1 + pow(dot(N, H), 2));\n\n    // Calculate the Fresnel term\n    color fresnel = (1 - pow(ior, 2)) / (1 + pow(ior, 2) - 2 * ior * dot(I, H));\n\n    // Calculate the melanin term\n    color melanin_term = melanin * (sigma_a + sigma_s);\n\n    // Calculate the spectral model for skin scattering\n    color spectral_model = 0;\n    if (scattering_model == 0)\n    {\n        // Calculate the spectral model using the skin_scattering function\n        spectral_model = skin_scattering(N, H, sigma_prime_s_coeff, sigma_prime_a_coeff);\n    }\n    else if (scattering_model == 1)\n    {\n        // Calculate the spectral model using the Henyey-Greenstein model\n        spectral_model = henyey_greenstein(N, H, g);\n    }\n    else if (scattering_model == 2)\n    {\n        // Calculate the spectral model using the Schlick model\n        spectral_model = schlick(N, H, g);\n    }\n\n    // Calculate the multiple scattering term\n    color multiple_scattering = exp(-multiscattering * (sigma_a + sigma_s));\n\n    // Calculate the final color\n    result = (diffuse + specular * beckmann * absorption * reduced_scattering * phase_function * fresnel * spectral_model * multiple_scattering) / sigma_t;\n\n    // Scale the diffuse, specular, subsurface, and melanin contributions\n    result *= diffuse_scale;\n    result *= specular_scale;\n    result += melanin_term * melanin_scale;\n    result += melanin_term * subsurface_scale;\n\n    // Calculate the tangent space basis vectors\n    vector U = normalize(T - dot(T, N) * N);\n    vector V = normalize(cross(N, U));\n\n    // Calculate the tangent space direction of the incident light\n    vector I_tangent = vector(dot(I, U), dot(I, V), dot(I, N));\n\n    // Calculate the BRDF in tangent space\n    color brdf_tangent = (diffuse + specular * beckmann * absorption * reduced_scattering * phase_function * fresnel * spectral_model * multiple_scattering) / sigma_t;\n\n    // Transform the BRDF back to world space\n    result = color(\n        dot(brdf_tangent, vector(U.x, V.x, N.x)),\n        dot(brdf_tangent, vector(U.y, V.y, N.y)),\n        dot(brdf_tangent, vector(U.z, V.z, N.z))\n    );\n}\n\n// Function to calculate the spectral model for skin scattering\ncolor skin_scattering(vector N, vector H, color sigma_prime_s_coeff, color sigma_prime_a_coeff)\n{\n    // Calculate the skin scattering coefficients\n    color sigma_prime_s = sigma_prime_s_coeff * (1 + 0.01 * pow(dot(N, H), 2));\n    color sigma_prime_a = sigma_prime_a_coeff * (1 + 0.0001 * pow(dot(N, H), 2));\n\n    // Calculate the reduced scattering term\n    color reduced_scattering = (1 - g) / (1 + (1 - g) * pow(dot(N, H), 2));\n\n    // Calculate the spectral model for skin scattering\n    return sigma_prime_s / (sigma_prime_s + sigma_prime_a);\n}\n\n// Function to calculate the spectral model using the Henyey-Greenstein model\ncolor henyey_greenstein(vector N, vector H, float g)\n{\n    return 1 / (4 * M_PI) * (1 - pow(g, 2)) / pow(1 + pow(g, 2) - 2 * g * dot(N, H), 1.5);\n}\n\n// Function to calculate the spectral model using the Schlick model\ncolor schlick(vector N, vector H, float g)\n{\n    return 1 / (4 * M_PI) * (1 + pow(g, 2)) / pow(1 + pow(g, 2) - 2 * g * dot(N, H), 1.5);\n}\n</code></pre> <p>In this updated implementation, the scattering_model parameter allows the user to choose between three different models for calculating the spectral model: the default skin scattering model, the Henyey-Greenstein model, or the Schlick model.</p> <p>The sigma_prime_s_coeff and sigma_prime_a_coeff parameters allow the user to adjust the coefficients for the sigma prime s and sigma prime a values in the skin scattering model. </p> <p>Additionally, the multiscattering parameter allows the user to adjust the amount of multiple scattering applied to the skin, and the subsurface_scale, specular_scale, diffuse_scale, and melanin_scale parameters allow the user to adjust the relative contributions of the subsurface, specular, diffuse, and melanin terms in the final color calculation.</p> <p>A Spectral BSSRDF for Shading Human Skin\" and \"Practical and Controllable Subsurface Scattering for Production Path Tracing\" </p>"},{"location":"rendering/OSL_samples/#probability-shader","title":"Probability shader","text":"<pre><code>shader my_shader(\n    output color result = color(0, 0, 0)\n)\n{\n    // Use the rand() function to generate a random value between 0 and 1\n    float random = rand();\n\n    // Use the random value to determine the output color\n    if (random &lt; 0.33) {\n        // Output red\n        result = color(1, 0, 0);\n    } else if (random &lt; 0.66) {\n        // Output green\n        result = color(0, 1, 0);\n    } else {\n        // Output blue\n        result = color(0, 0, 1);\n    }\n}\n</code></pre>"},{"location":"rendering/Raytracing101/","title":"Raytracing","text":"<p>:::caution</p> <p>\u3053\u306e\u30b5\u30a4\u30c8\u306f\u3001mkdocs \u3092\u4f7f\u7528\u3057\u3066\u4f5c\u6210\u3057\u3066\u3044\u305f\u3082\u306e\u304b\u3089 Docusaurus \u3092\u4f7f\u7528\u3057\u305f\u3082\u306e\u306b\u4ee5\u964d\u4e2d\u3067\u3059\u3002 \u8a18\u4e8b\u306e\u79fb\u690d\u306f\u968f\u6642\u5bfe\u5fdc\u4e2d\u3067\u3059\u3002</p>"},{"location":"vex/Attributes_cheatsheet/","title":"Vex CheatSheet","text":""},{"location":"vex/Attributes_cheatsheet/#vex-cheatsheet","title":"VEX cheatsheet","text":"<code>f@Frame</code> The current floating frame number, equivalent to the $FF Hscript variable <code>f@Time</code> The current time in seconds, equivalent to the $T Hscript variable <code>i@SimFrame</code> The integer simulation timestep number ($SF), only present in DOP contexts <code>f@SimTime</code> The simulation time in seconds ($ST), only present in DOP contexts <code>f@TimeInc</code> The timestep currently being used for simulation or playback - - Attribute Wrangle <code>v@P</code> The position of the current element <code>i@ptnum</code> The point number attached to the currently processed element <code>i@vtxnum</code> The linear number of the currently processed vertex <code>i@primnum</code> The primitive number attached to the currently processed element <code>i@elemnum</code> The index number of the currently processed element <code>i@numpt</code> The total number of points in the geometry <code>i@numvtx</code> The number of vertices in the primitive of the currently processed element <code>i@numprim</code> The total number of primitives in the geometry <code>i@numelem</code> The total number of elements being processed - - Volume Wrangle <code>v@P</code> The position of the current voxel <code>f@density</code> The value of the density field at the current voxel location <code>v@center</code> The center of the current volume <code>v@dPdx</code>, <code>v@dPdy</code>, <code>v@dPdz</code> These vectors store the change in P that occurs in the x, y, and z voxel indices <code>i@ix</code>, <code>i@iy</code>, <code>i@iz</code> Voxel indices For dense volumes (non-VDB) these range from 0 to resolution-1 <code>i@resx</code>, <code>i@resy</code>, <code>i@resz</code> The resolution of the current volume - - Common Geometry Attributes <code>i@id</code> A unique indexing number that remains the same throughout time  Used to match elements between frames <code>f@pscale</code> Particle radius size  Uniform scale  Set display particles as 'Discs' to visualize <code>f@width</code> Thickness of curves  Enable 'Shade Open Curves In Viewport' on the object node to visualize <code>f@Alpha</code> Alpha transparency override  The viewport uses this to set the alpha of OpenGL geometry <code>f@Pw</code> Spline weight  Mostly depreciated at this point <code>v@P</code> Point position  Used this to lay out points in 3D space <code>v@Cd</code> Diffuse color override  The viewport uses this to color OpenGL geometry <code>v@N</code> Surface or curve normal  Houdini will compute the normal if this attribute does not exist <code>v@scale</code> Vector scale  Allows directional scaling or stretching (in one direction) <code>v@rest</code> Used by procedural patterns and textures to stick on deforming and animated surfaces <code>v@up</code> Up vector  The up direction for local space, typically (0, 1, 0) <code>v@uv</code> UV texture coordinates for this point/vertex <code>v@v</code> Point velocity  The direction and speed of movement in units per second <code>p@orient</code> The local orientation of the point (represented as a quaternion) <code>p@rot</code> Additional rotation to be applied after orient, N, and up attributes <code>s@name</code> A unique name identifying which primitives belong to which piece  Also used to label volumes <code>s@instance</code> Path of an object node to be instanced at render time - - DOP Particle Attributes <code>f@age</code> Time in seconds since the particle was born <code>f@life</code> Time in seconds the particle is allowed to live When f@age&gt;f@life, i@dead will be set to 1 <code>f@nage</code> Normalized age, f@age divided by f@life  Implicit attribute, you cannot write to this <code>i@dead</code> Whether a particle is living (0) or dead (1)  A dead particle is deleted in the Reaping stage <code>i@id</code> A unique id for the particle that remains the same throughout a single simulation <code>i@stopped</code> Whether a particle is moving (0) or stopped (1) <code>i@stuck</code> Whether a particle is free (0) or stuck (1) <code>i@sliding</code> Whether a particle is free (0) or sliding along a surface (1) <code>f@cling</code> Force applied to sliding paritcles inwards (according to the collision's surface normal) <code>s@pospath</code> The path to the object that the particle is colliding with <code>i@posprim</code> Which collision primitive in the path geometry whose position we wish to refer to <code>v@posuv</code> Parametric uv on the collision primitive <code>i@hittotal</code> The cumulative total of all hits for the particle (only incremented once per timestep) <code>i@has_pprevious</code> This is set to 1 if v@pprevious contains valid values <code>v@pprevious</code> Stores the position of the particle on the previous frame  Used for collision detection <code>i@hitnum</code> The number of times the particle collided in the last POP Collision Detect <code>s@hitpath</code> The path to the object that was hit A path to a file on disk or an op: path <code>i@hitprim</code> The primitive hit Could be -1 if it the collision detector couldn\u2019t figure out which prim <code>v@hituv</code> The parametric UV space on the primitive <code>v@hitpos</code> Where the hit actually occurred  Useful if the colliding object was moving <code>v@hitnml</code> The normal of the surface at the time of the collision <code>v@hitv</code> The velocity of the surface at the time of the collision <code>f@hittime</code> When the collision occurred, that could be within a frame <code>f@hitimpulse</code> Records how much of an impulse was needed for the collision resolution  varies with timestep <code>f@bounce</code> When particles bounce off another object, this controls how much energy they keep <code>f@bounceforward</code> Controls how much energy they keep in the tangential direction <code>f@friction</code> When particles bounce, they are slowed down proportional to how hard they hit <code>s@collisionignore</code> Objects that match this pattern will not be collided <code>f@force</code> Forces on the particle for this frame <code>f@mass</code> Inertia of the particle <code>v@spinshape</code> This is multiplied by f@pscale to determine the shape of the particle for rotational inertia <code>f@drag</code> How much the particle is effected by any wind effects <code>f@dragexp</code> Ranges from 1 to 2, default is set on the solver  Used for both angular and linear drag <code>v@dragshape</code> How much the particle is dragged in each of its local axes <code>v@dragcenter</code> If specified, drag forces will also generate torques on the particle <code>v@targetv</code> The local wind speed Thought of as the goal, or target, velocity for the particle <code>f@airresist</code> How important it is to match the wind speed  Thickness of the air <code>f@speedmin</code> Minumum speed, in units per second, that a particle can move <code>f@speedmax</code> Maximum speed, in units per second, that a particle can move <code>p@orient</code> Orientation of the particle  Used for figuring out 'local' forces <code>v@w</code> Angular speed of the particle  A vector giving the rotation axis <code>v@torque</code> The equivalent of force for spins No inertial tensor (the equivalent of mass) is supported <code>v@targetw</code> The goal spin direction and speed for this particle <code>f@spinresist</code> How important it is to match the targetw <code>f@spinmin</code> Minumum speed in radians per second that a particle can spin <code>f@spinmax</code> Maximum speed in radians per second that a particle can spin - - DOP Grains Attributes <code>i@ispbd</code> A value of 1 causes the particle to behave as grains <code>f@pscale</code> Used to determine the radius of each particle <code>f@repulsionweight</code> How much the particle collision forces are weighted <code>f@repulsionstiffness</code> How strongly particles are kept apart  Higher values result in less bouncy repulsion <code>f@attractionweight</code> How much the particles will naturally stick together when close <code>f@attractionstiffness</code> How strongly nearby particles stick to each other <code>v@targetP</code> Particles are constrained to this location <code>f@targetweight</code> The weight of the v@targetP constraints <code>f@targetstiffness</code> The stiffness with which particles are fixed to their v@targetP attribute <code>f@restlength</code> Particles connected by polylines will be forced to maintain this distance (prim attribute) <code>f@constraintweight</code> Scale, on a per-particle basis of the constraint force <code>f@constraintstiffness</code> This controls the stiffness on a per-particle basis <code>f@strain</code> This primitive attribute records how much the constraint is stretched <code>f@strength</code> If f@strain exceeds this primitive attribute, the constraint will be removed - - DOP Packed RBD Attributes <code>i@active</code> Specifies whether the object is able to react to other objects <code>i@animated</code> Specifies whether the transform should be updated from its SOP geometry at each timestep <code>i@deforming</code> Specifies whether the collision shape should be rebuilt from its SOP geometry each timestep <code>f@bounce</code> The elasticity of the object <code>i@bullet_add_impact</code> Impacts that occur during the sim will be recorded in the Impacts or Feedback data <code>i@bullet_ignore</code> Specifies whether the object should be completely ignored by the Bullet solver <code>f@bullet_angular_sleep_threshold</code> The sleeping threshold for the object\u2019s angular velocity <code>f@bullet_linear_sleep_threshold</code> The sleeping threshold for the object\u2019s linear velocity <code>i@bullet_want_deactivate</code> Disables simulation of a non-moving object until the object moves again <code>i@computecom</code> Specifies whether the center of mass should be computed from the collision shape <code>i@computemass</code> Specifies whether the mass should be computedfrom the collision shape and density <code>f@creationtime</code> Stores the simulation time at which the object was created <code>i@dead</code> Specifies whether the object should be deleted during the next solve <code>f@density</code> The mass of an object is its volume times its density <code>f@friction</code> The coefficient of friction of the object <code>f@inertialtensorstiffness</code> Rotational stiffness  A scale factor applied to the inertial tensor <code>i@inheritvelocity</code> v and w point attributes from the SOP geometry will override the initial velocity <code>f@mass</code> The mass of the object <code>s@name</code> A unique name for the object Used by Constraint Networks <code>p@orient</code> The orientation of the object <code>v@P</code> The current position of the object\u2019s center of mass <code>v@pivot</code> The pivot that the orientation applies to If i@computecom is non-zero, this is auto-computed <code>v@v</code> Linear velocity of the object <code>v@w</code> Angular velocity of the object, in radians per second <code>i@bullet_adjust_geometry</code> Shrinks the collision geometry <code>i@bullet_autofit</code> Use the bounds of the object for Box, Capsule, Cylinder, Sphere, or Plane <code>f@bullet_collision_margin</code> Padding distance between collision shapes <code>s@bullet_georep</code> Can be convexhull, concave, box, capsule, cylinder, compound, sphere, or plane <code>i@bullet_groupconnected</code> Create convex hull per set of connected primitives <code>f@bullet_length</code> The length of the Capsule or Cylinder collision shape in the Y direction <code>v@bullet_primR</code> Orientation of the Box, Capsule, Cylinder, or Plane collision shape <code>v@bullet_primS</code> Size of the Box collision shape <code>v@bullet_primT</code> Position of the Box, Sphere, Capsule, Cylinder, or Plane collision shape <code>f@bullet_radius</code> Radius of the Sphere, Capsule, or Cylinder collision shape <code>f@bullet_shrink_amount</code> Specifies the amount of resizing done by Shrink Collision Geometry <code>s@activationignore</code> Won't be activated by collisions with any objects that match this pattern <code>s@collisiongroup</code> Specifies the name of a collision group that this object belongs to <code>s@collisionignore</code> The object will not collide against any objects that match this pattern <code>f@min_activation_impulse</code> Minimum impulse that will cause the object to switch from inactive to active <code>f@speedmin</code> Minumum speed, in units per second, that a particle can move <code>f@speedmax</code> Maximum speed, in units per second, that a particle can move <code>f@spinmin</code> Minumum speed in radians per second that a particle can spin <code>f@spinmax</code> Maximum speed in radians per second that a particle can spin <code>f@accelmax</code> Limits the change in the object\u2019s speed that is caused by enforcing constraints <code>f@angaccelmax</code> Limits the change in the object\u2019s angular speed that is caused by enforcing constraints <code>f@airresist</code> Specifies how important it is to match the target velocity (v@targetv) <code>f@drag</code> How much the the v@targetv and f@airresist attributes effect the object <code>f@dragexp</code> Ranges from 1 to 2, default is set on the solver  Used for both angular and linear drag <code>v@force</code> Specifies a force that will be applied to the center of mass of the object <code>f@spinresist</code> Specifies how important it is to match the target angular velocity (v@targetw) <code>v@targetv</code> Target velocity for the object Used in combination with the f@airresist attribute <code>v@targetw</code> Target angular velocity for the object Used in combination with the f@spinresist attribute <code>v@torque</code> Specifies a torque that will be applied to the object <code>i@bullet_autofit_valid</code> Stores whether the solver has already computed collision shape attributes <code>i@bullet_sleeping</code> Tracks whether the object has been put to sleep by the solver <code>f@deactivation_time</code> Amount of time the speed has been below the Linear Threshold or Angular Threshold <code>i@found_overlap</code> Used by the solver to determine whether it has performed the overlap test <code>i@id</code> A unique identifier for the object <code>i@nextid</code> Stores the i@id the solver will assign to the next new object - - DOP Constraint Network Attributes <code>f@width</code> Width of each edge <code>f@density</code> Density of each point <code>p@orient</code> Initial orientation of each point This value is stored as a quaternion <code>v@v</code> Initial velocity of each point <code>v@w</code> Initial angular velocity of each point measured in radians per second <code>f@friction</code> Friction of each point <code>f@klinear</code> Defines how strongly the wire resists stretching <code>f@damplinear</code> Defines how strongly the wire resists oscillation due to stretching forces <code>f@kangular</code> Defines how strongly the wire resists bending <code>f@dampangular</code> Defines how strongly the wire resists oscillation due to bending forces <code>f@targetstiffness</code> Defines how strongly the wire resists deforming from the animated position <code>f@targetdamping</code> Defines how strongly the wire resists oscillation due to stretch forces <code>f@normaldrag</code> The component of drag in the directions normal to the wire <code>f@tangentdrag</code> The component of drag in the direction tangent to the wire <code>i@nocollide</code> Collision detection for the edge is disabled (Only used if Collision Handling is SDF) <code>v@restP</code> Rest position of each point <code>p@restorient</code> Rest orientation of each point <code>i@gluetoanimation</code> Causes a point\u2019s position and orientation to be constrained to the input geometry <code>i@pintoanimation</code> Causes a point\u2019s position to be constrained to the input geometry <code>v@animationP</code> Target position of each point <code>p@animationorient</code> Target orientation of each point <code>v@animationv</code> Target velocity of each point <code>v@animationw</code> Target angular velocity of each point <code>i@independentcollisionallowed</code> Toggle external collisions (Only non-SDF Geometric Collision) <code>i@independentcollisionresolved</code> Unresolved external collisions (Only non-SDF Geometric Collision) <code>i@codependentcollisionallowed</code> Toggle soft body collisions (Only non-SDF Geometric Collision) <code>i@codependentcollisionresolved</code> Unresolved toggle soft body collisions (Only non-SDF Geometric Collision) <code>i@selfcollisionallowed</code> Toggle self collisions (Only non-SDF Geometric Collision) <code>i@selfcollisionresolved</code> Unresolved toggle self collisions (Only non-SDF Geometric Collision) - - DOP FLIP Attributes <code>f@pscale</code> Particle scale <code>v@v</code> Particle velocity <code>f@viscosity</code> The \"thickness\" of a fluid <code>f@density</code> The mass per unit volume <code>f@temperature</code> The temperature of the fluid <code>f@vorticity</code> Measures the amount of circulation in the fluid <code>f@divergence</code> Positive values cause particles to spread out, negative cause them to clump together <code>v@rest</code> Used to track the position of the fluid over time <code>v@rest2</code> Used for blending dual rest attributes, avoids stretching <code>f@droplet</code> Identifies particles that separate from the main body of fluid <code>f@underresolved</code> Particles that haven't fully resolved on the grid <code>i@ballistic</code> Specifies particles which will be ignored by the fluid solve <code>v@Lx</code> Angular momentum X axis <code>v@Ly</code> Angular momentum Y axis <code>v@Lz</code> Angular momentum Z axis"},{"location":"vex/Snippets/","title":"Snippets","text":"<p>Note</p> <p>Point Wrangle Bisects a vector fixing the first and last points</p> <pre><code>// Get the next points N\nvector prevPointP = point(0, 'P', @ptnum + 1);\nvector nextPointP = point(0, 'P', @ptnum - 1);\n\n// Create the bisected vector\nv@N = cross(normalize(nextPointP - prevPointP), v@up);\n\n// Check if it's the fist point\nif (@ptnum == 0){\n    // Create the bisected vector\n    v@N = -cross(normalize(prevPointP - @P), v@up);\n}\n\n// Check if it's the last point\nif (@ptnum == @numpt - 1){\n    vector vec = normalize(-cross(@P - point(0, 'P', @ptnum - 1), {0, 1, 0}));\n    @N = vec;\n}\n</code></pre> <p>Note</p> <p>Prim Wrangle Makes a primitive polywire sag good idea to resample it first</p> <pre><code>int    Count, Pt_List[];\nfloat  Fitted_Pt_Num, Ramped_Pt_Num;\nvector Modifier;\n\nPt_List =  primpoints(geoself(), 0);    \n\nfor(Count = 0; Count &lt; len(Pt_List); Count++){\n    Modifier      = set(0,0,0);\n    Fitted_Pt_Num = efit(Count, 0, (len(Pt_List) - 1), 0, 1);\n    Ramped_Pt_Num = chramp(\"sag_control\", Fitted_Pt_Num);\n    Modifier.y    = -(Ramped_Pt_Num * chf(\"sag_multiplier\"));\n    //Modifier      = set(1,Ramped_Pt_Num,1);\n    setpointattrib(geoself(), \"P\", Pt_List[Count], Modifier, \"add\");\n}\n</code></pre> <p>Note</p> <p>Prim wrangle (Points Line N at next point) Get Point Numbers (Should only ever be 2)</p> <pre><code>int points[] = primpoints(0, @primnum);\n// resize(points, 2);\n\n// Get P of each point\nvector points_P[];\nforeach (int point; points) {\n    vector P = point(0, 'P', point);\n    push(points_P, P);\n}\n\nsetpointattrib(0, 'N', points[0], (points_P[1] - points_P[0]));\nsetpointattrib(0, 'N', points[1], (points_P[0] - points_P[1]));\n</code></pre> <p>Note</p> <p>Look for a value in an array You should use find() instead</p> <pre><code>int contains(int array[]; int value){\n    foreach (int check_val; array){\n        if (check_val == value){\n            return 1;\n        }\n    }\n    return 0;\n}\n</code></pre> <p>Note</p> <p>Prim Wrangle This vex wrangle will find all the connected prim neighbors of an object</p> <pre><code>int     prim_edge, edge, prim, i, n, num;\nstring  neighbours = \"\";\n\ni = 0;\nprim_edge = primhedge(@OpInput1, @primnum);\nwhile(i &lt; primvertexcount(@OpInput1, @primnum))\n{\n    num = hedge_equivcount(@OpInput1, prim_edge);\n    n = 0;\n    while(n &lt; num)\n    {\n        edge = hedge_nextequiv(@OpInput1, prim_edge);\n        prim = hedge_prim(@OpInput1, edge);\n        if(prim != @primnum)\n            neighbours += sprintf(\"%g \", prim);\n\n        prim_edge = edge;\n        n++;\n    }\n\n    prim_edge = hedge_next(@OpInput1, prim_edge);\n    i++;      \n}\n\ns@neighbours = neighbours;\n</code></pre> <p>Note</p> <p>If a prim @Cd is red return its primnum</p> <p></p><pre><code>int has_red_neighbor(string primnums) {\n  foreach(string s_primnum; split(primnums)) {\n    int primnum = atoi(s_primnum);\n    if (prim(0, 'Cd', primnum) == set(1, 0, 0))\n      return primnum;\n  }\n  return -1;\n}\n</code></pre> <pre><code>int find_non_outerwall_neighbor(string primnums) {\n  foreach(string s_primnum; split(primnums)) {\n    int primnum = atoi(s_primnum);\n    if (prim(0, 'group_outer_walls', primnum) == 0)\n      return primnum;\n  }\n  return -1;\n}\n</code></pre> <p>Note</p> <p>Get an interger list of the primitive attribute neighbors</p> <pre><code>int[] get_neighbors_from_primnum(int primnum) {\n  int int_neighbors[] = array();\n  string neighbors[] = split(prim(0, 'neighbors', primnum));\n  foreach(string n_primnum; neighbors) {\n    push(int_neighbors, atoi(n_primnum));\n  }\n  return int_neighbors;\n}\n</code></pre> <p>Note</p> <p>If a primitive has any neighbor primitive points that share a point with any other neighbor that piece is an elbow piece</p> <pre><code>int is_elbow(int primnum) {\n  int seen_points[] = array();\n  // Get the neighbors\n  int neighbors[] = get_neighbors_from_primnum(primnum);\n  foreach(int n_primnum; neighbors) {\n    // get the neigbor points\n    int neighbor_points[] = primpoints(0, n_primnum);\n    foreach(int pointnum; neighbor_points) {\n      // If we've seen the point return immediatly\n      if (find(seen_points, pointnum) &gt;= 0) {\n        return 1;\n      }\n      // add the seen points to the arry\n      push(seen_points, pointnum);\n    }\n  }\n  return 0;\n}\n</code></pre> <pre><code>int[] split_neighbors_to_int_array(string neighbors) {\n  int ret_arr[] = array();\n  foreach(string primnum; split(neighbors)) {\n    push(ret_arr, atoi(primnum));\n  }\n  return ret_arr;\n}\n</code></pre> <pre><code>int[] get_blue_neighbors(string str_neighbors) {\n  int blue_neighbors[] = array();\n  int neighbors[] = split_neighbors_to_int_array(str_neighbors);\n  foreach(int primnum; neighbors) {\n    if (prim(0, 'Cd', primnum) == set(0, 0, 1)) {\n      push(blue_neighbors, primnum);\n    }\n  }\n  return blue_neighbors;\n}\n</code></pre> <pre><code>vector get_first_blue_neighbors_vector(int blue_neighbors[]) {\n  vector facing_arr[] = array();\n  foreach(int primnum; blue_neighbors) {\n    vector facing = prim(0, 'facing', primnum);\n    push(facing_arr, facing);\n  }\n  return facing_arr[1];\n}\n</code></pre> <pre><code>int find_first_unseen(int seen_prims[]; int neighbors[]) {\n  foreach(int neighbor; neighbors) {\n    if (find(seen_prims, neighbor) &gt;= 0) {} else {\n      return neighbor;\n    }\n  }\n}\n</code></pre> <p>Note</p> <p>OpInput1: is usually just @OpInput1 and primnum is @primnum</p> <pre><code>string neighbor_prims(string OpInput1; int primnum) {\n  int prim_edge, edge, prim, i, n, num;\n  int neighbours_arr[] = array();\n  string neighbors = \"\";\n\n  i = 0;\n  prim_edge = primhedge(OpInput1, primnum);\n  while (i &lt; primvertexcount(OpInput1, primnum)) {\n    num = hedge_equivcount(OpInput1, prim_edge);\n    n = 0;\n    while (n &lt; num) {\n      edge = hedge_nextequiv(OpInput1, prim_edge);\n      prim = hedge_prim(OpInput1, edge);\n      if (prim != primnum) {\n        neighbors += sprintf(\"%s \", prim);\n        push(neighbours_arr, prim);;\n      }\n      prim_edge = edge;\n      n++;\n    }\n    prim_edge = hedge_next(OpInput1, prim_edge);\n    i++;\n  }\n  return neighbors;\n}\n</code></pre> <p>Note</p> <p>OpInput1: is usually just @OpInput1 and primnum is @primnum</p> <pre><code>int[] neighbor_prims_i(string OpInput1; int primnum) {\n  int prim_edge, edge, prim, i, n, num;\n  int neighbours_arr[] = array();\n  i = 0;\n  prim_edge = primhedge(OpInput1, primnum);\n  while (i &lt; primvertexcount(OpInput1, primnum)) {\n    num = hedge_equivcount(OpInput1, prim_edge);\n    n = 0;\n    while (n &lt; num) {\n      edge = hedge_nextequiv(OpInput1, prim_edge);\n      prim = hedge_prim(OpInput1, edge);\n      if (prim != primnum) {\n        push(neighbours_arr, prim);;\n      }\n      prim_edge = edge;\n      n++;\n    }\n    prim_edge = hedge_next(OpInput1, prim_edge);\n    i++;\n  }\n  return neighbours_arr;\n}\n</code></pre> <pre><code>vector get_direction_from(int primnum1; int primnum2){\n  vector direction =  prim(0,'P', primnum2) - prim(0, 'P', primnum1);\n  return normalize(set(direction.x, direction.y, direction.z));\n}\n</code></pre> <pre><code>int is_left(int primnum; int neighbor_primnum){\n  if (get_direction_from(primnum, neighbor_primnum) == set(1, 0, 0) ){\n    return 1;\n  }\n  return 0;\n}\n</code></pre> <pre><code>int is_right(int primnum; int neighbor_primnum){\n  if (get_direction_from(primnum, neighbor_primnum) == set(-1, 0, 0) ){\n    return 1;\n  }\n  return 0;\n}```\n\n```c\n\nint is_above(int primnum; int neighbor_primnum){\n  if (get_direction_from(primnum, neighbor_primnum) == set(0, 0, 1) ){\n    return 1;\n  }\n  return 0;\n}\n</code></pre> <pre><code>int is_below(int primnum; int neighbor_primnum){\n  vector dir = get_direction_from(primnum, neighbor_primnum);\n  if (dir == set(0, 0, -1)){\n    return 1;\n  }\n  return 0;\n}\n</code></pre> <pre><code>int has_neighbors_above(int primnum){\n  int neighbors[] = get_neighbors_from_primnum(primnum);\n  foreach (int neighbor_prim; neighbors){\n      if (is_above(primnum, neighbor_prim)){\n        return 1;\n      }\n  }\n  return 0;\n}\n</code></pre> <pre><code>int has_neighbors_left(int primnum){\n  int neighbors[] = get_neighbors_from_primnum(primnum);\n  foreach (int neighbor_prim; neighbors){\n      if (is_left(primnum, neighbor_prim)){\n        return 1;\n      }\n  }\n  return 0;\n}\n</code></pre> <pre><code>int has_neighbors_right(int primnum){\n  int neighbors[] = get_neighbors_from_primnum(primnum);\n  foreach (int neighbor_prim; neighbors){\n      if (is_right(primnum, neighbor_prim)){\n        return 1;\n      }\n  }\n  return 0;\n}\n</code></pre> <pre><code>int has_neighbors_below(int primnum){\n  int neighbors[] = get_neighbors_from_primnum(primnum);\n  foreach (int neighbor_prim; neighbors){\n      if (is_below(primnum, neighbor_prim)){\n        return 1;\n      }\n  }\n  return 0;\n}\n</code></pre> <pre><code>int[] get_shape_array(int primnum){\n  int shape_array[] = array();\n  push(shape_array, has_neighbors_left(primnum));\n  push(shape_array, has_neighbors_right(primnum));\n  push(shape_array, has_neighbors_above(primnum));\n  push(shape_array, has_neighbors_below(primnum));\n  return shape_array;\n}\n</code></pre> <pre><code>int array_equivelant(int arr1[]; int arr2[]){\n  int i = 0;\n  foreach(int val; arr2) {\n    // If a value doesn't match return back false\n    if (val != arr1[i]){\n      return 0;\n    }\n    i += 1;\n  }\n  // All the values match return true\n  return 1;\n}\n</code></pre>"},{"location":"vex/vex/","title":"Variables","text":""},{"location":"vex/vex/#welcome-to-vex","title":"Welcome to Vex","text":""},{"location":"vex/vex/#global-variables","title":"Global Variables","text":"<p>A list of variables available in wrangles.</p> <p>Note</p> <p>The type indicator isn't necessary, but included as a reminder.</p> <pre><code>Available in all SOP wrangles\nf@Frame \u00a0 \u00a0 //The current floating frame number, equivalent to the $FF Hscript variable\nf@Time \u00a0 \u00a0 \u00a0//The current time in seconds, equivalent to the $T Hscript variable\ni@SimFrame \u00a0//The integer simulation timestep number ($SF), only present in DOP contexts.\nf@SimTime \u00a0 //The simulation time in seconds ($ST), only present in DOP contexts.\nf@TimeInc \u00a0 //The timestep currently being used for simulation or playback.\n\nAvailable in Attribute Wrangle (point, vertex, primitive and detail)\nv@P \u00a0 \u00a0 \u00a0 \u00a0 //The position of the current element.\ni@ptnum \u00a0 \u00a0 //The point number attached to the currently processed element.\ni@vtxnum \u00a0 \u00a0//The linear number of the currently processed vertex.\ni@primnum \u00a0 //The primitive number attached to the currently processed element.\ni@elemnum \u00a0 //The index number of the currently processed element.\ni@numpt \u00a0 \u00a0 //The total number of points in the geometry.\ni@numvtx \u00a0 \u00a0//The number of vertices in the primitive of the currently processed element.\ni@numprim \u00a0 //The total number of primitives in the geometry.\ni@numelem \u00a0 //The total number of elements being processed.\n\nAvailable in Volume Wrangle\nv@P \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 //The position of the current voxel.\nf@density \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 //The value of the density field at the current voxel location.\nv@center \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0//The center of the current volume.\nv@dPdx, v@dPdy, v@dPdz \u00a0//These vectors store the change in P that occurs in the x, y, and z voxel indices.\ni@ix, i@iy, i@iz \u00a0 \u00a0 \u00a0 \u00a0//Voxel indices. For dense volumes (non-VDB) these range from 0 to resolution-1.\ni@resx, i@resy, i@resz \u00a0//The resolution of the current volume.\n</code></pre>"},{"location":"vex/vex/#common-geometry-attributes","title":"Common Geometry Attributes","text":"<p>Note</p> <p>Houdini knows to cast these to the appropriate data type.</p> <pre><code>Int\n@id \u00a0 \u00a0 \u00a0 \u00a0 // A unique number that remains the same throughout a simulation.\n\n// Float\n@pscale \u00a0 \u00a0 // Particle radius size. \u00a0Uniform scale. \u00a0Set display particles as 'Discs' to visualize.\n@width \u00a0 \u00a0 \u00a0// Thickness of curves. \u00a0Enable 'Shade Open Curves In Viewport' on the object node to visualize.\n@Alpha \u00a0 \u00a0 \u00a0// Alpha transparency override. \u00a0The viewport uses this to set the alpha of OpenGL geometry.\n@Pw \u00a0 \u00a0 \u00a0 \u00a0 // Spline weight.\n\nVector3\n@P \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Point position. \u00a0Used this to lay out points in 3D space.\n@Cd \u00a0 \u00a0 \u00a0 \u00a0 // Diffuse color override. \u00a0The viewport uses this to color OpenGL geometry.\n@N \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Surface or curve normal. \u00a0Houdini will compute the normal if this attribute does not exist.\n@scale \u00a0 \u00a0 \u00a0// Vector scale. \u00a0Allows directional scaling or stretching (in one direction).\n@rest \u00a0 \u00a0 \u00a0 // Used by procedural patterns and textures to stick on deforming and animated surfaces.\n@up \u00a0 \u00a0 \u00a0 \u00a0 // Up vector. \u00a0The up direction for local space, typically (0, 1, 0).\n@uv \u00a0 \u00a0 \u00a0 \u00a0 // UV texture coordinates for this point/vertex.\n@v \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Point velocity. \u00a0The direction and speed of movement in units per second.\n\nVector4\n@orient \u00a0 \u00a0 // The local orientation of the point (represented as a quaternion).\n@rot \u00a0 \u00a0 \u00a0 \u00a0// Additional rotation to be applied after orient, N, and up attributes.\n\nString\n@name \u00a0 \u00a0 \u00a0 // A unique name identifying which primitives belong to which piece. \u00a0Also used to label volumes.\n@instance \u00a0 // Path of an object node to be instanced at render time.\n</code></pre>"},{"location":"vex/vex/#specifying-vex-data-types","title":"Specifying VEX Data Types","text":"<p>Note</p> <p>The following characters are used to cast to the corresponding data type.</p> <pre><code>float \u00a0 \u00a0 \u00a0 f@name \u00a0 \u00a0// Floating point scalar values.\nvector2 \u00a0 \u00a0 u@name \u00a0 \u00a0// Two floating point values. Could be used to store 2D positions.\nvector3 \u00a0 \u00a0 v@name \u00a0 \u00a0// Three floating point values. Usually positions, directions, normals, UVW or colors.\nvector4 \u00a0 \u00a0 p@name \u00a0 \u00a0// Four floating point values. Usually rotation quaternions, or color and alpha (RGBA).\nint \u00a0 \u00a0 \u00a0 \u00a0 i@name \u00a0 \u00a0// Integer values (VEX uses 32 bit integers).\nmatrix2 \u00a0 \u00a0 2@name \u00a0 \u00a0// Four floating point values representing a 2D rotation matrix.\nmatrix3 \u00a0 \u00a0 3@name \u00a0 \u00a0// Nine floating point values representing a 3D rotation matrix or 2D transform matrix.\nmatrix \u00a0 \u00a0 \u00a04@name \u00a0 \u00a0// Sixteen floating point values representing a 3D transform matrix.\nstring \u00a0 \u00a0 \u00a0s@name \u00a0 \u00a0// A string of characters.\n</code></pre>"},{"location":"vex/vex/#channel-shortcut-syntax","title":"Channel Shortcut Syntax","text":"<p>Note</p> <p>This is used to hint at the data type of auto generated wrangle parameters.</p> <pre><code>ch('flt1'); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Float\nchf('flt2'); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Float\nchi('int'); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Integer\nchv('vecparm'); \u00a0 \u00a0 \u00a0 \u00a0 // Vector 3\nchp('quat'); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Vector 4 / Quaternion\nch3('m3'); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// 3x3 Matrix\nch4('m4'); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// 4x4 Matrix\nchs('str'); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // String\nchramp('r', x); \u00a0 \u00a0 \u00a0 \u00a0 // Spline Ramp\nvector(chramp('c', x)); // RGB Ramp\u00a0\n</code></pre>"},{"location":"vex/vex/#dop-particle-attributes","title":"DOP Particle Attributes","text":"<p>Note</p> <p>Particle systems are driven by attributes, here are some of the attributes used.</p> <pre><code>f@age \u00a0 \u00a0 \u00a0 // Time in seconds since the particle was born.\nf@life \u00a0 \u00a0 \u00a0// Time in seconds the particle is allowed to live. When f@age&gt;f@life, i@dead will be set to 1.\nf@nage \u00a0 \u00a0 \u00a0// Normalized age, f@age divided by f@life. \u00a0Implicit attribute, you cannot write to this.\ni@dead \u00a0 \u00a0 \u00a0// Whether a particle is living (0) or dead (1). \u00a0A dead particle is deleted in the Reaping stage.\ni@id \u00a0 \u00a0 \u00a0 \u00a0// A unique id for the particle that remains the same throughout a single simulation.\n\ni@stopped \u00a0 // Whether a particle is moving (0) or stopped (1).\ni@stuck \u00a0 \u00a0 // Whether a particle is free (0) or stuck (1).\ni@sliding \u00a0 // Whether a particle is free (0) or sliding along a surface (1).\nf@cling \u00a0 \u00a0 // Force applied to sliding particles inwards (according to the collision's surface normal).\ns@pospath \u00a0 // The path to the object that the particle is colliding with.\ni@posprim \u00a0 // Which collision primitive in the path geometry whose position we wish to refer to.\nv@posuv \u00a0 \u00a0 // Parametric uv on the collision primitive.\n\ni@hittotal \u00a0// The cumulative total of all hits for the particle (only incremented once per timestep).\ni@has_pprevious // This is set to 1 if v@pprevious contains valid values.\nv@pprevious // Stores the position of the particle on the previous frame. \u00a0Used for collision detection.\ni@hitnum \u00a0 \u00a0// The number of times the particle collided in the last POP Collision Detect.\ns@hitpath \u00a0 // The path to the object that was hit. A path to a file on disk or an op: path.\ni@hitprim \u00a0 // The primitive hit. Could be -1 if it the collision detector couldn\u2019t figure out which prim.\nv@hituv \u00a0 \u00a0 // The parametric UV space on the primitive.\nv@hitpos \u00a0 \u00a0// Where the hit actually occurred. \u00a0Useful if the colliding object was moving.\nv@hitnml \u00a0 \u00a0// The normal of the surface at the time of the collision.\nv@hitv \u00a0 \u00a0 \u00a0// The velocity of the surface at the time of the collision.\nf@hittime \u00a0 // When the collision occurred, that could be within a frame.\nf@hitimpulse// Records how much of an impulse was needed for the collision resolution. \u00a0varies with timestep.\nf@bounce \u00a0 \u00a0// When particles bounce off another object, this controls how much energy they keep.\nf@bounceforward // Controls how much energy they keep in the tangential direction.\nf@friction \u00a0// When particles bounce, they are slowed down proportional to how hard they hit.\ns@collisionignore \u00a0 // Objects that match this pattern will not be collided.\n\nf@force \u00a0 \u00a0 // Forces on the particle for this frame.\nf@mass \u00a0 \u00a0 \u00a0// Inertia of the particle.\nv@spinshape // This is multiplied by f@pscale to determine the shape of the particle for rotational inertia.\nf@drag \u00a0 \u00a0 \u00a0// How much the particle is effected by any wind effects.\nf@dragexp \u00a0 // Ranges from 1 to 2, default is set on the solver. \u00a0Used for both angular and linear drag.\nv@dragshape // How much the particle is dragged in each of its local axes.\nv@dragcenter// If specified, drag forces will also generate torques on the particle.\nv@targetv \u00a0 // The local wind speed. Thought of as the goal, or target, velocity for the particle.\nf@airresist // How important it is to match the wind speed. \u00a0Thickness of the air.\nf@speedmin \u00a0// Minimum speed, in units per second, that a particle can move.\nf@speedmax \u00a0// Maximum speed, in units per second, that a particle can move.\n\np@orient \u00a0 \u00a0// Orientation of the particle. \u00a0Used for figuring out 'local' forces.\nv@w \u00a0 \u00a0 \u00a0 \u00a0 // Angular speed of the particle. \u00a0A vector giving the rotation axis.\nv@torque \u00a0 \u00a0// The equivalent of force for spins. No inertial tensor (the equivalent of mass) is supported.\nv@targetw \u00a0 // The goal spin direction and speed for this particle.\nf@spinresist// How important it is to match the targetw.\nf@spinmin \u00a0 // Minimum speed in radians per second that a particle can spin.\nf@spinmax \u00a0 // Maximum speed in radians per second that a particle can spin.\n</code></pre>"},{"location":"vex/vex/#dop-grains-attributes","title":"DOP Grains Attributes","text":"<p>Note</p> <p>Particles under control of POP Grains have the 'ispbd' attribute set to 1. This causes them to bypass movement update in the POP Solver, as the actual motion update is done by the POP Grains node.</p> <pre><code>i@ispbd \u00a0 \u00a0 // A value of 1 causes the particle to behave as grains.\nf@pscale \u00a0 \u00a0// Used to determine the radius of each particle.\nf@repulsionweight \u00a0 // How much the particle collision forces are weighted.\nf@repulsionstiffness// How strongly particles are kept apart. \u00a0Higher values result in less bouncy repulsion.\nf@attractionweight \u00a0// How much the particles will naturally stick together when close.\nf@attractionstiffness \u00a0 // How strongly nearby particles stick to each other.\nv@targetP \u00a0 // Particles are constrained to this location.\nf@targetweight \u00a0 \u00a0 \u00a0// The weight of the v@targetP constraints.\nf@targetstiffness \u00a0 // The stiffness with which particles are fixed to their v@targetP attribute.\n\nf@restlength// Particles connected by polylines will be forced to maintain this distance (prim attribute).\nf@constraintweight \u00a0// Scale, on a per-particle basis of the constraint force.\nf@constraintstiffness \u00a0 // This controls the stiffness on a per-particle basis.\nf@strain \u00a0 \u00a0// This primitive attribute records how much the constraint is stretched.\nf@strength \u00a0// If f@strain exceeds this primitive attribute, the constraint will be removed.\n</code></pre>"},{"location":"vex/vex/#dop-packed-rbd-attributes","title":"DOP Packed RBD Attributes","text":"<p>Note</p> <p>The Bullet Solver uses several point attributes to store the properties of each piece of a packed object.</p> <pre><code>i@active \u00a0 \u00a0// Specifies whether the object is able to react to other objects.\ni@animated \u00a0// Specifies whether the transform should be updated from its SOP geometry at each timestep.\ni@deforming // Specifies whether the collision shape should be rebuilt from its SOP geometry each timestep.\n\nf@bounce \u00a0 \u00a0// The elasticity of the object.\ni@bullet_add_impact // Impacts that occur during the sim will be recorded in the Impacts or Feedback data.\ni@bullet_ignore \u00a0 \u00a0 // Specifies whether the object should be completely ignored by the Bullet solver.\nf@bullet_angular_sleep_threshold// The sleeping threshold for the object\u2019s angular velocity.\nf@bullet_linear_sleep_threshold // The sleeping threshold for the object\u2019s linear velocity.\ni@bullet_want_deactivate// Disables simulation of a non-moving object until the object moves again.\u00a0\ni@computecom// Specifies whether the center of mass should be computed from the collision shape.\ni@computemass \u00a0 // Specifies whether the mass should be computed from the collision shape and density.\nf@creationtime \u00a0// Stores the simulation time at which the object was created.\ni@dead \u00a0// Specifies whether the object should be deleted during the next solve.\nf@density \u00a0 // The mass of an object is its volume times its density.\nf@friction \u00a0// The coefficient of friction of the object.\nf@inertialtensorstiffness \u00a0 // Rotational stiffness. \u00a0A scale factor applied to the inertial tensor.\u00a0\ni@inheritvelocity \u00a0 // v and w point attributes from the SOP geometry will override the initial velocity.\nf@mass \u00a0// The mass of the object.\ns@name \u00a0// A unique name for the object. Used by Constraint Networks.\np@orient// The orientation of the object.\nv@P \u00a0 \u00a0 // The current position of the object\u2019s center of mass.\nv@pivot // The pivot that the orientation applies to. If i@computecom is non-zero, this is auto-computed.\nv@v \u00a0 \u00a0 // Linear velocity of the object.\nv@w \u00a0 \u00a0 // Angular velocity of the object, in radians per second.\n\ni@bullet_adjust_geometry \u00a0 \u00a0// Shrinks the collision geometry.\ni@bullet_autofit \u00a0 \u00a0// Use the bounds of the object for Box, Capsule, Cylinder, Sphere, or Plane.\nf@bullet_collision_margin \u00a0 // Padding distance between collision shapes.\ns@bullet_georep \u00a0 \u00a0 // Can be convexhull, concave, box, capsule, cylinder, compound, sphere, or plane.\ni@bullet_groupconnected \u00a0 \u00a0 // Create convex hull per set of connected primitives.\nf@bullet_length \u00a0 \u00a0 // The length of the Capsule or Cylinder collision shape in the Y direction.\nv@bullet_primR \u00a0// Orientation of the Box, Capsule, Cylinder, or Plane collision shape.\nv@bullet_primS \u00a0// Size of the Box collision shape.\nv@bullet_primT \u00a0// Position of the Box, Sphere, Capsule, Cylinder, or Plane collision shape.\nf@bullet_radius // Radius of the Sphere, Capsule, or Cylinder collision shape.\nf@bullet_shrink_amount \u00a0// Specifies the amount of resizing done by Shrink Collision Geometry.\n\ns@activationignore \u00a0// Won't be activated by collisions with any objects that match this pattern.\ns@collisiongroup \u00a0 \u00a0// Specifies the name of a collision group that this object belongs to.\ns@collisionignore \u00a0 // The object will not collide against any objects that match this pattern.\nf@min_activation_impulse// Minimum impulse that will cause the object to switch from inactive to active.\n\nf@speedmin \u00a0// Minimum speed, in units per second, that a particle can move.\nf@speedmax \u00a0// Maximum speed, in units per second, that a particle can move.\nf@spinmin \u00a0 // Minimum speed in radians per second that a particle can spin.\nf@spinmax \u00a0 // Maximum speed in radians per second that a particle can spin.\nf@accelmax \u00a0// Limits the change in the object\u2019s speed that is caused by enforcing constraints.\nf@angaccelmax \u00a0 // Limits the change in the object\u2019s angular speed that is caused by enforcing constraints.\n\nf@airresist // Specifies how important it is to match the target velocity (v@targetv).\nf@drag \u00a0 \u00a0 \u00a0// How much the the v@targetv and f@airresist attributes effect the object.\nf@dragexp \u00a0 // Ranges from 1 to 2, default is set on the solver. \u00a0Used for both angular and linear drag.\nv@force \u00a0 \u00a0 // Specifies a force that will be applied to the center of mass of the object.\nf@spinresist// Specifies how important it is to match the target angular velocity (v@targetw).\nv@targetv \u00a0 // Target velocity for the object. Used in combination with the f@airresist attribute.\nv@targetw \u00a0 // Target angular velocity for the object. Used in combination with the f@spinresist attribute.\nv@torque \u00a0 \u00a0// Specifies a torque that will be applied to the object.\n\ni@bullet_autofit_valid \u00a0// Stores whether the solver has already computed collision shape attributes.\ni@bullet_sleeping \u00a0 \u00a0 \u00a0 // Tracks whether the object has been put to sleep by the solver.\nf@deactivation_time \u00a0 \u00a0 // Amount of time the speed has been below the Linear Threshold or Angular Threshold.\ni@found_overlap \u00a0 \u00a0 \u00a0 \u00a0 // Used by the solver to determine whether it has performed the overlap test.\ni@id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// A unique identifier for the object.\ni@nextid \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Stores the i@id the solver will assign to the next new object.\n</code></pre>"},{"location":"vex/vex/#dop-rbd-constraint-attributes","title":"DOP RBD Constraint Attributes","text":"<p>Note</p> <p>Attributes on the geometry to customize each constraint behavior and type. If a primitive attribute with the same name as a constraint property (such as damping) is present, the attribute value will be multiplied with the value from the constraint sub-data.</p> <pre><code>s@constraint_name \u00a0 \u00a0// Specifies a piece of relationship data by name, such as 'Glue' or 'Spring'.\ns@constraint_type \u00a0 \u00a0// Specifies whether the constraint affects 'position', 'rotation' or 'all' degrees of freedom.\nf@restlength \u00a0 \u00a0 \u00a0 \u00a0 // Specifies the desired length of the constraint to enforce.\nf@width \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Width of each edge.\nf@density \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Density of each point.\np@orient \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Initial orientation of each point. Value stored as a quaternion.\nv@v \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Initial velocity of each point.\nv@w \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Initial angular velocity of each point measured in radians per second.\nf@friction \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Friction of each point.\nf@klinear \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Defines how strongly the wire resists stretching.\nf@damplinear \u00a0 \u00a0 \u00a0 \u00a0 // Defines how strongly the wire resists oscillation due to stretching forces.\nf@kangular \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Defines how strongly the wire resists bending.\nf@dampangular \u00a0 \u00a0 \u00a0 \u00a0// Defines how strongly the wire resists oscillation due to bending forces.\nf@targetstiffness \u00a0 \u00a0// Defines how strongly the wire resists deforming from the animated position.\nf@targetdamping \u00a0 \u00a0 \u00a0// Defines how strongly the wire resists oscillation due to stretch forces.\nf@normaldrag \u00a0 \u00a0 \u00a0 \u00a0 // The component of drag in the directions normal to the wire.\nf@tangentdrag \u00a0 \u00a0 \u00a0 \u00a0// The component of drag in the direction tangent to the wire.\ni@nocollide \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Collision detection for the edge is disabled (Only used if Collision Handling is SDF).\nv@restP \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Rest position of each point.\np@restorient \u00a0 \u00a0 \u00a0 \u00a0 // Rest orientation of each point.\ni@gluetoanimation \u00a0 \u00a0// Causes a point\u2019s position and orientation to be constrained to the input geometry.\ni@pintoanimation \u00a0 \u00a0 // Causes a point\u2019s position to be constrained to the input geometry.\nv@animationP \u00a0 \u00a0 \u00a0 \u00a0 // Target position of each point.\np@animationorient \u00a0 \u00a0// Target orientation of each point.\nv@animationv \u00a0 \u00a0 \u00a0 \u00a0 // Target velocity of each point.\nv@animationw \u00a0 \u00a0 \u00a0 \u00a0 // Target angular velocity of each point.\ni@independentcollisionallowed \u00a0 // Toggle external collisions (Only non-SDF Geometric Collision).\ni@independentcollisionresolved \u00a0// Unresolved external collisions (Only non-SDF Geometric Collision).\ni@codependentcollisionallowed \u00a0 // Toggle soft body collisions (Only non-SDF Geometric Collision).\ni@codependentcollisionresolved \u00a0// Unresolved toggle soft body collisions (Only non-SDF Geometric Collision).\ni@selfcollisionallowed \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Toggle self collisions (Only non-SDF Geometric Collision).\ni@selfcollisionresolved \u00a0 \u00a0 \u00a0 \u00a0 // Unresolved toggle self collisions (Only non-SDF Geometric Collision).\n</code></pre>"},{"location":"vex/vex/#dop-flip-attributes","title":"DOP FLIP Attributes","text":"<p>Note</p> <p>The FLIP Solver contains an embedded POP Solver, so all of POP Attributes listed above apply.</p> <pre><code>f@pscale \u00a0 \u00a0 \u00a0 \u00a0// Particle scale\nv@v \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Particle velocity\nf@viscosity \u00a0 \u00a0 // The \"thickness\" of a fluid.\nf@density \u00a0 \u00a0 \u00a0 // The mass per unit volume.\nf@temperature \u00a0 // The temperature of the fluid.\nf@vorticity \u00a0 \u00a0 // Measures the amount of circulation in the fluid.\nf@divergence \u00a0 \u00a0// Positive values cause particles to spread out, negative cause them to clump together.\nv@rest \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Used to track the position of the fluid over time.\nv@rest2 \u00a0 \u00a0 \u00a0 \u00a0 // Used for blending dual rest attributes, avoids stretching.\u00a0\nf@droplet \u00a0 \u00a0 \u00a0 // Identifies particles that separate from the main body of fluid.\nf@underresolved // Particles that haven't fully resolved on the grid.\ni@ballistic \u00a0 \u00a0 // Specifies particles which will be ignored by the fluid solve.\nv@Lx \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Angular momentum X axis\nv@Ly \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Angular momentum Y axis\nv@Lz \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Angular momentum Z axis\n</code></pre>"},{"location":"vex/vex/#dop-vellum-point-attributes","title":"DOP Vellum Point Attributes","text":"<p>Note</p> <p>Vellum geometry is also considered particles, so all of POP Attributes listed above apply.</p> <pre><code>i@isgrain \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // A value of 1 causes the particle to behave as grains, 0 behaves as cloth.\nf@attractionweight \u00a0// How much the particles will naturally stick together when close, zero disables clumping.\nf@friction \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// How much to scale the static friction.\nf@dynamicfriction \u00a0 // How much to scale the dynamic friction.\nf@inertia \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Resistance of a particle to rotational constraints. If zero, the particle won't rotate.\nv@v \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Point velocity.\np@w \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // (Hair or Wire) angular velocity.\np@orient \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// (Hair or Wire) orientations.\ni@stopped \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Used to pin points (0=free, 1=no motion, 2=no rotation, 3=no rotate or move).\ni@pintoanimation \u00a0 \u00a0// If 1, the pinned points' position will be updated to match the target point.\ni@gluetoanimation \u00a0 // If 1, both the position and orientation will be updated\ns@target_path \u00a0 \u00a0 \u00a0 // target path for any pins (when the Target parameter is set in Vellum Source)\ni@target_pt \u00a0 \u00a0 \u00a0 \u00a0 // target point number for any pins (when the Target parameter is set in Vellum Source)\nf@targetweight \u00a0 \u00a0 \u00a0// Affect the strength of the pinned points using a 0..1 weighting value.\ni@weld \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Weld this point to a point number. \u00a0If there is an @id attribute, then to a point id.\ni@branchweld \u00a0 \u00a0 \u00a0 \u00a0// built by hair constraints when it is forced to split points for hair simulation.\ni@collisionweld \u00a0 \u00a0 // generated on demand to provide a single weld to the detangle algorithm.\nf@breakthreshold \u00a0 \u00a0// The threshold for breaking welds and branch welds\ns@breaktype \u00a0 \u00a0 \u00a0 \u00a0 // 'stretchstress', 'bendstress', 'stretchdistance', 'stretchratio', or 'bendangle'.\n\n// Collisions\nf@pscale \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Used to determine the thickness of cloth or radius of each particle.\nf@overlap_self \u00a0 \u00a0 \u00a0// Stores how much of the original pscale is overlapped.\nf@overlap_external \u00a0// Stores how much of the original pscale is overlapped.\ni@layer \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Indicates belonging to different layers of cloth. Higher numbers refer to higher layers.\ni@disableself \u00a0 \u00a0 \u00a0 // A value of 0 means this point will use self collisions.\ni@disableexternal \u00a0 // A value of 0 means this point will use external collisions.\ns@collisionignore \u00a0 // Stores a pattern for the objects and collision groups to not collide with.\ns@collisiongroup \u00a0 \u00a0// Gives the collision group that this point belongs to.\n\n// Internal worker variables (Kept to avoid removing/adding attributes every frame)\nv@pprevious \u00a0 \u00a0 \u00a0 \u00a0 // For 1st order integration, the previous frames position (beginning of timestep).\nv@plast \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // For 2nd order integration, the position from two frames earlier.\nv@vprevious \u00a0 \u00a0 \u00a0 \u00a0 // For 1st order integration, the previous frames velocity (beginning of timestep).\nv@vlast \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // For 2nd order integration, the velocity from two frames earlier.\np@orientprevious \u00a0 \u00a0// For 1st order integration, the previous frames orientation (beginning of timestep).\np@orientlast \u00a0 \u00a0 \u00a0 \u00a0// For 2nd order integration, the orientation from two frames earlier.\np@wprevious \u00a0 \u00a0 \u00a0 \u00a0 // For 1st order integration, the previous frames angular velocity (beginning of timestep).\np@wlast \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // For 2nd order integration, the angular velocity from two frames earlier.\nf@dP \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Constraint displacements. \u00a0Likely of last iteration.\nf@dPw \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Constraint weights. \u00a0Likely of last iteration.\ns@patchname \u00a0 \u00a0 \u00a0 \u00a0 // Identifies each generated patch in a simulation so it can be updated/replaced.\n// When a point is part of a Pressure constraint, these attrs hold values computed during constraint update.\nv@pressuregradient \u00a0// a vector pointing outwards along the direction of greatest volume gain.\ni[]@volumepts \u00a0 \u00a0 \u00a0 // contains array of the points needed to compute the volume attribute.\ni@volume \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// compare against the Pressure constraint\u2019s restlength value.\n</code></pre>"},{"location":"vex/vex/#dop-vellum-constraint-attributes","title":"DOP Vellum Constraint Attributes","text":"<p>Note</p> <p>There are many types of constraints, so the meaning of these variables is often dependent on the constraint type. They usually live on the primitive.</p> <pre><code>s@type // Type of the constraint.\ns@type = 'distance' // Each edge in the display geometry is turned into a distance constraint maintaining that edge length.\ns@type = 'stitch' // Stitch points within the same geometry together using distance constraints. The points do not need to actually be connected by geometry. This is useful for keeping jackets closed or preventing pockets from flapping\ns@type = 'branchstitch' //\u00a0\ns@type = 'ptprim' //\u00a0\ns@type = 'bend' // Each pair of triangles (or implied triangles if input is quads or higher) creates a constraint maintaining the initial dihedral angle between the triangles.\ns@type = 'trianglebend' // Each pair of triangles (or implied triangles if input is quads or higher) creates a constraint maintaining the initial dihedral angle between the triangles.\n\u00a0 \u00a0 angle\n\u00a0 \u00a0 tetvolume\n\u00a0 \u00a0 pressure // Each piece, as determined by the Define Pieces parameter, stores its original volume and a many-point constraint is built to maintain it. The enforcement is global, so squishing one place will expand another, like a balloon.\n\u00a0 \u00a0 attach, pin\n\u00a0 \u00a0 attachnormal\n\u00a0 \u00a0 pinorient\n\u00a0 \u00a0 bendtwist\n\u00a0 \u00a0 stretchshear\n\u00a0 \u00a0 tetfiber\n\u00a0 \u00a0 triarap\n\u00a0 \u00a0 tetarap*\n\nf@stiffness // The stiffness of the constraint, which controls how strongly the constraint pulls.\nf@restlength\nf@restlengthorig\nf@dampingratio // Damping reduces jitter by bleeding energy when evaluating the constraint. Too much damping can prevent the constraint from being satisfied. Values less than 1 must be used.\nf@stress // Estimate of work done by the constraint (updated by Vellum solver).\ns@constraint_tag // The name of the node which created the constraint\n</code></pre>"},{"location":"vex/vex/#kinefx-attributes","title":"KineFX Attributes","text":"<p>Note</p> <p>A KineFX hierarchy or skeleton is represented by a collection of points connected by polygon lines. The parent-child relationship between joints in a hierarchy is determined by vertex order.</p> <pre><code>s@name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // joint name attribute\n3@transform \u00a0 \u00a0 \u00a0 \u00a0// World space 3\u00d73 transform of the point (rotation, scale, and shear).\n4@localtransform \u00a0 // The transform of the point relative to its parent.\ni@scaleinheritance // Determines how a point inherits the local scale from its parent.\n</code></pre>"},{"location":"vex/vex/#viewport-display-attributes","title":"Viewport Display Attributes","text":"<p>Note</p> <p>Override the viewport display mode attributes</p> <pre><code>i@gl_wireframe \u00a0 \u00a0 // Detail attribute, force wireframe (1) or shaded (-1)\ni@gl_lit \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Detail attribute, draw with lighting (1) or no lighting (0)\ni@gl_showallpoints // Detail attribute, draw points even if connected to geometry\nf@vm_cuspangle \u00a0 \u00a0 // Detail attribute, angle in degrees for generating cusped normals\ni@gl_spherepoints \u00a0// Detail attribute, (1) causes unconnected points to be drawn as spheres\ni@gl_xray \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Detail attribute, (1) geometry will visible even when it is hidden behind other geometry\n\nv@Cd \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Color Diffuse\nf@Alpha \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Surface opacity\nv@N \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Surface Normal for lighting\nf@width \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Curve width\nf@pscale \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // If no pscale exists the viewport defaults to 1.0, while Mantra defaults to 0.1\ns@spritepath\ns@shop_materialpath\ni@group__3d_hidden_primitives \u00a0// Add primitives to this group to hide them from the 3D viewport\n\nf@intrinsic:volumevisualdensity // Primitive intrinsic attribute controlling the opacity of volumes.\nf@volvis_shadowscale // Detail attribute controlling the shadow strength for volumes.\n</code></pre>"},{"location":"vex/vex/#copying-and-instancing-attributes","title":"Copying and Instancing Attributes","text":"<p>Note</p> <p>When copying or instancing, Houdini looks for these point attributes to transform each copy/instance. instanceattrs instanceattrs</p> <pre><code>p@orient \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Orientation of the copy.\nf@pscale \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Uniform scale.\nv@scale \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Non-uniform scale.\nv@N \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normal (+Z axis of the copy, if no p@orient).\nv@up \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Up vector of the copy (+Y axis of the copy, if no p@orient).\nv@v \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Velocity of the copy (motion blur), used as +Z axis if no p@orient or v@N.\np@rot \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Additional rotation (applied after the orientation attributes above).\nv@P \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Translation of the copy.\nv@trans \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Translation of the copy, in addition to v@P.\nv@pivot \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Local pivot point for the copy.\n3@transform or 4@transform \u00a0// Transform matrix overriding everything except v@P, v@pivot, and v@trans.\ns@shop_materialpath \u00a0 \u00a0 \u00a0 \u00a0 // The instanced object uses this material.\ns@material_override \u00a0 \u00a0 \u00a0 \u00a0 // A serialized Python dictionary mapping material parameter names to values.\ns@instance\ns@instancefile \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// File path indicating what geometry to instance.\ns@instancepath \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Geometry to instance. This is either a path to a file on disk or an op: path.\n</code></pre>"},{"location":"vex/vex/#accessing-other-inputs","title":"Accessing Other Inputs","text":"<p>Note</p> <p>This syntax is used to refer to nodes wired into the inputs of the wrangle, or other nodes in the network.</p> <pre><code>'opinput:X' is the most legible and always works (the first input is input 0).\npoint('opinput:0', 'P', i@ptnum)\npoint('opinput:1', 'P', i@ptnum)\npoint('opinput:2', 'P', i@ptnum)\npoint('opinput:3', 'P', i@ptnum)\n\nThe integer input number (the first input is 0). \u00a0Some functions don't support this but it's easy to type.\npoint(0, 'P', i@ptnum)\npoint(1, 'P', i@ptnum)\npoint(2, 'P', i@ptnum)\npoint(3, 'P', i@ptnum)\n\n@OpInputX works as well, but be careful as it isn't 0 based, instead it starts at 1 which is confusing\npoint(@OpInput1, 'P', i@ptnum)\npoint(@OpInput2, 'P', i@ptnum)\npoint(@OpInput3, 'P', i@ptnum)\npoint(@OpInput4, 'P', i@ptnum)\n\nv@opinputX_* reads an attribute from the same element on the numbered input (first input is input 0).\nv@opinput0_P\nv@opinput1_P\nv@opinput2_P\nv@opinput3_P\n\nAbsolute and relative paths to other nodes look like this.\npoint('op:/obj/geo1/OUT', 'P', i@ptnum)\npoint('op:../../OUT', 'P', i@ptnum)\n</code></pre>"},{"location":"vex/vex/#for-loop-metadata","title":"For Loop Metadata","text":"<p>Note</p> <p>Detail attributes, You can get this information with a looping_metadata \u201cFetch metadata\u201d Block Begin node].</p> <pre><code>i@numiterations \u00a0 \u00a0 \u00a0// The expected total number of iterations\ni@iteration \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// The current iteration number, always starting at 0 and increasing by 1 each loop.\nf@value \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// In piece-wise loops, this is the current value of the attribute\ni@ivalue \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // In simple repetition, this is an integer version of value.\n</code></pre>"},{"location":"vex/vex/#mantra-shader","title":"Mantra Shader","text":"<p>Note</p> <p>Mantra shader global variables. \u00a0See a primitive_spaces detailed explanation of implicit parametric UVs</p> <pre><code>v@Cf \u00a0 \u00a0 \u00a0// Surface Color.\nv@Of \u00a0 \u00a0 \u00a0// Surface Opacity.\nf@Af \u00a0 \u00a0 \u00a0// Surface Alpha.\nv@P \u00a0 \u00a0 \u00a0 // Surface Position (camera space).\nf@Pz \u00a0 \u00a0 \u00a0// Surface Depth.\nv@I \u00a0 \u00a0 \u00a0 // Direction from Eye (camera) to Surface.\nv@dPds \u00a0 \u00a0// Directions or Derivatives of surface implicit s coordinate.\nv@dPdt \u00a0 \u00a0// Directions or Derivatives of surface implicit t coordinate.\nv@N \u00a0 \u00a0 \u00a0 // Surface Normal.\nv@Ng \u00a0 \u00a0 \u00a0// Surface Geometric Normal.\nv@Eye \u00a0 \u00a0 // Position of Eye (camera).\nf@s \u00a0 \u00a0 \u00a0 // implicit parametric s coordinate (u).\nf@t \u00a0 \u00a0 \u00a0 // implicit parametric t coordinate (v).\nf@Time \u00a0 \u00a0// Shading Time.\nf@dPdz \u00a0 \u00a0// Change in Position with depth.\ni@SID \u00a0 \u00a0 // Sample Identifier. A sample id to be used with the nextsample() VEX function to generate consistent random samples that don\u2019t change when re-rendering or between frames.\n</code></pre>"},{"location":"vex/vex/#wireframeinviewport","title":"wireframeInViewport","text":"<p>Note</p> <p>Showing a geometry as wireframes.</p> <pre><code>// Set true (1) for ON and false (0) for OFF. Need Detail in Run Over [Attribute Wrangle]\n  @gl_wireframe = 1;\n</code></pre>"},{"location":"vex/vex/#groupexpand","title":"GroupExpand","text":"<p>Note</p> <p>Uniformly expanding group by a specified distance.</p> <pre><code>// Need a group filter ('group1') [Point Wrangle]\n  int pc = pcopen(0, 'P', @P, ch('radius'), chi('maxpts'));\n  while (pciterate(pc) &gt; 0)\n  {\n    int currentpt;\n    pcimport(pc, 'point.number', currentpt);\n    setpointgroup(0, 'group1', currentpt, 1);\n  }\n</code></pre> <p></p> <code>f@Frame</code> The current floating frame number, equivalent to the $FF Hscript variable <code>f@Time</code> The current time in seconds, equivalent to the $T Hscript variable <code>i@SimFrame</code> The integer simulation timestep number ($SF), only present in DOP contexts <code>f@SimTime</code> The simulation time in seconds ($ST), only present in DOP contexts <code>f@TimeInc</code> The timestep currently being used for simulation or playback - - Attribute Wrangle <code>v@P</code> The position of the current element <code>i@ptnum</code> The point number attached to the currently processed element <code>i@vtxnum</code> The linear number of the currently processed vertex <code>i@primnum</code> The primitive number attached to the currently processed element <code>i@elemnum</code> The index number of the currently processed element <code>i@numpt</code> The total number of points in the geometry <code>i@numvtx</code> The number of vertices in the primitive of the currently processed element <code>i@numprim</code> The total number of primitives in the geometry <code>i@numelem</code> The total number of elements being processed - - Volume Wrangle <code>v@P</code> The position of the current voxel <code>f@density</code> The value of the density field at the current voxel location <code>v@center</code> The center of the current volume <code>v@dPdx</code>, <code>v@dPdy</code>, <code>v@dPdz</code> These vectors store the change in P that occurs in the x, y, and z voxel indices <code>i@ix</code>, <code>i@iy</code>, <code>i@iz</code> Voxel indices For dense volumes (non-VDB) these range from 0 to resolution-1 <code>i@resx</code>, <code>i@resy</code>, <code>i@resz</code> The resolution of the current volume - - Common Geometry Attributes <code>i@id</code> A unique indexing number that remains the same throughout time  Used to match elements between frames <code>f@pscale</code> Particle radius size  Uniform scale  Set display particles as 'Discs' to visualize <code>f@width</code> Thickness of curves  Enable 'Shade Open Curves In Viewport' on the object node to visualize <code>f@Alpha</code> Alpha transparency override  The viewport uses this to set the alpha of OpenGL geometry <code>f@Pw</code> Spline weight  Mostly depreciated at this point <code>v@P</code> Point position  Used this to lay out points in 3D space <code>v@Cd</code> Diffuse color override  The viewport uses this to color OpenGL geometry <code>v@N</code> Surface or curve normal  Houdini will compute the normal if this attribute does not exist <code>v@scale</code> Vector scale  Allows directional scaling or stretching (in one direction) <code>v@rest</code> Used by procedural patterns and textures to stick on deforming and animated surfaces <code>v@up</code> Up vector  The up direction for local space, typically (0, 1, 0) <code>v@uv</code> UV texture coordinates for this point/vertex <code>v@v</code> Point velocity  The direction and speed of movement in units per second <code>p@orient</code> The local orientation of the point (represented as a quaternion) <code>p@rot</code> Additional rotation to be applied after orient, N, and up attributes <code>s@name</code> A unique name identifying which primitives belong to which piece  Also used to label volumes <code>s@instance</code> Path of an object node to be instanced at render time - - DOP Particle Attributes <code>f@age</code> Time in seconds since the particle was born <code>f@life</code> Time in seconds the particle is allowed to live When f@age&gt;f@life, i@dead will be set to 1 <code>f@nage</code> Normalized age, f@age divided by f@life  Implicit attribute, you cannot write to this <code>i@dead</code> Whether a particle is living (0) or dead (1)  A dead particle is deleted in the Reaping stage <code>i@id</code> A unique id for the particle that remains the same throughout a single simulation <code>i@stopped</code> Whether a particle is moving (0) or stopped (1) <code>i@stuck</code> Whether a particle is free (0) or stuck (1) <code>i@sliding</code> Whether a particle is free (0) or sliding along a surface (1) <code>f@cling</code> Force applied to sliding paritcles inwards (according to the collision's surface normal) <code>s@pospath</code> The path to the object that the particle is colliding with <code>i@posprim</code> Which collision primitive in the path geometry whose position we wish to refer to <code>v@posuv</code> Parametric uv on the collision primitive <code>i@hittotal</code> The cumulative total of all hits for the particle (only incremented once per timestep) <code>i@has_pprevious</code> This is set to 1 if v@pprevious contains valid values <code>v@pprevious</code> Stores the position of the particle on the previous frame  Used for collision detection <code>i@hitnum</code> The number of times the particle collided in the last POP Collision Detect <code>s@hitpath</code> The path to the object that was hit A path to a file on disk or an op: path <code>i@hitprim</code> The primitive hit Could be -1 if it the collision detector couldn\u2019t figure out which prim <code>v@hituv</code> The parametric UV space on the primitive <code>v@hitpos</code> Where the hit actually occurred  Useful if the colliding object was moving <code>v@hitnml</code> The normal of the surface at the time of the collision <code>v@hitv</code> The velocity of the surface at the time of the collision <code>f@hittime</code> When the collision occurred, that could be within a frame <code>f@hitimpulse</code> Records how much of an impulse was needed for the collision resolution  varies with timestep <code>f@bounce</code> When particles bounce off another object, this controls how much energy they keep <code>f@bounceforward</code> Controls how much energy they keep in the tangential direction <code>f@friction</code> When particles bounce, they are slowed down proportional to how hard they hit <code>s@collisionignore</code> Objects that match this pattern will not be collided <code>f@force</code> Forces on the particle for this frame <code>f@mass</code> Inertia of the particle <code>v@spinshape</code> This is multiplied by f@pscale to determine the shape of the particle for rotational inertia <code>f@drag</code> How much the particle is effected by any wind effects <code>f@dragexp</code> Ranges from 1 to 2, default is set on the solver  Used for both angular and linear drag <code>v@dragshape</code> How much the particle is dragged in each of its local axes <code>v@dragcenter</code> If specified, drag forces will also generate torques on the particle <code>v@targetv</code> The local wind speed Thought of as the goal, or target, velocity for the particle <code>f@airresist</code> How important it is to match the wind speed  Thickness of the air <code>f@speedmin</code> Minumum speed, in units per second, that a particle can move <code>f@speedmax</code> Maximum speed, in units per second, that a particle can move <code>p@orient</code> Orientation of the particle  Used for figuring out 'local' forces <code>v@w</code> Angular speed of the particle  A vector giving the rotation axis <code>v@torque</code> The equivalent of force for spins No inertial tensor (the equivalent of mass) is supported <code>v@targetw</code> The goal spin direction and speed for this particle <code>f@spinresist</code> How important it is to match the targetw <code>f@spinmin</code> Minumum speed in radians per second that a particle can spin <code>f@spinmax</code> Maximum speed in radians per second that a particle can spin - - DOP Grains Attributes <code>i@ispbd</code> A value of 1 causes the particle to behave as grains <code>f@pscale</code> Used to determine the radius of each particle <code>f@repulsionweight</code> How much the particle collision forces are weighted <code>f@repulsionstiffness</code> How strongly particles are kept apart  Higher values result in less bouncy repulsion <code>f@attractionweight</code> How much the particles will naturally stick together when close <code>f@attractionstiffness</code> How strongly nearby particles stick to each other <code>v@targetP</code> Particles are constrained to this location <code>f@targetweight</code> The weight of the v@targetP constraints <code>f@targetstiffness</code> The stiffness with which particles are fixed to their v@targetP attribute <code>f@restlength</code> Particles connected by polylines will be forced to maintain this distance (prim attribute) <code>f@constraintweight</code> Scale, on a per-particle basis of the constraint force <code>f@constraintstiffness</code> This controls the stiffness on a per-particle basis <code>f@strain</code> This primitive attribute records how much the constraint is stretched <code>f@strength</code> If f@strain exceeds this primitive attribute, the constraint will be removed - - DOP Packed RBD Attributes <code>i@active</code> Specifies whether the object is able to react to other objects <code>i@animated</code> Specifies whether the transform should be updated from its SOP geometry at each timestep <code>i@deforming</code> Specifies whether the collision shape should be rebuilt from its SOP geometry each timestep <code>f@bounce</code> The elasticity of the object <code>i@bullet_add_impact</code> Impacts that occur during the sim will be recorded in the Impacts or Feedback data <code>i@bullet_ignore</code> Specifies whether the object should be completely ignored by the Bullet solver <code>f@bullet_angular_sleep_threshold</code> The sleeping threshold for the object\u2019s angular velocity <code>f@bullet_linear_sleep_threshold</code> The sleeping threshold for the object\u2019s linear velocity <code>i@bullet_want_deactivate</code> Disables simulation of a non-moving object until the object moves again <code>i@computecom</code> Specifies whether the center of mass should be computed from the collision shape <code>i@computemass</code> Specifies whether the mass should be computedfrom the collision shape and density <code>f@creationtime</code> Stores the simulation time at which the object was created <code>i@dead</code> Specifies whether the object should be deleted during the next solve <code>f@density</code> The mass of an object is its volume times its density <code>f@friction</code> The coefficient of friction of the object <code>f@inertialtensorstiffness</code> Rotational stiffness  A scale factor applied to the inertial tensor <code>i@inheritvelocity</code> v and w point attributes from the SOP geometry will override the initial velocity <code>f@mass</code> The mass of the object <code>s@name</code> A unique name for the object Used by Constraint Networks <code>p@orient</code> The orientation of the object <code>v@P</code> The current position of the object\u2019s center of mass <code>v@pivot</code> The pivot that the orientation applies to If i@computecom is non-zero, this is auto-computed <code>v@v</code> Linear velocity of the object <code>v@w</code> Angular velocity of the object, in radians per second <code>i@bullet_adjust_geometry</code> Shrinks the collision geometry <code>i@bullet_autofit</code> Use the bounds of the object for Box, Capsule, Cylinder, Sphere, or Plane <code>f@bullet_collision_margin</code> Padding distance between collision shapes <code>s@bullet_georep</code> Can be convexhull, concave, box, capsule, cylinder, compound, sphere, or plane <code>i@bullet_groupconnected</code> Create convex hull per set of connected primitives <code>f@bullet_length</code> The length of the Capsule or Cylinder collision shape in the Y direction <code>v@bullet_primR</code> Orientation of the Box, Capsule, Cylinder, or Plane collision shape <code>v@bullet_primS</code> Size of the Box collision shape <code>v@bullet_primT</code> Position of the Box, Sphere, Capsule, Cylinder, or Plane collision shape <code>f@bullet_radius</code> Radius of the Sphere, Capsule, or Cylinder collision shape <code>f@bullet_shrink_amount</code> Specifies the amount of resizing done by Shrink Collision Geometry <code>s@activationignore</code> Won't be activated by collisions with any objects that match this pattern <code>s@collisiongroup</code> Specifies the name of a collision group that this object belongs to <code>s@collisionignore</code> The object will not collide against any objects that match this pattern <code>f@min_activation_impulse</code> Minimum impulse that will cause the object to switch from inactive to active <code>f@speedmin</code> Minumum speed, in units per second, that a particle can move <code>f@speedmax</code> Maximum speed, in units per second, that a particle can move <code>f@spinmin</code> Minumum speed in radians per second that a particle can spin <code>f@spinmax</code> Maximum speed in radians per second that a particle can spin <code>f@accelmax</code> Limits the change in the object\u2019s speed that is caused by enforcing constraints <code>f@angaccelmax</code> Limits the change in the object\u2019s angular speed that is caused by enforcing constraints <code>f@airresist</code> Specifies how important it is to match the target velocity (v@targetv) <code>f@drag</code> How much the the v@targetv and f@airresist attributes effect the object <code>f@dragexp</code> Ranges from 1 to 2, default is set on the solver  Used for both angular and linear drag <code>v@force</code> Specifies a force that will be applied to the center of mass of the object <code>f@spinresist</code> Specifies how important it is to match the target angular velocity (v@targetw) <code>v@targetv</code> Target velocity for the object Used in combination with the f@airresist attribute <code>v@targetw</code> Target angular velocity for the object Used in combination with the f@spinresist attribute <code>v@torque</code> Specifies a torque that will be applied to the object <code>i@bullet_autofit_valid</code> Stores whether the solver has already computed collision shape attributes <code>i@bullet_sleeping</code> Tracks whether the object has been put to sleep by the solver <code>f@deactivation_time</code> Amount of time the speed has been below the Linear Threshold or Angular Threshold <code>i@found_overlap</code> Used by the solver to determine whether it has performed the overlap test <code>i@id</code> A unique identifier for the object <code>i@nextid</code> Stores the i@id the solver will assign to the next new object - - DOP Constraint Network Attributes <code>f@width</code> Width of each edge <code>f@density</code> Density of each point <code>p@orient</code> Initial orientation of each point This value is stored as a quaternion <code>v@v</code> Initial velocity of each point <code>v@w</code> Initial angular velocity of each point measured in radians per second <code>f@friction</code> Friction of each point <code>f@klinear</code> Defines how strongly the wire resists stretching <code>f@damplinear</code> Defines how strongly the wire resists oscillation due to stretching forces <code>f@kangular</code> Defines how strongly the wire resists bending <code>f@dampangular</code> Defines how strongly the wire resists oscillation due to bending forces <code>f@targetstiffness</code> Defines how strongly the wire resists deforming from the animated position <code>f@targetdamping</code> Defines how strongly the wire resists oscillation due to stretch forces <code>f@normaldrag</code> The component of drag in the directions normal to the wire <code>f@tangentdrag</code> The component of drag in the direction tangent to the wire <code>i@nocollide</code> Collision detection for the edge is disabled (Only used if Collision Handling is SDF) <code>v@restP</code> Rest position of each point <code>p@restorient</code> Rest orientation of each point <code>i@gluetoanimation</code> Causes a point\u2019s position and orientation to be constrained to the input geometry <code>i@pintoanimation</code> Causes a point\u2019s position to be constrained to the input geometry <code>v@animationP</code> Target position of each point <code>p@animationorient</code> Target orientation of each point <code>v@animationv</code> Target velocity of each point <code>v@animationw</code> Target angular velocity of each point <code>i@independentcollisionallowed</code> Toggle external collisions (Only non-SDF Geometric Collision) <code>i@independentcollisionresolved</code> Unresolved external collisions (Only non-SDF Geometric Collision) <code>i@codependentcollisionallowed</code> Toggle soft body collisions (Only non-SDF Geometric Collision) <code>i@codependentcollisionresolved</code> Unresolved toggle soft body collisions (Only non-SDF Geometric Collision) <code>i@selfcollisionallowed</code> Toggle self collisions (Only non-SDF Geometric Collision) <code>i@selfcollisionresolved</code> Unresolved toggle self collisions (Only non-SDF Geometric Collision) - - DOP FLIP Attributes <code>f@pscale</code> Particle scale <code>v@v</code> Particle velocity <code>f@viscosity</code> The \"thickness\" of a fluid <code>f@density</code> The mass per unit volume <code>f@temperature</code> The temperature of the fluid <code>f@vorticity</code> Measures the amount of circulation in the fluid <code>f@divergence</code> Positive values cause particles to spread out, negative cause them to clump together <code>v@rest</code> Used to track the position of the fluid over time <code>v@rest2</code> Used for blending dual rest attributes, avoids stretching <code>f@droplet</code> Identifies particles that separate from the main body of fluid <code>f@underresolved</code> Particles that haven't fully resolved on the grid <code>i@ballistic</code> Specifies particles which will be ignored by the fluid solve <code>v@Lx</code> Angular momentum X axis <code>v@Ly</code> Angular momentum Y axis <code>v@Lz</code> Angular momentum Z axis"}]}